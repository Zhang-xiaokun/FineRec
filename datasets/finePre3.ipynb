{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# senti preprocess 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train_session 70260\n",
      "#train_items 8614\n",
      "#train_price 10\n",
      "#train_category 48\n",
      "#test_session:  7766\n",
      "no data augment\n",
      "no data augment\n",
      "train sequence:  [[1], [3], [5], [7, 8], [10, 11]]\n",
      "train price:  [[1], [3], [3], [4, 1], [3, 1]]\n",
      "train category:  [[1], [2], [3], [5, 5], [6, 2]]\n",
      "train lab:  [2, 4, 6, 9, 12]\n",
      "#train_interactions:  176784\n",
      "#test_interactions:  19592\n",
      "#train_session:  70260\n",
      "#test_session:  7766\n",
      "sequence average length:  2.5168020916104887\n",
      "dataset:  Cell_Phones_and_Accessories\n",
      "2022-09-23 16:34:44\n",
      "data formulation: id_seq, price_seq, cate_seq, price_list, cate_list, labs\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "datasets_name = 'Cell_Phones_and_Accessories'\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "\n",
    "price_num = '100'\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID', 'priceLevel', 'category']]\n",
    "\n",
    "# item_inter_num = pd.DataFrame(data.groupby(data['itemID']).count())\n",
    "# item_inter_num = item_inter_num.reset_index()[['sessionID', 'itemID']]\n",
    "# item_num=item_inter_num.rename(columns={'sessionID':'item_num'})\n",
    "# data = pd.merge(data, item_num, how='left', on = 'itemID')\n",
    "# data = data[data['item_num']>4]\n",
    "# data = data[['sessionID', 'itemID', 'priceLevel']]\n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "# dict (sessionID:[priceLevel, priceLevel])\n",
    "price_all = {}\n",
    "\n",
    "\n",
    "\n",
    "# dict (sessionID:[cate, cate])\n",
    "cate_all = {}\n",
    "# dict (sessionID:[brand, brand])\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    price = row['priceLevel']\n",
    "    cate = row['category']\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "        price_all[sess_id].append(price)\n",
    "        cate_all[sess_id].append(cate)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "        price_all[sess_id] = []\n",
    "        price_all[sess_id].append(price)\n",
    "        cate_all[sess_id] = []\n",
    "        cate_all[sess_id].append(cate)\n",
    "\n",
    "sess_total = data['sessionID'].max()\n",
    "split_num = int(sess_total/10*9)\n",
    "\n",
    "tra_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "tes_sess = dict() \n",
    "tra_price = dict()# dict(session_id:[price,price])\n",
    "tes_price = dict()\n",
    "tra_cate = dict()# dict(session_id:[cate,cate])\n",
    "tes_cate = dict()\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    all_price = price_all[sess_temp]\n",
    "    all_cate = cate_all[sess_temp]\n",
    "    if len(all_seqs) < 2:\n",
    "        continue\n",
    "    if len(all_seqs) > 20:\n",
    "        all_seqs = all_seqs[:20]\n",
    "        all_price = all_price[:20]\n",
    "        all_cate = all_cate[:20]\n",
    "    if int(sess_temp) < split_num:\n",
    "        tra_sess[sess_temp] = all_seqs\n",
    "        tra_price[sess_temp] = all_price\n",
    "        tra_cate[sess_temp] = all_cate\n",
    "    else:\n",
    "        tes_sess[sess_temp] = all_seqs\n",
    "        tes_price[sess_temp] = all_price\n",
    "        tes_cate[sess_temp] = all_cate\n",
    "\n",
    "# the number of item & price, renumber\n",
    "# item_ctr = 1\n",
    "# price_ctr = 1\n",
    "# cate_ctr = 1\n",
    "\n",
    "item_dict = {} #dict(old_itemID: new_itemID)\n",
    "cate_dict = {} #dict(old_cate: new_cate)\n",
    "price_dict = {} #dict(old_price: new_price)\n",
    "\n",
    "\n",
    "# item_id: price_level, item_id: rating\n",
    "item_price = {} #dict[new_itemID: priceLevel]\n",
    "item_cate = {} #dict[new_itemID: cate]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# tra_sess tra_price tra_cate\n",
    "# Convert training sessions to sequences and renumber items to start from 1\n",
    "def obtian_tra():\n",
    "#     global item_ctr\n",
    "#     print(item_ctr)\n",
    "    train_seqs = []\n",
    "    train_price = []\n",
    "    train_cate = []\n",
    "#     train_brand = []\n",
    "    item_ctr = 1\n",
    "    price_ctr = 1\n",
    "    cate_ctr = 1\n",
    "    for s in tra_sess:\n",
    "        seq = tra_sess[s]\n",
    "        price_seq = tra_price[s]\n",
    "        cate_seq = tra_cate[s]\n",
    "        outseq = []\n",
    "        pri_outseq = []\n",
    "#         rating_outseq = []\n",
    "        cate_outseq = []\n",
    "        for (i, j_pri, w_cate) in zip(seq, price_seq, cate_seq):\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "            else:\n",
    "                outseq += [item_ctr]\n",
    "                item_dict[i] = item_ctr\n",
    "                item_ctr += 1\n",
    "            if j_pri in price_dict:\n",
    "                pri_outseq += [price_dict[j_pri]]\n",
    "            else:\n",
    "                pri_outseq += [price_ctr]\n",
    "                price_dict[j_pri] = price_ctr\n",
    "                price_ctr += 1\n",
    "            if w_cate in cate_dict:\n",
    "                cate_outseq += [cate_dict[w_cate]]\n",
    "            else:\n",
    "                cate_outseq += [cate_ctr]\n",
    "                cate_dict[w_cate] = cate_ctr\n",
    "                cate_ctr += 1\n",
    "            if item_dict[i] not in item_price:\n",
    "                item_price[item_dict[i]] = price_dict[j_pri]\n",
    "                item_cate[item_dict[i]] = cate_dict[w_cate]\n",
    "        if len(outseq) < 2:  # Doesn't occur\n",
    "            print('session length is 1')\n",
    "            continue\n",
    "        train_seqs += [outseq]\n",
    "        train_price += [pri_outseq]\n",
    "        train_cate += [cate_outseq]\n",
    "    print(\"#train_session\",len(train_seqs))\n",
    "    print(\"#train_items\",item_ctr-1)\n",
    "    print(\"#train_price\",price_ctr-1)\n",
    "    print(\"#train_category\",cate_ctr-1)\n",
    "    return train_seqs, train_price, train_cate\n",
    "\n",
    "\n",
    "# Convert test sessions to sequences, ignoring items that do not appear in training set\n",
    "def obtian_tes():\n",
    "    test_seqs = []\n",
    "    test_price = []\n",
    "    test_cate = []\n",
    "    for s in tes_sess:\n",
    "        outseq = []\n",
    "        out_price = []\n",
    "        out_rating = []\n",
    "        out_cate = []\n",
    "        for i, j, w in zip(tes_sess[s], tes_price[s], tes_cate[s]):\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "                out_price += [price_dict[j]]\n",
    "                out_cate += [cate_dict[w]]\n",
    "        if len(outseq) < 2:\n",
    "            print('obtain test session length is 1')\n",
    "            continue\n",
    "        test_seqs += [outseq]\n",
    "        test_price += [out_price]\n",
    "        test_cate += [out_cate]\n",
    "    print(\"#test_session: \",len(test_seqs))\n",
    "    return test_seqs, test_price, test_cate\n",
    "\n",
    "# Convert test sessions to sequences, renumbering items that do not appear in training set\n",
    "def obtian_tes_cold():\n",
    "    test_seqs = []\n",
    "    test_price = []\n",
    "    test_seqs_cold = []\n",
    "    test_price_cold = []\n",
    "    num_cold = 0\n",
    "    global item_ctr,price_ctr\n",
    "#     test_cate = []\n",
    "    for s in tes_sess:\n",
    "        outseq = []\n",
    "        out_price = []\n",
    "        contain_cold = False\n",
    "        for i, p in zip(tes_sess[s], tes_price[s]):\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "            else:\n",
    "                num_cold += 1\n",
    "                contain_cold = True\n",
    "                outseq += [item_ctr]\n",
    "                item_dict[i] = item_ctr\n",
    "                item_ctr += 1\n",
    "            if p in price_dict:\n",
    "                out_price += [price_dict[p]]\n",
    "            else:\n",
    "                out_price += [price_ctr]\n",
    "                price_dict[p]= price_ctr\n",
    "                price_ctr += 1\n",
    "#             if i in item_dict:\n",
    "#                 outseq += [item_dict[i]]\n",
    "#                 out_price += [price_dict[j]]\n",
    "\n",
    "        if len(outseq) < 2:\n",
    "            print('obtain test session length is 1')\n",
    "            continue\n",
    "        if contain_cold:\n",
    "            test_seqs_cold += [outseq]\n",
    "            test_price_cold += [out_price]\n",
    "        else:\n",
    "            test_seqs += [outseq]\n",
    "            test_price += [out_price]\n",
    "    print(\"#test_session: \",len(test_seqs))\n",
    "    print(\"#cold_test_session: \",len(test_seqs_cold))\n",
    "    print('#cold_items',str(num_cold))\n",
    "    return test_seqs, test_price, test_seqs_cold, test_price_cold\n",
    "\n",
    "\n",
    "# 数据增强\n",
    "def process_seqs(iseqs, iprice, icate):\n",
    "    out_seqs = []\n",
    "    out_price = []\n",
    "    out_cate = []\n",
    "    labs = []\n",
    "    max_length = 19\n",
    "    for seq, pri, rat, cat in zip(iseqs, iprice, icate):\n",
    "        for i in range(1, len(seq)):\n",
    "            tar = seq[-i]\n",
    "            labs += [tar]\n",
    "            out_seqs += [seq[:-i]]\n",
    "            out_price += [pri[:-i]]\n",
    "            out_cate += [cat[:-i]]\n",
    "    return out_seqs, out_price, out_cate, labs\n",
    "\n",
    "# 无数据增强\n",
    "def process_seqs_no(iseqs, iprice, icate):\n",
    "    print(\"no data augment\")\n",
    "    out_seqs = []\n",
    "    out_price = []\n",
    "    out_cate = []\n",
    "    labs = []\n",
    "    max_length = 19\n",
    "    for seq, pri, cat in zip(iseqs, iprice, icate):     \n",
    "        labs += [seq[-1]]\n",
    "        out_seqs += [seq[:-1]]\n",
    "        out_price += [pri[:-1]]\n",
    "        out_cate += [cat[:-1]]\n",
    "    return out_seqs, out_price, out_cate, labs\n",
    "\n",
    "tra_seqs, tra_pri, tra_cat  = obtian_tra()\n",
    "tes_seqs, tes_pri, tes_cat  = obtian_tes()\n",
    "# save item_dict:(old_itemID: new_itemID) nocold\n",
    "old2new_nocold_path = './dict/' + datasets_name + '/old2newNocold.csv'\n",
    "old_list_nocold = list(item_dict.keys())\n",
    "new_list_nocold = list(item_dict.values())\n",
    "old2new_nocold_dict = {'old':old_list_nocold, 'new':new_list_nocold}\n",
    "data_old_new_no = pd.DataFrame(old2new_nocold_dict)\n",
    "data_old_new_no.to_csv(old2new_nocold_path)\n",
    "\n",
    "# tes_seqs_nocold, tes_pri_nocold, tes_seqs_cold, tes_pri_cold  = obtian_tes_cold()\n",
    "# # save item_dict:(old_itemID: new_itemID) cold\n",
    "# old2new_cold_path = './dict/' + datasets_name + '/old2newCold.csv'\n",
    "# old_list_cold = list(item_dict.keys())\n",
    "# new_list_cold = list(item_dict.values())\n",
    "# old2new_cold_dict = {'old':old_list_cold, 'new':new_list_cold}\n",
    "# data_old_new_cold = pd.DataFrame(old2new_cold_dict)\n",
    "# data_old_new_cold.to_csv(old2new_cold_path)\n",
    "\n",
    "tr_seqs, tr_pri, tr_cat, tr_labs = process_seqs_no(tra_seqs, tra_pri, tra_cat)\n",
    "te_seqs, te_pri, te_cat, te_labs = process_seqs_no(tes_seqs, tes_pri, tes_cat)\n",
    "\n",
    "# 冷启动情况下，两个数据集\n",
    "# te_seqs_nocold, te_pri_nocold, te_labs_nocold = process_seqs_no(tes_seqs_nocold, tes_pri_nocold)\n",
    "# te_seqs_cold, te_pri_cold, te_labs_cold = process_seqs_no(tes_seqs_cold, tes_pri_cold)\n",
    "\n",
    "print('train sequence: ',tr_seqs[:5])\n",
    "print('train price: ',tr_pri[:5])\n",
    "print('train category: ',tr_cat[:5])\n",
    "print('train lab: ',tr_labs[:5])\n",
    "\n",
    "# construct all matrics whose shape is similar as session-items [[],[]]\n",
    "#  Cell_Phones_and_Accessories_brand 各类型数据量\n",
    "# def tomatrix(all_seqs, all_pri):\n",
    "\n",
    "#     price_item_dict = {}\n",
    "#     price_item = []\n",
    "\n",
    "#     # price-item-dict -> {price_id:[1, 3, 4]}\n",
    "\n",
    "#     for s_seq, p_seq in zip(all_seqs, all_pri):\n",
    "#         for i_temp, p_temp in zip(s_seq, p_seq):\n",
    "#             if p_temp not in price_item_dict:\n",
    "# #                 print('price_new: ',p_temp)\n",
    "#                 price_item_dict[p_temp] = []\n",
    "#             price_item_dict[p_temp].append(i_temp)\n",
    "\n",
    "#     price_item_dict = dict(sorted(price_item_dict.items()))\n",
    "#     print(\"#price\",len(price_item_dict))\n",
    "    \n",
    "#     price_item = list(price_item_dict.values())\n",
    "#     return price_item\n",
    "\n",
    "# def data_masks(all_sessions):\n",
    "#     indptr, indices, data = [], [], []\n",
    "#     indptr.append(0)\n",
    "#     for j in range(len(all_sessions)):\n",
    "#         session = np.unique(all_sessions[j]) #统计session中不同item，去重，并按照item_id排序\n",
    "#         length = len(session)\n",
    "#         s = indptr[-1]\n",
    "#         indptr.append((s + length))\n",
    "#         for i in range(length):\n",
    "#             indices.append(session[i]-1)\n",
    "#             data.append(1)\n",
    "#     # indptr:session长度累加和; indices:item_id 减1, 由每个session内item组成; data:item在session内的权重，全部为1.\n",
    "#     results = (data, indices, indptr)\n",
    "#     # 10000 * 6558 #sessions * #items H in paper 稀疏矩阵存储\n",
    "#     return results\n",
    "\n",
    "# tra_pi = tomatrix(tra_seqs+tes_seqs, tra_pri+tes_pri)\n",
    "# # tes_pi, tes_pb, tes_pc, tes_bi, tes_bc, tes_ci = tomatrix(te_seqs, te_pri, te_cat, te_bra)\n",
    "\n",
    "# # 冷启动下两个测试集，一个不包含cold_item 一个包含cold_item\n",
    "# tra_pi_nocold = tomatrix(tra_seqs+te_seqs_nocold, tra_pri+te_pri_nocold)\n",
    "# tra_pi_cold = tomatrix(tra_seqs+te_seqs_cold, tra_pri+te_pri_cold)\n",
    "\n",
    "# # tra = (tr_seqs, tr_pri, data_masks(tr_seqs), data_masks(tr_pri), data_masks(tra_pi), tr_labs)\n",
    "# # tes = (te_seqs, te_pri, data_masks(te_seqs), data_masks(te_pri), data_masks(tra_pi), te_labs)\n",
    "# # 暂时不加tr_pri 即 session_id: [p1, p2, p3] 原代码如上两句\n",
    "# tra = (tr_seqs, tr_pri, data_masks(tr_seqs), data_masks(tra_pi), tr_labs)\n",
    "# tes = (te_seqs, te_pri, data_masks(te_seqs), data_masks(tra_pi), te_labs)\n",
    "# # 冷启动下两个测试集，一个不包含cold_item 一个包含cold_item\n",
    "# tes_nocold = (te_seqs_nocold, te_pri_nocold, data_masks(te_seqs_nocold), data_masks(tra_pi_nocold), te_labs_nocold)\n",
    "# tes_cold = (te_seqs_cold, te_pri_cold, data_masks(te_seqs_cold), data_masks(tra_pi_cold), te_labs_cold)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 增加 all_item_price_list & all_item_rating_list\n",
    "\n",
    "ip_id_list = list(item_price.keys())\n",
    "ip_price_list = list(item_price.values())\n",
    "id_price_dict = {'id':ip_id_list, 'price':ip_price_list}\n",
    "id_price_dict = pd.DataFrame(id_price_dict)\n",
    "id_price_dict = id_price_dict.reset_index()[['id', 'price']]\n",
    "\n",
    "id_price_dict.sort_values(by=[\"id\"],inplace=True,ascending=[True])\n",
    "price_list_save = id_price_dict['price'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "ic_id_list = list(item_cate.keys())\n",
    "ic_cate_list = list(item_cate.values())\n",
    "id_cate_dict = {'id':ic_id_list, 'cate':ic_cate_list}\n",
    "id_cate_dict = pd.DataFrame(id_cate_dict)\n",
    "id_cate_dict = id_cate_dict.reset_index()[['id', 'cate']]\n",
    "\n",
    "id_cate_dict.sort_values(by=[\"id\"],inplace=True,ascending=[True])\n",
    "cate_list_save = id_cate_dict['cate'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "tra = (tr_seqs, tr_pri, tr_cat, price_list_save, cate_list_save, tr_labs)\n",
    "tes = (te_seqs, te_pri, te_cat, price_list_save, cate_list_save, te_labs)\n",
    "\n",
    "# print(len(tra[3]))\n",
    "all_train = 0\n",
    "all_tes = 0\n",
    "# all_tes_nocold = 0\n",
    "# all_tes_cold = 0\n",
    "for seq in tra_seqs:\n",
    "    all_train += len(seq)\n",
    "for seq in tes_seqs:\n",
    "    all_tes += len(seq)\n",
    "# for seq in tes_seqs_nocold:\n",
    "#     all_tes_nocold += len(seq)\n",
    "# for seq in tes_seqs_cold:\n",
    "#     all_tes_cold += len(seq)\n",
    "print('#train_interactions: ',all_train)\n",
    "print('#test_interactions: ',all_tes)\n",
    "# print('#nocold_test_interactions: ',all_tes_nocold)\n",
    "# print('#cold_test_interactions: ',all_tes_cold)\n",
    "\n",
    "print('#train_session: ',(len(tra_seqs)))\n",
    "print('#test_session: ',(len(tes_seqs)))\n",
    "# print('#nocold_test_session: ',(len(tes_seqs_nocold)))\n",
    "# print('#cold_test_session: ',(len(tes_seqs_cold)))\n",
    "\n",
    "print('sequence average length: ', (all_train+all_tes)/(len(tra_seqs) + len(tes_seqs) * 1.0))\n",
    "# print('cold sequence average length: ', (all_train+all_tes_nocold+all_tes_cold)/(len(tra_seqs) + len(tes_seqs_nocold) +len(tes_seqs_cold) * 1.0))\n",
    "train_data_path = './datasets/' + datasets_name + str(price_num) + 'logistic'\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "path_data_train = train_data_path + \"/train.txt\"\n",
    "path_normal_test = train_data_path + \"/test.txt\"\n",
    "# path_nocold_test = train_data_path + \"/nocoldtest.txt\"\n",
    "# path_cold_test = train_data_path + \"/coldtest.txt\"\n",
    "# path_data_pc = train_data_path + \"/itemlistPCB.txt\"\n",
    "\n",
    "pickle.dump(tra, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes, open(path_normal_test, 'wb'))\n",
    "# pickle.dump(tes_nocold, open(path_nocold_test, 'wb'))\n",
    "# pickle.dump(tes_cold, open(path_cold_test, 'wb'))\n",
    "# pickle.dump(price_cate_list, open(path_data_pc, 'wb'))\n",
    "\n",
    "\n",
    "# get asin item_id 获得asin list 有顺序, 给处理 img及text\n",
    "asin2old_path = 'dict/' + datasets_name +'/asin2itemID.csv'\n",
    "old2new_path = 'dict/' + datasets_name +'/old2newNocold.csv'\n",
    "# id_img_emb = 'dict/' + datasets_name +'/id_imgEmbedding.csv'\n",
    "# id_text_emb = 'dict/' + datasets_name +'/id_textEmbedding.csv'\n",
    "\n",
    "# text_matrix_path = 'dict/' + datasets_name +'/text_matrix.npy'\n",
    "# image_matrix_path = 'dict/' + datasets_name +'/image_matrix.npy'\n",
    "\n",
    "\n",
    "asin2old = pd.read_csv(asin2old_path)\n",
    "asin2old = asin2old.rename(columns={ 'itemID':'old'})\n",
    "asin2old = asin2old[['asin', 'old']]\n",
    "\n",
    "\n",
    "old2new = pd.read_csv(old2new_path)\n",
    "old2new = old2new[['old', 'new']]\n",
    "\n",
    "\n",
    "asin2new = pd.merge(asin2old, old2new, how='left', on = 'old')\n",
    "\n",
    "\n",
    "asin2new = asin2new.dropna(axis=0)\n",
    "asin2ItemID = asin2new.reset_index()[['asin', 'new']]\n",
    "\n",
    "asin2ItemID.sort_values(by=[\"new\"],inplace=True,ascending=[True])\n",
    "\n",
    "\n",
    "asin_list = asin2ItemID['asin'].tolist()\n",
    "# new_list = asin2ItemID['new'].tolist()\n",
    "asin_list_save = train_data_path + '/asinlist.npy'\n",
    "np.save(asin_list_save, asin_list)\n",
    "\n",
    "\n",
    "\n",
    "print(\"dataset: \", datasets_name)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"data formulation: id_seq, price_seq, cate_seq, price_list, cate_list, labs\")\n",
    "print(\"done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test length features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train_session 70260\n",
      "#train_items 8614\n",
      "#train_price 10\n",
      "#train_category 48\n",
      "#test_session:  7766\n",
      "no data augment\n",
      "no data augment\n",
      "train sequence:  [[1], [3], [5], [7, 8], [10, 11]]\n",
      "train price:  [[1], [3], [3], [4, 1], [3, 1]]\n",
      "train category:  [[1], [2], [3], [5, 5], [6, 2]]\n",
      "train lab:  [2, 4, 6, 9, 12]\n",
      "#train_interactions:  176784\n",
      "#test_interactions:  19592\n",
      "#train_session:  70260\n",
      "#test_session:  7766\n",
      "length_1: 5351\n",
      "length_2: 1496\n",
      "length_3: 521\n",
      "length_4: 240\n",
      "length_5: 158\n",
      "sequence average length:  2.5168020916104887\n",
      "dataset:  Cell_Phones_and_Accessories\n",
      "2022-09-28 15:34:59\n",
      "data formulation: id_seq, price_seq, cate_seq, price_list, cate_list, labs\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "datasets_name = 'Cell_Phones_and_Accessories'\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "\n",
    "price_num = '100'\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID', 'priceLevel', 'category']]\n",
    "\n",
    "# item_inter_num = pd.DataFrame(data.groupby(data['itemID']).count())\n",
    "# item_inter_num = item_inter_num.reset_index()[['sessionID', 'itemID']]\n",
    "# item_num=item_inter_num.rename(columns={'sessionID':'item_num'})\n",
    "# data = pd.merge(data, item_num, how='left', on = 'itemID')\n",
    "# data = data[data['item_num']>4]\n",
    "# data = data[['sessionID', 'itemID', 'priceLevel']]\n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "# dict (sessionID:[priceLevel, priceLevel])\n",
    "price_all = {}\n",
    "\n",
    "\n",
    "\n",
    "# dict (sessionID:[cate, cate])\n",
    "cate_all = {}\n",
    "# dict (sessionID:[brand, brand])\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    price = row['priceLevel']\n",
    "    cate = row['category']\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "        price_all[sess_id].append(price)\n",
    "        cate_all[sess_id].append(cate)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "        price_all[sess_id] = []\n",
    "        price_all[sess_id].append(price)\n",
    "        cate_all[sess_id] = []\n",
    "        cate_all[sess_id].append(cate)\n",
    "\n",
    "sess_total = data['sessionID'].max()\n",
    "split_num = int(sess_total/10*9)\n",
    "\n",
    "tra_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "tes_sess = dict() \n",
    "tra_price = dict()# dict(session_id:[price,price])\n",
    "tes_price = dict()\n",
    "tra_cate = dict()# dict(session_id:[cate,cate])\n",
    "tes_cate = dict()\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    all_price = price_all[sess_temp]\n",
    "    all_cate = cate_all[sess_temp]\n",
    "    if len(all_seqs) < 2:\n",
    "        continue\n",
    "    if len(all_seqs) > 20:\n",
    "        all_seqs = all_seqs[:20]\n",
    "        all_price = all_price[:20]\n",
    "        all_cate = all_cate[:20]\n",
    "    if int(sess_temp) < split_num:\n",
    "        tra_sess[sess_temp] = all_seqs\n",
    "        tra_price[sess_temp] = all_price\n",
    "        tra_cate[sess_temp] = all_cate\n",
    "    else:\n",
    "        tes_sess[sess_temp] = all_seqs\n",
    "        tes_price[sess_temp] = all_price\n",
    "        tes_cate[sess_temp] = all_cate\n",
    "\n",
    "# the number of item & price, renumber\n",
    "# item_ctr = 1\n",
    "# price_ctr = 1\n",
    "# cate_ctr = 1\n",
    "\n",
    "item_dict = {} #dict(old_itemID: new_itemID)\n",
    "cate_dict = {} #dict(old_cate: new_cate)\n",
    "price_dict = {} #dict(old_price: new_price)\n",
    "\n",
    "\n",
    "# item_id: price_level, item_id: rating\n",
    "item_price = {} #dict[new_itemID: priceLevel]\n",
    "item_cate = {} #dict[new_itemID: cate]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# tra_sess tra_price tra_cate\n",
    "# Convert training sessions to sequences and renumber items to start from 1\n",
    "def obtian_tra():\n",
    "#     global item_ctr\n",
    "#     print(item_ctr)\n",
    "    train_seqs = []\n",
    "    train_price = []\n",
    "    train_cate = []\n",
    "#     train_brand = []\n",
    "    item_ctr = 1\n",
    "    price_ctr = 1\n",
    "    cate_ctr = 1\n",
    "    for s in tra_sess:\n",
    "        seq = tra_sess[s]\n",
    "        price_seq = tra_price[s]\n",
    "        cate_seq = tra_cate[s]\n",
    "        outseq = []\n",
    "        pri_outseq = []\n",
    "#         rating_outseq = []\n",
    "        cate_outseq = []\n",
    "        for (i, j_pri, w_cate) in zip(seq, price_seq, cate_seq):\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "            else:\n",
    "                outseq += [item_ctr]\n",
    "                item_dict[i] = item_ctr\n",
    "                item_ctr += 1\n",
    "            if j_pri in price_dict:\n",
    "                pri_outseq += [price_dict[j_pri]]\n",
    "            else:\n",
    "                pri_outseq += [price_ctr]\n",
    "                price_dict[j_pri] = price_ctr\n",
    "                price_ctr += 1\n",
    "            if w_cate in cate_dict:\n",
    "                cate_outseq += [cate_dict[w_cate]]\n",
    "            else:\n",
    "                cate_outseq += [cate_ctr]\n",
    "                cate_dict[w_cate] = cate_ctr\n",
    "                cate_ctr += 1\n",
    "            if item_dict[i] not in item_price:\n",
    "                item_price[item_dict[i]] = price_dict[j_pri]\n",
    "                item_cate[item_dict[i]] = cate_dict[w_cate]\n",
    "        if len(outseq) < 2:  # Doesn't occur\n",
    "            print('session length is 1')\n",
    "            continue\n",
    "        train_seqs += [outseq]\n",
    "        train_price += [pri_outseq]\n",
    "        train_cate += [cate_outseq]\n",
    "    print(\"#train_session\",len(train_seqs))\n",
    "    print(\"#train_items\",item_ctr-1)\n",
    "    print(\"#train_price\",price_ctr-1)\n",
    "    print(\"#train_category\",cate_ctr-1)\n",
    "    return train_seqs, train_price, train_cate\n",
    "\n",
    "\n",
    "# Convert test sessions to sequences, ignoring items that do not appear in training set\n",
    "def obtian_tes():\n",
    "    test_seqs = []\n",
    "    test_price = []\n",
    "    test_cate = []\n",
    "    for s in tes_sess:\n",
    "        outseq = []\n",
    "        out_price = []\n",
    "        out_rating = []\n",
    "        out_cate = []\n",
    "        for i, j, w in zip(tes_sess[s], tes_price[s], tes_cate[s]):\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "                out_price += [price_dict[j]]\n",
    "                out_cate += [cate_dict[w]]\n",
    "        if len(outseq) < 2:\n",
    "            print('obtain test session length is 1')\n",
    "            continue\n",
    "        test_seqs += [outseq]\n",
    "        test_price += [out_price]\n",
    "        test_cate += [out_cate]\n",
    "    print(\"#test_session: \",len(test_seqs))\n",
    "    return test_seqs, test_price, test_cate\n",
    "\n",
    "# Convert test sessions to sequences, renumbering items that do not appear in training set\n",
    "\n",
    "\n",
    "# 数据增强\n",
    "def process_seqs(iseqs, iprice, icate):\n",
    "    out_seqs = []\n",
    "    out_price = []\n",
    "    out_cate = []\n",
    "    labs = []\n",
    "    max_length = 19\n",
    "    for seq, pri, rat, cat in zip(iseqs, iprice, icate):\n",
    "        for i in range(1, len(seq)):\n",
    "            tar = seq[-i]\n",
    "            labs += [tar]\n",
    "            out_seqs += [seq[:-i]]\n",
    "            out_price += [pri[:-i]]\n",
    "            out_cate += [cat[:-i]]\n",
    "    return out_seqs, out_price, out_cate, labs\n",
    "\n",
    "# 无数据增强\n",
    "def process_seqs_no(iseqs, iprice, icate):\n",
    "    print(\"no data augment\")\n",
    "    out_seqs = []\n",
    "    out_price = []\n",
    "    out_cate = []\n",
    "    labs = []\n",
    "    max_length = 19\n",
    "    for seq, pri, cat in zip(iseqs, iprice, icate):     \n",
    "        labs += [seq[-1]]\n",
    "        out_seqs += [seq[:-1]]\n",
    "        out_price += [pri[:-1]]\n",
    "        out_cate += [cat[:-1]]\n",
    "    return out_seqs, out_price, out_cate, labs\n",
    "\n",
    "tra_seqs, tra_pri, tra_cat  = obtian_tra()\n",
    "tes_seqs, tes_pri, tes_cat  = obtian_tes()\n",
    "# save item_dict:(old_itemID: new_itemID) nocold\n",
    "old2new_nocold_path = './dict/' + datasets_name + '/old2newNocold.csv'\n",
    "old_list_nocold = list(item_dict.keys())\n",
    "new_list_nocold = list(item_dict.values())\n",
    "old2new_nocold_dict = {'old':old_list_nocold, 'new':new_list_nocold}\n",
    "data_old_new_no = pd.DataFrame(old2new_nocold_dict)\n",
    "data_old_new_no.to_csv(old2new_nocold_path)\n",
    "\n",
    "\n",
    "\n",
    "tr_seqs, tr_pri, tr_cat, tr_labs = process_seqs_no(tra_seqs, tra_pri, tra_cat)\n",
    "te_seqs, te_pri, te_cat, te_labs = process_seqs_no(tes_seqs, tes_pri, tes_cat)\n",
    "\n",
    "te_seqs1 = []\n",
    "te_seqs2 = []\n",
    "te_seqs3 = []\n",
    "te_seqs4 = []\n",
    "te_seqs5 = []\n",
    "\n",
    "te_pri1 = []\n",
    "te_pri2 = []\n",
    "te_pri3 = []\n",
    "te_pri4 = []\n",
    "te_pri5 = []\n",
    "\n",
    "te_cat1 = []\n",
    "te_cat2 = []\n",
    "te_cat3 = []\n",
    "te_cat4 = []\n",
    "te_cat5 = []\n",
    "\n",
    "te_labs1 = []\n",
    "te_labs2 = []\n",
    "te_labs3 = []\n",
    "te_labs4 = []\n",
    "te_labs5 = []\n",
    "\n",
    "num_1 = 0\n",
    "num_2 = 0\n",
    "num_3 = 0\n",
    "num_4 = 0\n",
    "num_5 = 0\n",
    "\n",
    "for i_sess, j_pri, k_cat, l_lab in zip(te_seqs, te_pri, te_cat, te_labs):\n",
    "    if len(i_sess) == 1:\n",
    "        te_seqs1 += [i_sess]\n",
    "        te_pri1 += [j_pri]\n",
    "        te_cat1 += [k_cat]\n",
    "        te_labs1 += [l_lab]\n",
    "        num_1 += 1\n",
    "    elif len(i_sess) == 2:\n",
    "        te_seqs2 += [i_sess]\n",
    "        te_pri2 += [j_pri]\n",
    "        te_cat2 += [k_cat]\n",
    "        te_labs2 += [l_lab]\n",
    "        num_2 += 1\n",
    "    elif len(i_sess) == 3:\n",
    "        te_seqs3 += [i_sess]\n",
    "        te_pri3 += [j_pri]\n",
    "        te_cat3 += [k_cat]\n",
    "        te_labs3 += [l_lab]\n",
    "        num_3 += 1\n",
    "    elif len(i_sess) == 4:\n",
    "        te_seqs4 += [i_sess]\n",
    "        te_pri4 += [j_pri]\n",
    "        te_cat4 += [k_cat]\n",
    "        te_labs4 += [l_lab]\n",
    "        num_4 += 1\n",
    "    else:\n",
    "        te_seqs5 += [i_sess]\n",
    "        te_pri5 += [j_pri]\n",
    "        te_cat5 += [k_cat]\n",
    "        te_labs5 += [l_lab]\n",
    "        num_5 += 1\n",
    "\n",
    "# 冷启动情况下，两个数据集\n",
    "# te_seqs_nocold, te_pri_nocold, te_labs_nocold = process_seqs_no(tes_seqs_nocold, tes_pri_nocold)\n",
    "# te_seqs_cold, te_pri_cold, te_labs_cold = process_seqs_no(tes_seqs_cold, tes_pri_cold)\n",
    "\n",
    "print('train sequence: ',tr_seqs[:5])\n",
    "print('train price: ',tr_pri[:5])\n",
    "print('train category: ',tr_cat[:5])\n",
    "print('train lab: ',tr_labs[:5])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 增加 all_item_price_list & all_item_rating_list\n",
    "\n",
    "ip_id_list = list(item_price.keys())\n",
    "ip_price_list = list(item_price.values())\n",
    "id_price_dict = {'id':ip_id_list, 'price':ip_price_list}\n",
    "id_price_dict = pd.DataFrame(id_price_dict)\n",
    "id_price_dict = id_price_dict.reset_index()[['id', 'price']]\n",
    "\n",
    "id_price_dict.sort_values(by=[\"id\"],inplace=True,ascending=[True])\n",
    "price_list_save = id_price_dict['price'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "ic_id_list = list(item_cate.keys())\n",
    "ic_cate_list = list(item_cate.values())\n",
    "id_cate_dict = {'id':ic_id_list, 'cate':ic_cate_list}\n",
    "id_cate_dict = pd.DataFrame(id_cate_dict)\n",
    "id_cate_dict = id_cate_dict.reset_index()[['id', 'cate']]\n",
    "\n",
    "id_cate_dict.sort_values(by=[\"id\"],inplace=True,ascending=[True])\n",
    "cate_list_save = id_cate_dict['cate'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "tra = (tr_seqs, tr_pri, tr_cat, price_list_save, cate_list_save, tr_labs)\n",
    "tes = (te_seqs, te_pri, te_cat, price_list_save, cate_list_save, te_labs)\n",
    "\n",
    "tes1 = (te_seqs1, te_pri1, te_cat1, price_list_save, cate_list_save, te_labs1)\n",
    "tes2 = (te_seqs2, te_pri2, te_cat2, price_list_save, cate_list_save, te_labs2)\n",
    "tes3 = (te_seqs3, te_pri3, te_cat3, price_list_save, cate_list_save, te_labs3)\n",
    "tes4 = (te_seqs4, te_pri4, te_cat4, price_list_save, cate_list_save, te_labs4)\n",
    "tes5 = (te_seqs5, te_pri5, te_cat5, price_list_save, cate_list_save, te_labs5)\n",
    "\n",
    "# print(len(tra[3]))\n",
    "all_train = 0\n",
    "all_tes = 0\n",
    "# all_tes_nocold = 0\n",
    "# all_tes_cold = 0\n",
    "for seq in tra_seqs:\n",
    "    all_train += len(seq)\n",
    "for seq in tes_seqs:\n",
    "    all_tes += len(seq)\n",
    "# for seq in tes_seqs_nocold:\n",
    "#     all_tes_nocold += len(seq)\n",
    "# for seq in tes_seqs_cold:\n",
    "#     all_tes_cold += len(seq)\n",
    "print('#train_interactions: ',all_train)\n",
    "print('#test_interactions: ',all_tes)\n",
    "# print('#nocold_test_interactions: ',all_tes_nocold)\n",
    "# print('#cold_test_interactions: ',all_tes_cold)\n",
    "\n",
    "print('#train_session: ',(len(tra_seqs)))\n",
    "print('#test_session: ',(len(tes_seqs)))\n",
    "print('length_1:',str(num_1))\n",
    "print('length_2:',str(num_2))\n",
    "print('length_3:',str(num_3))\n",
    "print('length_4:',str(num_4))\n",
    "print('length_5:',str(num_5))\n",
    "# print('#nocold_test_session: ',(len(tes_seqs_nocold)))\n",
    "# print('#cold_test_session: ',(len(tes_seqs_cold)))\n",
    "\n",
    "print('sequence average length: ', (all_train+all_tes)/(len(tra_seqs) + len(tes_seqs) * 1.0))\n",
    "# print('cold sequence average length: ', (all_train+all_tes_nocold+all_tes_cold)/(len(tra_seqs) + len(tes_seqs_nocold) +len(tes_seqs_cold) * 1.0))\n",
    "train_data_path = './datasets/' + datasets_name + str(price_num) + 'length'\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "path_data_train = train_data_path + \"/train.txt\"\n",
    "path_normal_test = train_data_path + \"/test.txt\"\n",
    "path_test1 = train_data_path + \"/test1.txt\"\n",
    "path_test2 = train_data_path + \"/test2.txt\"\n",
    "path_test3 = train_data_path + \"/test3.txt\"\n",
    "path_test4 = train_data_path + \"/test4.txt\"\n",
    "path_test5 = train_data_path + \"/test5.txt\"\n",
    "# path_nocold_test = train_data_path + \"/nocoldtest.txt\"\n",
    "# path_cold_test = train_data_path + \"/coldtest.txt\"\n",
    "# path_data_pc = train_data_path + \"/itemlistPCB.txt\"\n",
    "\n",
    "pickle.dump(tra, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes, open(path_normal_test, 'wb'))\n",
    "pickle.dump(tes1, open(path_test1, 'wb'))\n",
    "pickle.dump(tes2, open(path_test2, 'wb'))\n",
    "pickle.dump(tes3, open(path_test3, 'wb'))\n",
    "pickle.dump(tes4, open(path_test4, 'wb'))\n",
    "pickle.dump(tes5, open(path_test5, 'wb'))\n",
    "\n",
    "# pickle.dump(tes_nocold, open(path_nocold_test, 'wb'))\n",
    "# pickle.dump(tes_cold, open(path_cold_test, 'wb'))\n",
    "# pickle.dump(price_cate_list, open(path_data_pc, 'wb'))\n",
    "\n",
    "\n",
    "# get asin item_id 获得asin list 有顺序, 给处理 img及text\n",
    "asin2old_path = 'dict/' + datasets_name +'/asin2itemID.csv'\n",
    "old2new_path = 'dict/' + datasets_name +'/old2newNocold.csv'\n",
    "# id_img_emb = 'dict/' + datasets_name +'/id_imgEmbedding.csv'\n",
    "# id_text_emb = 'dict/' + datasets_name +'/id_textEmbedding.csv'\n",
    "\n",
    "# text_matrix_path = 'dict/' + datasets_name +'/text_matrix.npy'\n",
    "# image_matrix_path = 'dict/' + datasets_name +'/image_matrix.npy'\n",
    "\n",
    "\n",
    "asin2old = pd.read_csv(asin2old_path)\n",
    "asin2old = asin2old.rename(columns={ 'itemID':'old'})\n",
    "asin2old = asin2old[['asin', 'old']]\n",
    "\n",
    "\n",
    "old2new = pd.read_csv(old2new_path)\n",
    "old2new = old2new[['old', 'new']]\n",
    "\n",
    "\n",
    "asin2new = pd.merge(asin2old, old2new, how='left', on = 'old')\n",
    "\n",
    "\n",
    "asin2new = asin2new.dropna(axis=0)\n",
    "asin2ItemID = asin2new.reset_index()[['asin', 'new']]\n",
    "\n",
    "asin2ItemID.sort_values(by=[\"new\"],inplace=True,ascending=[True])\n",
    "\n",
    "\n",
    "asin_list = asin2ItemID['asin'].tolist()\n",
    "# new_list = asin2ItemID['new'].tolist()\n",
    "asin_list_save = train_data_path + '/asinlist.npy'\n",
    "np.save(asin_list_save, asin_list)\n",
    "\n",
    "\n",
    "\n",
    "print(\"dataset: \", datasets_name)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"data formulation: id_seq, price_seq, cate_seq, price_list, cate_list, labs\")\n",
    "print(\"done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NARM, S-POP, SR-GNN, DHCN 使用相同的 train&test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train_items 9151\n",
      "train sequence:  89983\n",
      "test sequence:  10031\n",
      "train sample:  [[1, 2, 3], [4], [6]] [3, 5, 7]\n",
      "test sample:  [[3686], [1102, 5865], [7243, 1682]] [2915, 2429, 1870]\n",
      "sequence average length:  2.866678664986902\n",
      "dataset:  Office_Products\n",
      "2022-09-12 08:52:57\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "datasets_name = 'Office_Products'\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "# data_path = 'digineticaBuy/digineticaBuy_data.csv'\n",
    "\n",
    "# datasets_name = '2019-Oct'\n",
    "# price_num = 5\n",
    "\n",
    "# data_path = 'cosmeticsdata/' + datasets_name + '_data.csv'\n",
    "# datasets_name = 'digineticaBuy'\n",
    "# data_path = 'digineticaBuy/' + datasets_name + '_data.csv'\n",
    "\n",
    "\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID']]\n",
    "\n",
    "# item_inter_num = pd.DataFrame(data.groupby(data['itemID']).count())\n",
    "# item_num=item_inter_num.reset_index().rename(columns={'sessionID':'item_num'})\n",
    "# data = pd.merge(data, item_num, how='left', on = 'itemID')\n",
    "# data = data[data['item_num']>9]\n",
    "# data = data[['sessionID', 'itemID']]\n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "\n",
    "sess_total = data['sessionID'].max()\n",
    "split_num = int(sess_total/10*9)\n",
    "\n",
    "tra_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "tes_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if len(all_seqs) < 2:\n",
    "        continue\n",
    "    if len(all_seqs) > 20:\n",
    "        all_seqs = all_seqs[:20]\n",
    "    if int(sess_temp) < split_num:\n",
    "        tra_sess[sess_temp] = all_seqs\n",
    "    else:\n",
    "        tes_sess[sess_temp] = all_seqs\n",
    "\n",
    "item_dict = {}#dict(old_itemID: new_itemID)\n",
    "# Convert training sessions to sequences and renumber items to start from 1\n",
    "def obtian_tra():\n",
    "    train_seqs = []\n",
    "    item_ctr = 1\n",
    "    for s in tra_sess:\n",
    "        seq = tra_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "            else:\n",
    "                outseq += [item_ctr]\n",
    "                item_dict[i] = item_ctr\n",
    "                item_ctr += 1\n",
    "        if len(outseq) < 2:  # Doesn't occur\n",
    "            continue\n",
    "        train_seqs += [outseq]\n",
    "    print(\"#train_items\",item_ctr)    \n",
    "    return train_seqs\n",
    "\n",
    "\n",
    "# Convert test sessions to sequences, ignoring items that do not appear in training set\n",
    "def obtian_tes():\n",
    "    test_seqs = []\n",
    "    for s in tes_sess:\n",
    "        seq = tes_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "        if len(outseq) < 2:\n",
    "            continue\n",
    "        test_seqs += [outseq]\n",
    "    return test_seqs\n",
    "\n",
    "# 数据增强\n",
    "def process_seqs(iseqs):\n",
    "    out_seqs = []\n",
    "    labs = []\n",
    "    for seq in iseqs:\n",
    "        for i in range(1, len(seq)):\n",
    "            tar = seq[-i]\n",
    "            labs += [tar]\n",
    "            out_seqs += [seq[:-i]]\n",
    "    return out_seqs, labs\n",
    "\n",
    "# 无数据增强\n",
    "def process_seqs_no(iseqs):\n",
    "    out_seqs = []\n",
    "    labs = []\n",
    "    for seq in iseqs:\n",
    "        labs += [seq[-1]]\n",
    "        out_seqs += [seq[:-1]]\n",
    "    return out_seqs, labs\n",
    "\n",
    "\n",
    "tra_seqs = obtian_tra()\n",
    "tes_seqs = obtian_tes()\n",
    "\n",
    "tr_seqs, tr_labs = process_seqs_no(tra_seqs)\n",
    "te_seqs, te_labs = process_seqs_no(tes_seqs)\n",
    "\n",
    "tra = (tr_seqs, tr_labs)\n",
    "tes = (te_seqs, te_labs)\n",
    "print(\"train sequence: \", len(tr_seqs))\n",
    "print(\"test sequence: \", len(te_seqs))\n",
    "print(\"train sample: \", tr_seqs[:3], tr_labs[:3])\n",
    "print(\"test sample: \", te_seqs[:3], te_labs[:3])\n",
    "all = 0\n",
    "\n",
    "for seq in tra_seqs:\n",
    "    all += len(seq)\n",
    "for seq in tes_seqs:\n",
    "    all += len(seq)\n",
    "print('sequence average length: ', all/(len(tra_seqs) + len(tes_seqs) * 1.0))\n",
    "\n",
    "train_data_path = 'NARM/' + datasets_name\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "path_data_train = train_data_path + \"/train.txt\"\n",
    "path_data_test = train_data_path + \"/test.txt\"\n",
    "\n",
    "pickle.dump(tra, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes, open(path_data_test, 'wb'))\n",
    "print(\"dataset: \", datasets_name)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train_items 9151\n",
      "#train_category 528\n",
      "train sequence:  89983\n",
      "test sequence:  10031\n",
      "train sample:  [[1, 2, 3], [4], [6]] [3, 5, 7]\n",
      "test sample:  [[3686], [1102, 5865], [7243, 1682]] [2915, 2429, 1870]\n",
      "sequence average length:  2.866678664986902\n",
      "dataset:  Office_Products\n",
      "2022-09-12 08:53:35\n",
      "model: MGS\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "datasets_name = 'Office_Products'\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "# data_path = 'digineticaBuy/digineticaBuy_data.csv'\n",
    "\n",
    "# datasets_name = '2019-Oct'\n",
    "# price_num = 5\n",
    "\n",
    "# data_path = 'cosmeticsdata/' + datasets_name + '_data.csv'\n",
    "# datasets_name = 'digineticaBuy'\n",
    "# data_path = 'digineticaBuy/' + datasets_name + '_data.csv'\n",
    "\n",
    "\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID', 'category']]\n",
    "\n",
    "# item_inter_num = pd.DataFrame(data.groupby(data['itemID']).count())\n",
    "# item_num=item_inter_num.reset_index().rename(columns={'sessionID':'item_num'})\n",
    "# data = pd.merge(data, item_num, how='left', on = 'itemID')\n",
    "# data = data[data['item_num']>9]\n",
    "# data = data[['sessionID', 'itemID']]\n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "# dict (sessionID:[cate, cate])\n",
    "cate_all = {}\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    cate = row['category']\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "        cate_all[sess_id].append(cate)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "        cate_all[sess_id] = []\n",
    "        cate_all[sess_id].append(cate)\n",
    "\n",
    "sess_total = data['sessionID'].max()\n",
    "split_num = int(sess_total/10*9)\n",
    "\n",
    "tra_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "tes_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "tra_cate = dict()# dict(session_id:[cate,cate])\n",
    "tes_cate = dict()\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    all_cate = cate_all[sess_temp]\n",
    "    if len(all_seqs) < 2:\n",
    "        continue\n",
    "    if len(all_seqs) > 20:\n",
    "        all_seqs = all_seqs[:20]\n",
    "        all_cate = all_cate[:20]\n",
    "    if int(sess_temp) < split_num:\n",
    "        tra_sess[sess_temp] = all_seqs\n",
    "        tra_cate[sess_temp] = all_cate\n",
    "    else:\n",
    "        tes_sess[sess_temp] = all_seqs\n",
    "        tes_cate[sess_temp] = all_cate\n",
    "\n",
    "item_dict = {}#dict(old_itemID: new_itemID)\n",
    "cate_dict = {} #dict(old_cate: new_cate)\n",
    "\n",
    "\n",
    "# Convert training sessions to sequences and renumber items to start from 1\n",
    "def obtian_tra():\n",
    "    train_seqs = []\n",
    "    train_cate = []\n",
    "    item_ctr = 1\n",
    "    cate_ctr = 1\n",
    "    for s in tra_sess:\n",
    "        seq = tra_sess[s]\n",
    "        cate_seq = tra_cate[s]\n",
    "        outseq = []\n",
    "        cate_outseq = []\n",
    "        for (i, w_cate) in zip(seq, cate_seq):\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "            else:\n",
    "                outseq += [item_ctr]\n",
    "                item_dict[i] = item_ctr\n",
    "                item_ctr += 1\n",
    "            if w_cate in cate_dict:\n",
    "                cate_outseq += [cate_dict[w_cate]]\n",
    "            else:\n",
    "                cate_outseq += [cate_ctr]\n",
    "                cate_dict[w_cate] = cate_ctr\n",
    "                cate_ctr += 1\n",
    "        if len(outseq) < 2:  # Doesn't occur\n",
    "            continue\n",
    "        train_seqs += [outseq]\n",
    "        train_cate += [cate_outseq]\n",
    "    print(\"#train_items\",item_ctr)\n",
    "    print(\"#train_category\",cate_ctr)\n",
    "    return train_seqs, train_cate\n",
    "\n",
    "\n",
    "# Convert test sessions to sequences, ignoring items that do not appear in training set\n",
    "def obtian_tes():\n",
    "    test_seqs = []\n",
    "    for s in tes_sess:\n",
    "        seq = tes_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "        if len(outseq) < 2:\n",
    "            continue\n",
    "        test_seqs += [outseq]\n",
    "    return test_seqs\n",
    "\n",
    "# 数据增强\n",
    "def process_seqs(iseqs):\n",
    "    out_seqs = []\n",
    "    labs = []\n",
    "    for seq in iseqs:\n",
    "        for i in range(1, len(seq)):\n",
    "            tar = seq[-i]\n",
    "            labs += [tar]\n",
    "            out_seqs += [seq[:-i]]\n",
    "    return out_seqs, labs\n",
    "\n",
    "# 无数据增强\n",
    "def process_seqs_no(iseqs):\n",
    "    out_seqs = []\n",
    "    labs = []\n",
    "    for seq in iseqs:\n",
    "        labs += [seq[-1]]\n",
    "        out_seqs += [seq[:-1]]\n",
    "    return out_seqs, labs\n",
    "\n",
    "\n",
    "tra_seqs,train_cate = obtian_tra()\n",
    "tes_seqs = obtian_tes()\n",
    "\n",
    "#  形成 json about cate\n",
    "item_cate = {} #dict[new_itemID: new_cate]\n",
    "cate_itemlist = {} #dict[new_cate: [new_itemID,new_itemID]]\n",
    "for (item_seq, cate_seq) in zip(tra_seqs, train_cate):\n",
    "    for (x_item, y_cate) in zip(item_seq,cate_seq):\n",
    "        if x_item not in item_cate:\n",
    "            item_cate[x_item] = y_cate\n",
    "        if y_cate not in cate_itemlist:\n",
    "            cate_itemlist[y_cate] = []\n",
    "            cate_itemlist[y_cate].append(x_item)\n",
    "        else:\n",
    "            if x_item not in cate_itemlist[y_cate]:\n",
    "                cate_itemlist[y_cate].append(x_item)\n",
    "\n",
    "dict_results =dict()\n",
    "for x in item_cate:\n",
    "    x_str = x\n",
    "    if x_str not in dict_results:\n",
    "        dict_results[x_str] = {\"category\":item_cate[x_str], \"same_cate\":cate_itemlist[item_cate[x_str]]}\n",
    "\n",
    "tr_seqs, tr_labs = process_seqs_no(tra_seqs)\n",
    "te_seqs, te_labs = process_seqs_no(tes_seqs)\n",
    "\n",
    "tra = (tr_seqs, tr_labs)\n",
    "tes = (te_seqs, te_labs)\n",
    "print(\"train sequence: \", len(tr_seqs))\n",
    "print(\"test sequence: \", len(te_seqs))\n",
    "print(\"train sample: \", tr_seqs[:3], tr_labs[:3])\n",
    "print(\"test sample: \", te_seqs[:3], te_labs[:3])\n",
    "all = 0\n",
    "\n",
    "for seq in tra_seqs:\n",
    "    all += len(seq)\n",
    "for seq in tes_seqs:\n",
    "    all += len(seq)\n",
    "print('sequence average length: ', all/(len(tra_seqs) + len(tes_seqs) * 1.0))\n",
    "\n",
    "train_data_path = 'MGS/' + datasets_name\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "path_data_train = train_data_path + \"/train.txt\"\n",
    "path_data_test = train_data_path + \"/test.txt\"\n",
    "path_json = train_data_path + \"/product_attributes.json\"\n",
    "\n",
    "pickle.dump(tra, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes, open(path_data_test, 'wb'))\n",
    "with open(path_json, \"w\") as file_object:\n",
    "    json.dump(dict_results, file_object)\n",
    "print(\"dataset: \", datasets_name)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"model:\",\"MGS\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UniSRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train_items 9150\n",
      "train sequence:  89983\n",
      "test sequence:  10031\n",
      "train sample:  [[0, 1, 2], [3], [5]] [2, 4, 6]\n",
      "test sample:  [[3685], [1101, 5864], [7242, 1681]] [2914, 2428, 1869]\n",
      "sequence average length:  2.866678664986902\n",
      "dataset:  Office_Products\n",
      "model: UniSRec\n",
      "2022-09-12 08:55:19\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "datasets_name = 'Office_Products'\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "# data_path = 'digineticaBuy/digineticaBuy_data.csv'\n",
    "\n",
    "# datasets_name = '2019-Oct'\n",
    "# price_num = 5\n",
    "\n",
    "# data_path = 'cosmeticsdata/' + datasets_name + '_data.csv'\n",
    "# datasets_name = 'digineticaBuy'\n",
    "# data_path = 'digineticaBuy/' + datasets_name + '_data.csv'\n",
    "\n",
    "\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID']]\n",
    "\n",
    "# item_inter_num = pd.DataFrame(data.groupby(data['itemID']).count())\n",
    "# item_num=item_inter_num.reset_index().rename(columns={'sessionID':'item_num'})\n",
    "# data = pd.merge(data, item_num, how='left', on = 'itemID')\n",
    "# data = data[data['item_num']>9]\n",
    "# data = data[['sessionID', 'itemID']]\n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "\n",
    "sess_total = data['sessionID'].max()\n",
    "split_num = int(sess_total/10*9)\n",
    "\n",
    "tra_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "tes_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if len(all_seqs) < 2:\n",
    "        continue\n",
    "    if len(all_seqs) > 20:\n",
    "        all_seqs = all_seqs[:20]\n",
    "    if int(sess_temp) < split_num:\n",
    "        tra_sess[sess_temp] = all_seqs\n",
    "    else:\n",
    "        tes_sess[sess_temp] = all_seqs\n",
    "\n",
    "item_dict = {}#dict(old_itemID: new_itemID)\n",
    "# Convert training sessions to sequences and renumber items to start from 1\n",
    "def obtian_tra():\n",
    "    train_seqs = []\n",
    "#   UniSRec item 0 序\n",
    "    item_ctr = 0\n",
    "    for s in tra_sess:\n",
    "        seq = tra_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "            else:\n",
    "                outseq += [item_ctr]\n",
    "                item_dict[i] = item_ctr\n",
    "                item_ctr += 1\n",
    "        if len(outseq) < 2:  # Doesn't occur\n",
    "            continue\n",
    "        train_seqs += [outseq]\n",
    "    print(\"#train_items\",item_ctr)    \n",
    "    return train_seqs\n",
    "\n",
    "\n",
    "# Convert test sessions to sequences, ignoring items that do not appear in training set\n",
    "def obtian_tes():\n",
    "    test_seqs = []\n",
    "    for s in tes_sess:\n",
    "        seq = tes_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "        if len(outseq) < 2:\n",
    "            continue\n",
    "        test_seqs += [outseq]\n",
    "    return test_seqs\n",
    "\n",
    "# 数据增强\n",
    "def process_seqs(iseqs):\n",
    "    out_seqs = []\n",
    "    labs = []\n",
    "    for seq in iseqs:\n",
    "        for i in range(1, len(seq)):\n",
    "            tar = seq[-i]\n",
    "            labs += [tar]\n",
    "            out_seqs += [seq[:-i]]\n",
    "    return out_seqs, labs\n",
    "\n",
    "# 无数据增强\n",
    "def process_seqs_no(iseqs):\n",
    "    out_seqs = []\n",
    "    labs = []\n",
    "    for seq in iseqs:\n",
    "        labs += [seq[-1]]\n",
    "        out_seqs += [seq[:-1]]\n",
    "    return out_seqs, labs\n",
    "\n",
    "\n",
    "tra_seqs = obtian_tra()\n",
    "tes_seqs = obtian_tes()\n",
    "\n",
    "tr_seqs, tr_labs = process_seqs_no(tra_seqs)\n",
    "te_seqs, te_labs = process_seqs_no(tes_seqs)\n",
    "\n",
    "# tra = (tr_seqs, tr_labs)\n",
    "# tes = (te_seqs, te_labs)\n",
    "print(\"train sequence: \", len(tr_seqs))\n",
    "print(\"test sequence: \", len(te_seqs))\n",
    "print(\"train sample: \", tr_seqs[:3], tr_labs[:3])\n",
    "print(\"test sample: \", te_seqs[:3], te_labs[:3])\n",
    "all = 0\n",
    "\n",
    "for seq in tra_seqs:\n",
    "    all += len(seq)\n",
    "for seq in tes_seqs:\n",
    "    all += len(seq)\n",
    "print('sequence average length: ', all/(len(tra_seqs) + len(tes_seqs) * 1.0))\n",
    "\n",
    "train_data_path = 'UniSRec/' + datasets_name + '/'\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "    \n",
    "path_data_train = train_data_path + f'{datasets_name}.train.inter'\n",
    "path_data_valid = train_data_path + f'{datasets_name}.valid.inter'\n",
    "path_data_test = train_data_path + f'{datasets_name}.test.inter'\n",
    "\n",
    "split_valid = int(len(tr_labs)/9*7)\n",
    "train_sess = tr_seqs[:split_valid]\n",
    "train_labs = tr_labs[:split_valid]\n",
    "valid_sess = tr_seqs[split_valid:]\n",
    "valid_labs = tr_labs[split_valid:]\n",
    "\n",
    "with open(path_data_train, 'w') as file:\n",
    "    file.write('user_id:token\\titem_id_list:token_seq\\titem_id:token\\n')\n",
    "    for index, sess, lab in zip(range(len(train_labs)),train_sess, train_labs):\n",
    "        i_str = str(index)\n",
    "        s_str = [str(x) for x in sess]\n",
    "        l_str = str(lab)\n",
    "        file.write(f'{i_str}\\t{\" \".join(s_str)}\\t{l_str}\\n')\n",
    "                   \n",
    "with open(path_data_valid, 'w') as file:\n",
    "    file.write('user_id:token\\titem_id_list:token_seq\\titem_id:token\\n')\n",
    "    for index, sess, lab in zip(range(len(valid_labs)),valid_sess, valid_labs):\n",
    "        i_str = str(index)\n",
    "        s_str = [str(x) for x in sess]\n",
    "        l_str = str(lab)\n",
    "        file.write(f'{i_str}\\t{\" \".join(s_str)}\\t{l_str}\\n')\n",
    "                   \n",
    "with open(path_data_test, 'w') as file:\n",
    "    file.write('user_id:token\\titem_id_list:token_seq\\titem_id:token\\n')\n",
    "    for index, sess, lab in zip(range(len(te_labs)),te_seqs, te_labs):\n",
    "        i_str = str(index)\n",
    "        s_str = [str(x) for x in sess]\n",
    "        l_str = str(lab)\n",
    "        file.write(f'{i_str}\\t{\" \".join(s_str)}\\t{l_str}\\n')\n",
    "                   \n",
    "print(\"dataset: \", datasets_name)\n",
    "print(\"model: UniSRec\")\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSLREC - senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "datasets_name = 'Cell_Phones_and_Accessories'\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "# data_path = 'digineticaBuy/digineticaBuy_data.csv'\n",
    "\n",
    "# datasets_name = '2019-Oct'\n",
    "# price_num = 5\n",
    "\n",
    "# data_path = 'cosmeticsdata/' + datasets_name + '_data.csv'\n",
    "# datasets_name = 'digineticaBuy'\n",
    "# data_path = 'digineticaBuy/' + datasets_name + '_data.csv'\n",
    "\n",
    "\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID']]\n",
    "\n",
    "item_count = 0\n",
    "user_count = 0\n",
    "inter_count = 0\n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "#  dict {old_itemID: title}\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "\n",
    "# 限制每个seq长度\n",
    "for sess_temp in sess_all.keys():\n",
    "    seq = sess_all[sess_temp]\n",
    "    if len(seq) >20:\n",
    "        sess_all[sess_temp] = seq[-20:]\n",
    "        \n",
    "item_num = 1\n",
    "# dict{lod_itemID:new_itemID}\n",
    "item_dict = {}\n",
    "# train_set [[u1, u2],[x1, x2, x3], [label...]\n",
    "tra = [[],[],[]]\n",
    "# dict {train_sessionID:[itemID,itemID]}  去掉了seq中last item(不包含test的label)， 也可以调整为去掉两个item\n",
    "train_sess = {}\n",
    "\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    train_seqs_t = all_seqs[:-1]\n",
    "    inter_count += len(train_seqs_t)\n",
    "#     item_seq with new ID\n",
    "    new_seq = []\n",
    "    for x in train_seqs_t:\n",
    "        if x in item_dict:\n",
    "            new_seq.append(item_dict[x])\n",
    "        else:\n",
    "            item_dict[x] = item_num\n",
    "            new_seq.append(item_num)            \n",
    "            item_num += 1\n",
    "    train_sess[sess_temp] = new_seq\n",
    "    tra[0].append(sess_temp)\n",
    "    tra[1].append(new_seq[:-1])\n",
    "#     预测时考虑last item\n",
    "    tra[2].append(new_seq[-1])\n",
    "    \n",
    "item_count =  item_num-1\n",
    "user_count = len(tra[0])\n",
    "\n",
    "# test_set [[x_id ...],[label...]\n",
    "tes = [[],[],[]]\n",
    "tes_seq_num = 0\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if all_seqs[-1] in item_dict:\n",
    "        new_seq = []\n",
    "        for x in all_seqs:\n",
    "            new_seq.append(item_dict[x])\n",
    "        tes_seq_num += 1\n",
    "        tes[0].append(sess_temp)\n",
    "        tes[1].append(new_seq[:-1])\n",
    "    #     预测时考虑last item\n",
    "        tes[2].append(new_seq[-1])\n",
    "    \n",
    "\n",
    "print('item_num: ',str(item_count))\n",
    "print('train_seq_num & user_num: ',str(user_count))\n",
    "print('test_seq_num: ',str(tes_seq_num))\n",
    "\n",
    "inter_count += tes_seq_num\n",
    "\n",
    "\n",
    "print('#interactions: ',inter_count)\n",
    "\n",
    "print('sequence average length: ', (inter_count)/(user_count) * 1.0)\n",
    "\n",
    "train_data_path = 'SSLRecFine/' + datasets_name + 'Fine/'\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "\n",
    "\n",
    "path_data_train = train_data_path + 'train.tsv'\n",
    "# path_data_valid = train_data_path + f'{datasets_name_c}.valid.inter'\n",
    "path_data_test = train_data_path + 'test.tsv'\n",
    "# train_sessid_list = range(len(tr_labs))\n",
    "with open(path_data_train, 'w') as file:\n",
    "    file.write('session_id\\t item_id_seq\\t item_id \\n')\n",
    "    for index, sess, lab in zip(tra[0],tra[1], tra[2]):\n",
    "        i_str = str(index-1)\n",
    "        s_str = [str(x) for x in sess]\n",
    "        l_str = str(lab)\n",
    "        file.write(f'{i_str}\\t{\" \".join(s_str)}\\t{l_str}\\n')\n",
    "\n",
    "\n",
    "with open(path_data_test, 'w') as file:\n",
    "    file.write('session_id\\t item_id_seq\\t item_id \\n')\n",
    "    for index, sess, lab in zip(tes[0],tes[1], tes[2]):\n",
    "        i_str = str(index-1)\n",
    "        s_str = [str(x) for x in sess]\n",
    "        l_str = str(lab)\n",
    "        file.write(f'{i_str}\\t{\" \".join(s_str)}\\t{l_str}\\n')\n",
    "\n",
    "\n",
    "# train_sessid_list = range(len(tr_labs))\n",
    "# train_seq_list = tr_seqs\n",
    "# train_lab_list = tr_labs\n",
    "# train_pd = pd.DataFrame({'session_id':train_sessid_list, 'item_id_seq':train_seq_list, 'item_id':train_lab_list})\n",
    "\n",
    "# test_sessid_list = range(len(te_labs))\n",
    "# test_seq_list = te_seqs\n",
    "# test_lab_list = te_labs\n",
    "# test_pd = pd.DataFrame({'session_id':test_sessid_list, 'item_id_seq':test_seq_list, 'item_id':test_lab_list})\n",
    "\n",
    "# train_pd.to_csv(path_data_train, sep = '\\t', index=False)\n",
    "# test_pd.to_csv(path_data_test, sep = '\\t', index=False)\n",
    "\n",
    "print(\"dataset: \", datasets_name)\n",
    "print(\"model: SSLRec\")\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train_items 8615\n",
      "sequence average length:  2.5168020916104887\n",
      "split sess id:  70260\n",
      "dataset:  Cell_Phones_and_Accessories\n",
      "model: MML\n",
      "2023-06-27 18:35:50\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "datasets_name = 'Cell_Phones_and_Accessories'\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "# data_path = 'digineticaBuy/digineticaBuy_data.csv'\n",
    "\n",
    "# datasets_name = '2019-Oct'\n",
    "# price_num = 5\n",
    "\n",
    "# data_path = 'cosmeticsdata/' + datasets_name + '_data.csv'\n",
    "# datasets_name = 'digineticaBuy'\n",
    "# data_path = 'digineticaBuy/' + datasets_name + '_data.csv'\n",
    "\n",
    "\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID']]\n",
    "\n",
    "# item_inter_num = pd.DataFrame(data.groupby(data['itemID']).count())\n",
    "# item_num=item_inter_num.reset_index().rename(columns={'sessionID':'item_num'})\n",
    "# data = pd.merge(data, item_num, how='left', on = 'itemID')\n",
    "# data = data[data['item_num']>9]\n",
    "# data = data[['sessionID', 'itemID']]\n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "\n",
    "sess_total = data['sessionID'].max()\n",
    "split_num = int(sess_total/10*9)\n",
    "\n",
    "tra_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "tes_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if len(all_seqs) < 2:\n",
    "        continue\n",
    "    if len(all_seqs) > 20:\n",
    "        all_seqs = all_seqs[:20]\n",
    "    if int(sess_temp) < split_num:\n",
    "        tra_sess[sess_temp] = all_seqs\n",
    "    else:\n",
    "        tes_sess[sess_temp] = all_seqs\n",
    "\n",
    "item_dict = {}#dict(old_itemID: new_itemID)\n",
    "# Convert training sessions to sequences and renumber items to start from 1\n",
    "def obtian_tra():\n",
    "    train_seqs = []\n",
    "#   MML item 1 序\n",
    "    item_ctr = 1\n",
    "    for s in tra_sess:\n",
    "        seq = tra_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "            else:\n",
    "                outseq += [item_ctr]\n",
    "                item_dict[i] = item_ctr\n",
    "                item_ctr += 1\n",
    "        if len(outseq) < 2:  # Doesn't occur\n",
    "            continue\n",
    "        train_seqs += [outseq]\n",
    "    print(\"#train_items\",item_ctr)    \n",
    "    return train_seqs\n",
    "\n",
    "\n",
    "# Convert test sessions to sequences, ignoring items that do not appear in training set\n",
    "def obtian_tes():\n",
    "    test_seqs = []\n",
    "    for s in tes_sess:\n",
    "        seq = tes_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "        if len(outseq) < 2:\n",
    "            continue\n",
    "        test_seqs += [outseq]\n",
    "    return test_seqs\n",
    "\n",
    "# 数据增强\n",
    "def process_seqs(iseqs):\n",
    "    out_seqs = []\n",
    "    labs = []\n",
    "    for seq in iseqs:\n",
    "        for i in range(1, len(seq)):\n",
    "            tar = seq[-i]\n",
    "            labs += [tar]\n",
    "            out_seqs += [seq[:-i]]\n",
    "    return out_seqs, labs\n",
    "\n",
    "# 无数据增强\n",
    "def process_seqs_no(iseqs):\n",
    "    out_seqs = []\n",
    "    labs = []\n",
    "    for seq in iseqs:\n",
    "        labs += [seq[-1]]\n",
    "        out_seqs += [seq[:-1]]\n",
    "    return out_seqs, labs\n",
    "\n",
    "\n",
    "tra_seqs = obtian_tra()\n",
    "tes_seqs = obtian_tes()\n",
    "\n",
    "mml_seqs = tra_seqs + tes_seqs\n",
    "\n",
    "# tra = (tr_seqs, tr_labs)\n",
    "# tes = (te_seqs, te_labs)\n",
    "\n",
    "all = 0\n",
    "\n",
    "for seq in mml_seqs:\n",
    "    all += len(seq)\n",
    "\n",
    "print('sequence average length: ', all/(len(mml_seqs) * 1.0))\n",
    "print('split sess id: ',str(len(tra_seqs)))\n",
    "\n",
    "train_data_path = 'MML/' + datasets_name + '/'\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "    \n",
    "path_data_train = train_data_path + f'{datasets_name}.inter'\n",
    "\n",
    "\n",
    "with open(path_data_train, 'w') as file:\n",
    "    file.write('user_id:token\\titem_id:token\\trating:float\\ttimestamp:float\\n')\n",
    "    for index, sess in zip(range(len(mml_seqs)),mml_seqs):\n",
    "        i_str = str(index+1)\n",
    "        for item_str in sess:\n",
    "            item_str = str(item_str)\n",
    "            file.write(f'{i_str}\\t{item_str}\\t{i_str}\\t{i_str}\\n')\n",
    "                   \n",
    "                   \n",
    "print(\"dataset: \", datasets_name)\n",
    "print(\"model: MML\")\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COTREC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train_items 9151\n",
      "train sequence:  89983\n",
      "test sequence:  10031\n",
      "train sample:  [[1, 2, 3], [4], [6]] [3, 5, 7]\n",
      "test sample:  [[3686], [1102, 5865], [7243, 1682]] [2915, 2429, 1870]\n",
      "all train sequences: [[1, 2, 3, 3], [4, 5], [6, 7]]\n",
      "sequence average length:  2.866678664986902\n",
      "dataset:  Office_Products\n",
      "COTREC\n",
      "2022-09-12 08:55:46\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "datasets_name = 'Office_Products'\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "\n",
    "# datasets_name = '2019-Oct'\n",
    "# price_num = 'logistic'\n",
    "\n",
    "# data_path = 'cosmeticsdata/' + datasets_name + '_data.csv'\n",
    "\n",
    "# datasets_name = 'digineticaBuy'\n",
    "# data_path = 'digineticaBuy/' + datasets_name + '_data.csv'\n",
    "\n",
    "# data_path = 'data_time_interval/' + datasets_name + '_data.csv'\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID']]\n",
    "\n",
    "# item_inter_num = pd.DataFrame(data.groupby(data['itemID']).count())\n",
    "# item_num=item_inter_num.reset_index().rename(columns={'sessionID':'item_num'})\n",
    "# data = pd.merge(data, item_num, how='left', on = 'itemID')\n",
    "# data = data[data['item_num']>9]\n",
    "# data = data[['sessionID', 'itemID']]\n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "\n",
    "sess_total = data['sessionID'].max()\n",
    "split_num = int(sess_total/10*9)\n",
    "\n",
    "tra_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "tes_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if len(all_seqs) < 2:\n",
    "        continue\n",
    "    if len(all_seqs) > 20:\n",
    "        all_seqs = all_seqs[:20]\n",
    "    if int(sess_temp) < split_num:\n",
    "        tra_sess[sess_temp] = all_seqs\n",
    "    else:\n",
    "        tes_sess[sess_temp] = all_seqs\n",
    "\n",
    "item_dict = {}#dict(old_itemID: new_itemID)\n",
    "# Convert training sessions to sequences and renumber items to start from 1\n",
    "def obtian_tra():\n",
    "    train_seqs = []\n",
    "    item_ctr = 1\n",
    "    for s in tra_sess:\n",
    "        seq = tra_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "            else:\n",
    "                outseq += [item_ctr]\n",
    "                item_dict[i] = item_ctr\n",
    "                item_ctr += 1\n",
    "        if len(outseq) < 2:  # Doesn't occur\n",
    "            continue\n",
    "        train_seqs += [outseq]\n",
    "    print(\"#train_items\",item_ctr)    \n",
    "    return train_seqs\n",
    "\n",
    "\n",
    "# Convert test sessions to sequences, ignoring items that do not appear in training set\n",
    "def obtian_tes():\n",
    "    test_seqs = []\n",
    "    for s in tes_sess:\n",
    "        seq = tes_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "        if len(outseq) < 2:\n",
    "            continue\n",
    "        test_seqs += [outseq]\n",
    "    return test_seqs\n",
    "\n",
    "# 数据增强\n",
    "def process_seqs(iseqs):\n",
    "    out_seqs = []\n",
    "    labs = []\n",
    "    for seq in iseqs:\n",
    "        for i in range(1, len(seq)):\n",
    "            tar = seq[-i]\n",
    "            labs += [tar]\n",
    "            out_seqs += [seq[:-i]]\n",
    "    return out_seqs, labs\n",
    "\n",
    "# 无数据增强\n",
    "def process_seqs_no(iseqs):\n",
    "    out_seqs = []\n",
    "    labs = []\n",
    "    for seq in iseqs:\n",
    "        labs += [seq[-1]]\n",
    "        out_seqs += [seq[:-1]]\n",
    "    return out_seqs, labs\n",
    "\n",
    "tra_seqs = obtian_tra()\n",
    "tes_seqs = obtian_tes()\n",
    "\n",
    "all_train_seqs = tra_seqs + tes_seqs\n",
    "\n",
    "tr_seqs, tr_labs = process_seqs_no(tra_seqs)\n",
    "te_seqs, te_labs = process_seqs_no(tes_seqs)\n",
    "\n",
    "tra = (tr_seqs, tr_labs)\n",
    "tes = (te_seqs, te_labs)\n",
    "print(\"train sequence: \", len(tr_seqs))\n",
    "print(\"test sequence: \", len(te_seqs))\n",
    "print(\"train sample: \", tr_seqs[:3], tr_labs[:3])\n",
    "print(\"test sample: \", te_seqs[:3], te_labs[:3])\n",
    "print(\"all train sequences:\", all_train_seqs[:3])\n",
    "all = 0\n",
    "\n",
    "for seq in tra_seqs:\n",
    "    all += len(seq)\n",
    "for seq in tes_seqs:\n",
    "    all += len(seq)\n",
    "print('sequence average length: ', all/(len(tra_seqs) + len(tes_seqs) * 1.0))\n",
    "\n",
    "train_data_path = 'COTREC/' + datasets_name\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "path_data_train = train_data_path + \"/train.txt\"\n",
    "path_data_test = train_data_path + \"/test.txt\"\n",
    "path_data_all = train_data_path+ \"/all_train_seq.txt\"\n",
    "\n",
    "pickle.dump(tra, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes, open(path_data_test, 'wb'))\n",
    "pickle.dump(all_train_seqs, open(path_data_all, 'wb'))\n",
    "print(\"dataset: \", datasets_name)\n",
    "print(\"COTREC\")\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LESSR & MSGIFSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train_items 9151\n",
      "train sequence:  89983\n",
      "test sequence:  10031\n",
      "train sample:  [[1, 2, 3, 3], [4, 5], [6, 7]]\n",
      "test sample:  [[3686, 2915], [1102, 5865, 2429], [7243, 1682, 1870]]\n",
      "sequence average length:  2.866678664986902\n",
      "save item_num : 9150\n",
      "dataset:  Office_Products\n",
      "2022-09-12 08:56:19\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# datasets_name = 'digineticaBuy'\n",
    "# data_path = 'digineticaBuy/' + datasets_name + '_data.csv'\n",
    "\n",
    "# datasets_name = '2019-Oct'\n",
    "# price_num = 'logistic'\n",
    "\n",
    "# data_path = 'cosmeticsdata/' + datasets_name + '_data.csv'\n",
    "\n",
    "datasets_name = 'Office_Products'\n",
    "# data_path = 'data_time_interval/' + datasets_name +'_data.csv'\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID']]\n",
    "\n",
    "# item_inter_num = pd.DataFrame(data.groupby(data['itemID']).count())\n",
    "# item_num=item_inter_num.reset_index().rename(columns={'sessionID':'item_num'})\n",
    "# data = pd.merge(data, item_num, how='left', on = 'itemID')\n",
    "# data = data[data['item_num']>9]\n",
    "# data = data[['sessionID', 'itemID']]\n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "\n",
    "sess_total = data['sessionID'].max()\n",
    "split_num = int(sess_total/10*9)\n",
    "\n",
    "tra_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "tes_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if len(all_seqs) < 2:\n",
    "        continue\n",
    "    if len(all_seqs) > 20:\n",
    "        all_seqs = all_seqs[:20]\n",
    "    if int(sess_temp) < split_num:\n",
    "        tra_sess[sess_temp] = all_seqs\n",
    "    else:\n",
    "        tes_sess[sess_temp] = all_seqs\n",
    "\n",
    "item_dict = {}#dict(old_itemID: new_itemID)\n",
    "# Convert training sessions to sequences and renumber items to start from 1\n",
    "def obtian_tra():\n",
    "    train_seqs = []\n",
    "    item_ctr = 1\n",
    "    for s in tra_sess:\n",
    "        seq = tra_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "            else:\n",
    "                outseq += [item_ctr]\n",
    "                item_dict[i] = item_ctr\n",
    "                item_ctr += 1\n",
    "        if len(outseq) < 2:  # Doesn't occur\n",
    "            continue\n",
    "        train_seqs += [outseq]\n",
    "    print(\"#train_items\",item_ctr)    \n",
    "    return train_seqs\n",
    "\n",
    "\n",
    "# Convert test sessions to sequences, ignoring items that do not appear in training set\n",
    "def obtian_tes():\n",
    "    test_seqs = []\n",
    "    for s in tes_sess:\n",
    "        seq = tes_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "        if len(outseq) < 2:\n",
    "            continue\n",
    "        test_seqs += [outseq]\n",
    "    return test_seqs\n",
    "\n",
    "\n",
    "tra_seqs = obtian_tra()\n",
    "tes_seqs = obtian_tes()\n",
    "\n",
    "print(\"train sequence: \", len(tra_seqs))\n",
    "print(\"test sequence: \", len(tes_seqs))\n",
    "print(\"train sample: \", tra_seqs[:3])\n",
    "print(\"test sample: \", tes_seqs[:3])\n",
    "all = 0\n",
    "\n",
    "for seq in tra_seqs:\n",
    "    all += len(seq)\n",
    "for seq in tes_seqs:\n",
    "    all += len(seq)\n",
    "print('sequence average length: ', all/(len(tra_seqs) + len(tes_seqs) * 1.0))\n",
    "\n",
    "# train_data_path = 'LESSR/Cell_Phones_and_Accessories'\n",
    "train_data_path = 'LESSRFeature/' + datasets_name \n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "path_data_train = train_data_path + \"/train.txt\"\n",
    "path_data_test = train_data_path + \"/test.txt\"\n",
    "path_item_num = train_data_path + '/num_items.txt'\n",
    "\n",
    "print('save item_num :', len(item_dict))\n",
    "pickle.dump(len(item_dict)+1, open(path_item_num, 'wb'))\n",
    "pickle.dump(tra_seqs, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes_seqs, open(path_data_test, 'wb'))\n",
    "print(\"dataset: \", datasets_name)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT4Rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train_items 9151\n",
      "train sequence:  89983\n",
      "test sequence:  10031\n",
      "train sample:  [[1, 2, 3], [4], [6]] [3, 5, 7]\n",
      "test sample:  [[3686], [1102, 5865], [7243, 1682]] [2915, 2429, 1870]\n",
      "sequence average length:  2.866678664986902\n",
      "dataset:  Office_Products\n",
      "2022-09-12 08:56:55\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# datasets_name = 'digineticaBuy'\n",
    "# data_path = 'digineticaBuy/' + datasets_name + '_data.csv'\n",
    "\n",
    "# datasets_name = '2019-Oct'\n",
    "# price_num = 'logistic'\n",
    "\n",
    "# data_path = 'cosmeticsdata/' + datasets_name + '_data.csv'\n",
    "\n",
    "datasets_name = 'Office_Products'\n",
    "# data_path = 'data_time_interval/' + datasets_name +'_data.csv'\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID']]\n",
    "\n",
    "# item_inter_num = pd.DataFrame(data.groupby(data['itemID']).count())\n",
    "# item_num=item_inter_num.reset_index().rename(columns={'sessionID':'item_num'})\n",
    "# data = pd.merge(data, item_num, how='left', on = 'itemID')\n",
    "# data = data[data['item_num']>9]\n",
    "# data = data[['sessionID', 'itemID']]\n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "\n",
    "sess_total = data['sessionID'].max()\n",
    "split_num = int(sess_total/10*9)\n",
    "\n",
    "tra_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "tes_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if len(all_seqs) < 2:\n",
    "        continue\n",
    "    if len(all_seqs) > 20:\n",
    "        all_seqs = all_seqs[:20]\n",
    "    if int(sess_temp) < split_num:\n",
    "        tra_sess[sess_temp] = all_seqs\n",
    "    else:\n",
    "        tes_sess[sess_temp] = all_seqs\n",
    "\n",
    "item_dict = {}#dict(old_itemID: new_itemID)\n",
    "# Convert training sessions to sequences and renumber items to start from 1\n",
    "def obtian_tra():\n",
    "    train_seqs = []\n",
    "    sessid_seqs = []\n",
    "    item_ctr = 1\n",
    "    for s in tra_sess:\n",
    "        seq = tra_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "            else:\n",
    "                outseq += [item_ctr]\n",
    "                item_dict[i] = item_ctr\n",
    "                item_ctr += 1\n",
    "        if len(outseq) < 2:  # Doesn't occur\n",
    "            continue\n",
    "        train_seqs += [outseq]\n",
    "        sessid_seqs += [s]\n",
    "    print(\"#train_items\",item_ctr)    \n",
    "    return train_seqs, sessid_seqs\n",
    "\n",
    "\n",
    "# Convert test sessions to sequences, ignoring items that do not appear in training set\n",
    "def obtian_tes():\n",
    "    test_seqs = []\n",
    "    test_sessid_seqs = []\n",
    "    for s in tes_sess:\n",
    "        seq = tes_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "        if len(outseq) < 2:\n",
    "            continue\n",
    "        test_seqs += [outseq]\n",
    "        test_sessid_seqs += [s]\n",
    "    return test_seqs, test_sessid_seqs\n",
    "\n",
    "# 数据增强\n",
    "def process_seqs(iseqs):\n",
    "    out_seqs = []\n",
    "    labs = []\n",
    "    for seq in iseqs:\n",
    "        for i in range(1, len(seq)):\n",
    "            tar = seq[-i]\n",
    "            labs += [tar]\n",
    "            out_seqs += [seq[:-i]]\n",
    "    return out_seqs, labs\n",
    "\n",
    "# 无数据增强\n",
    "def process_seqs_no(iseqs):\n",
    "    out_seqs = []\n",
    "    labs = []\n",
    "    for seq in iseqs:\n",
    "        labs += [seq[-1]]\n",
    "        out_seqs += [seq[:-1]]\n",
    "    return out_seqs, labs\n",
    "\n",
    "\n",
    "tra_seqs, tra_sessid = obtian_tra()\n",
    "tes_seqs, tes_sessid = obtian_tes()\n",
    "\n",
    "tr_seqs, tr_labs = process_seqs_no(tra_seqs)\n",
    "te_seqs, te_labs = process_seqs_no(tes_seqs)\n",
    "\n",
    "tra = (tra_sessid, tr_seqs, tr_labs)\n",
    "tes = (tes_sessid, te_seqs, te_labs)\n",
    "print(\"train sequence: \", len(tr_seqs))\n",
    "print(\"test sequence: \", len(te_seqs))\n",
    "print(\"train sample: \", tr_seqs[:3], tr_labs[:3])\n",
    "print(\"test sample: \", te_seqs[:3], te_labs[:3])\n",
    "all = 0\n",
    "\n",
    "for seq in tra_seqs:\n",
    "    all += len(seq)\n",
    "for seq in tes_seqs:\n",
    "    all += len(seq)\n",
    "print('sequence average length: ', all/(len(tra_seqs) + len(tes_seqs) * 1.0))\n",
    "\n",
    "train_data_path = 'Bert4RecFeature/' + datasets_name\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "path_data_train = train_data_path + \"/train.txt\"\n",
    "path_data_test = train_data_path + \"/test.txt\"\n",
    "\n",
    "pickle.dump(tra, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes, open(path_data_test, 'wb'))\n",
    "print(\"dataset: \", datasets_name)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******SKNN******\n",
      "#train_items 18797\n",
      "train sequence:  190840\n",
      "test sequence:  21119\n",
      "train sample:  [[1, 2], [4], [6, 7, 8]] [3, 5, 9]\n",
      "test sample:  [[5282, 15212], [900, 2046, 12546, 787], [461, 3225, 461, 3225]] [4461, 4053, 3226]\n",
      "sequence average length:  2.6727055704169205\n",
      "dataset:  Sports_and_Outdoors\n",
      "2022-09-21 13:47:31\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "datasets_name = 'Sports_and_Outdoors'\n",
    "# data_path = 'data_time_interval/' + datasets_name +'_data.csv'\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "\n",
    "# datasets_name = '2019-Oct'\n",
    "# price_num = 'logistic'\n",
    "\n",
    "# data_path = 'cosmeticsdata/' + datasets_name + '_data.csv'\n",
    "\n",
    "# datasets_name = 'digineticaBuy'\n",
    "# data_path = 'digineticaBuy/' + datasets_name + '_data.csv'\n",
    "\n",
    "\n",
    "print(\"******SKNN******\")\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID', 'time']]\n",
    "\n",
    "\n",
    "# item_inter_num = pd.DataFrame(data.groupby(data['itemID']).count())\n",
    "# item_num=item_inter_num.reset_index().rename(columns={'sessionID':'item_num'})[['itemID', 'item_num']]\n",
    "# data = pd.merge(data, item_num, how='left', on = 'itemID')\n",
    "# data = data[data['item_num']>9]\n",
    "# data = data[['sessionID', 'itemID', 'time']]\n",
    " \n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "sess_date = {}\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    timestamp = row['time']\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "        sess_date[sess_id] = timestamp\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "        sess_date[sess_id] = timestamp\n",
    "\n",
    "\n",
    "sess_total = data['sessionID'].max()\n",
    "split_num = int(sess_total/10*9)\n",
    "\n",
    "tra_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "tes_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if len(all_seqs) < 2:\n",
    "        continue\n",
    "    if len(all_seqs) > 20:\n",
    "        all_seqs = all_seqs[:20]\n",
    "    if int(sess_temp) < split_num:\n",
    "        tra_sess[sess_temp] = all_seqs\n",
    "    else:\n",
    "        tes_sess[sess_temp] = all_seqs\n",
    "\n",
    "\n",
    "item_dict = {}#dict(old_itemID: new_itemID)\n",
    "# Convert training sessions to sequences and renumber items to start from 1\n",
    "def obtian_tra():\n",
    "    train_seqs = []\n",
    "    sessid_seqs = []\n",
    "    time_seqs = []\n",
    "    item_ctr = 1\n",
    "    for s in tra_sess:\n",
    "        seq = tra_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "            else:\n",
    "                outseq += [item_ctr]\n",
    "                item_dict[i] = item_ctr\n",
    "                item_ctr += 1\n",
    "        if len(outseq) < 2:  # Doesn't occur\n",
    "            continue\n",
    "        train_seqs += [outseq]\n",
    "        sessid_seqs += [s]\n",
    "        time_seqs += [sess_date[s]]\n",
    "    print(\"#train_items\",item_ctr)    \n",
    "    return train_seqs, sessid_seqs, time_seqs\n",
    "\n",
    "\n",
    "# Convert test sessions to sequences, ignoring items that do not appear in training set\n",
    "def obtian_tes():\n",
    "    test_seqs = []\n",
    "    test_sessid_seqs = []\n",
    "    test_time_seqs = []\n",
    "    for s in tes_sess:\n",
    "        seq = tes_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "        if len(outseq) < 2:\n",
    "            continue\n",
    "        test_seqs += [outseq]\n",
    "        test_sessid_seqs += [s]\n",
    "        test_time_seqs += [sess_date[s]]\n",
    "    return test_seqs, test_sessid_seqs, test_time_seqs\n",
    "\n",
    "# 数据增强\n",
    "def process_seqs(iseqs):\n",
    "    out_seqs = []\n",
    "    labs = []\n",
    "    for seq in iseqs:\n",
    "        for i in range(1, len(seq)):\n",
    "            tar = seq[-i]\n",
    "            labs += [tar]\n",
    "            out_seqs += [seq[:-i]]\n",
    "    return out_seqs, labs\n",
    "\n",
    "# 无数据增强\n",
    "def process_seqs_no(iseqs):\n",
    "    out_seqs = []\n",
    "    labs = []\n",
    "    for seq in iseqs:\n",
    "        labs += [seq[-1]]\n",
    "        out_seqs += [seq[:-1]]\n",
    "    return out_seqs, labs\n",
    "\n",
    "\n",
    "tra_seqs, tra_sessid, tra_time = obtian_tra()\n",
    "tes_seqs, tes_sessid, tes_time = obtian_tes()\n",
    "\n",
    "tr_seqs, tr_labs = process_seqs_no(tra_seqs)\n",
    "te_seqs, te_labs = process_seqs_no(tes_seqs)\n",
    "\n",
    "tra = (tra_sessid, tr_seqs, tr_labs, tra_time)\n",
    "tes = (tes_sessid, te_seqs, te_labs, tes_time)\n",
    "print(\"train sequence: \", len(tr_seqs))\n",
    "print(\"test sequence: \", len(te_seqs))\n",
    "print(\"train sample: \", tr_seqs[:3], tr_labs[:3])\n",
    "print(\"test sample: \", te_seqs[:3], te_labs[:3])\n",
    "all = 0\n",
    "\n",
    "for seq in tra_seqs:\n",
    "    all += len(seq)\n",
    "for seq in tes_seqs:\n",
    "    all += len(seq)\n",
    "print('sequence average length: ', all/(len(tra_seqs) + len(tes_seqs) * 1.0))\n",
    "\n",
    "train_data_path = 'SKNNFeature/' + datasets_name\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "path_data_train = train_data_path + \"/train.txt\"\n",
    "path_data_test = train_data_path + \"/test.txt\"\n",
    "\n",
    "pickle.dump(tra, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes, open(path_data_test, 'wb'))\n",
    "print(\"dataset: \", datasets_name)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKNN Fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "datasets_name = 'Sports_and_Outdoors'\n",
    "# data_path = 'data_time_interval/' + datasets_name +'_data.csv'\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "\n",
    "# datasets_name = '2019-Oct'\n",
    "# price_num = 'logistic'\n",
    "\n",
    "# data_path = 'cosmeticsdata/' + datasets_name + '_data.csv'\n",
    "\n",
    "# datasets_name = 'digineticaBuy'\n",
    "# data_path = 'digineticaBuy/' + datasets_name + '_data.csv'\n",
    "\n",
    "\n",
    "print(\"******SKNN******\")\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID', 'review_time']]\n",
    "\n",
    "\n",
    "# item_inter_num = pd.DataFrame(data.groupby(data['itemID']).count())\n",
    "# item_num=item_inter_num.reset_index().rename(columns={'sessionID':'item_num'})[['itemID', 'item_num']]\n",
    "# data = pd.merge(data, item_num, how='left', on = 'itemID')\n",
    "# data = data[data['item_num']>9]\n",
    "# data = data[['sessionID', 'itemID', 'time']]\n",
    " \n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "sess_date = {}\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    timestamp = row['review_time']\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "        sess_date[sess_id] = timestamp\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "        sess_date[sess_id] = timestamp\n",
    "        \n",
    "# 限制每个seq长度\n",
    "for sess_temp in sess_all.keys():\n",
    "    seq = sess_all[sess_temp]\n",
    "    time_seq = sess_date[sess_temp]\n",
    "    if len(seq) >20:\n",
    "        sess_all[sess_temp] = seq[-20:]\n",
    "        sess_date[sess_temp] = time_seq[-20:]\n",
    "\n",
    "tra = [[],[],[], []]\n",
    "# dict {train_sessionID:[itemID,itemID]}  去掉了seq中last item(不包含test的label)， 也可以调整为去掉两个item\n",
    "train_sess = {}\n",
    "\n",
    "item_count = 0\n",
    "user_count = 0\n",
    "inter_count = 0\n",
    "item_num = 1\n",
    "\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    train_seqs_t = all_seqs[:-1]\n",
    "    t_time_seq = sess_date[sess_temp][:-1]\n",
    "    inter_count += len(train_seqs_t)\n",
    "#     item_seq with new ID\n",
    "    new_seq = []\n",
    "    for x in train_seqs_t:\n",
    "        if x in item_dict:\n",
    "            new_seq.append(item_dict[x])\n",
    "        else:\n",
    "            item_dict[x] = item_num\n",
    "            newitem_title[item_num] = olditem_title[x]\n",
    "            new_seq.append(item_num)\n",
    "            item_num += 1\n",
    "    train_sess[sess_temp] = new_seq\n",
    "    tra[0].append(sess_temp)\n",
    "    tra[1].append(new_seq[:-1])\n",
    "#     预测时考虑last item\n",
    "    tra[2].append(new_seq[-1])\n",
    "    tra[3].append(t_time_seq)\n",
    "    \n",
    "item_count =  item_num-1\n",
    "user_count = len(tra[0])\n",
    "\n",
    "# test_set [[x_id ...],[label...]\n",
    "tes = [[],[],[], []]\n",
    "tes_seq_num = 0\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    all_time = sess_date[sess_temp]\n",
    "    if all_seqs[-1] in item_dict:\n",
    "        new_seq = []\n",
    "        for x in all_seqs:\n",
    "            new_seq.append(item_dict[x])\n",
    "        tes_seq_num += 1\n",
    "        tes[0].append(sess_temp)\n",
    "        tes[1].append(new_seq[:-1])\n",
    "    #     预测时考虑last item\n",
    "        tes[2].append(new_seq[-1])\n",
    "        tes[3].append(all_time)\n",
    "    \n",
    "\n",
    "print('item_num: ',str(item_count))\n",
    "print('train_seq_num & user_num: ',str(user_count))\n",
    "print('test_seq_num: ',str(tes_seq_num))\n",
    "\n",
    "inter_count += tes_seq_num\n",
    "\n",
    "\n",
    "print('#interactions: ',inter_count)\n",
    "\n",
    "print('sequence average length: ', (inter_count)/(user_count) * 1.0)\n",
    "\n",
    "tra = (tra[0], tra[1], tra[2], tra[3])\n",
    "tes = (tes[0], tes[1], tes[2], tes[3])\n",
    "\n",
    "train_data_path = './SKNN/' + datasets_name\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "path_data_train = train_data_path + \"/train.txt\"\n",
    "path_data_test = train_data_path + \"/test.txt\"\n",
    "\n",
    "pickle.dump(tra, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes, open(path_data_test, 'wb'))\n",
    "print(\"dataset: \", datasets_name)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"SKNN done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU4Rec with tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "datasets_name = 'Cell_Phones_and_Accessories'\n",
    "data_path = 'SKNNFeature/' + datasets_name\n",
    "path_data_train = data_path + \"/train.txt\"\n",
    "path_data_test = data_path + \"/test.txt\"\n",
    "\n",
    "csv_train_path = './GRU4RecFeature/' + datasets_name + '/train.tsv'\n",
    "csv_test_path = './GRU4RecFeature/' + datasets_name + '/test.tsv'\n",
    "\n",
    "train_data = pickle.load(open(path_data_train, 'rb'))\n",
    "test_data = pickle.load(open(path_data_test, 'rb'))\n",
    "\n",
    "\n",
    "def txt2csv(txtdata, data_path):\n",
    "    listdata = []\n",
    "    id_seqs = txtdata[0]\n",
    "    sess_seqs = txtdata[1]\n",
    "    time_seqs = txtdata[3]\n",
    "    lab_seqs = txtdata[2]\n",
    "    for x_id, x_sess, x_time, x_lab in zip(id_seqs, sess_seqs, time_seqs, lab_seqs):\n",
    "        for y_item in x_sess:\n",
    "            x_list = []\n",
    "            x_list.append(x_id)\n",
    "            x_list.append(y_item)\n",
    "            x_list.append(x_time)\n",
    "            listdata.append(x_list)\n",
    "        x_list = []\n",
    "        x_list.append(x_id)\n",
    "        x_list.append(x_lab)\n",
    "        x_list.append(x_time)\n",
    "        listdata.append(x_list)\n",
    "    names = ['SessionId', 'ItemId', 'TimeStamp']\n",
    "    file = pd.DataFrame(columns = names, data = listdata)\n",
    "    file.to_csv(data_path,index=False,sep='\\t')\n",
    "    print('transfor done')\n",
    "    return 1\n",
    "\n",
    "txt2csv(train_data, csv_train_path)\n",
    "txt2csv(test_data, csv_test_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

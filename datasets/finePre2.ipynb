{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Senti preprocess 2 使用的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_num:  12391\n",
      "train_seq_num & user_num:  12373\n",
      "test_seq_num:  12311\n",
      "#interactions:  110313\n",
      "sequence average length:  8.915622726905358\n",
      "dataset:  Yelp\n",
      "2023-12-29 08:51:17\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "meta_dataset = 'Yelp'\n",
    "data_path =  './dict/' + meta_dataset + '/session_data.csv'\n",
    "\n",
    "if meta_dataset == 'Beauty' or meta_dataset == 'Beauty_4':\n",
    "    # 'Brand','Ingredients','Effectiveness','Scent','Price','Color','Size'\n",
    "    attr_list = ['Brand','Ingredients','Effectiveness','Scent','Price','Color','Size', 'Compatibility', 'Longevity']\n",
    "elif meta_dataset == 'Cell_Phones_and_Accessories':\n",
    "    # Brand, Performance, color, Battery ,Connectivity,Price, Size\n",
    "    attr_list = ['Brand', 'Performance', 'Color', 'Battery' ,'Connectivity','Price', 'Size']\n",
    "elif meta_dataset == 'Clothing_Shoes_and_Jewelry':\n",
    "    # Price, Style, Size, Fabric, Brand, Quality, Color\n",
    "    attr_list = ['Price', 'Style', 'Size', 'Fabric', 'Brand', 'Quality', 'Color']\n",
    "elif meta_dataset == 'Home_and_Kitchen':\n",
    "    # Price, Brand, Quality, Material, Size, Appearance, Washability, Functionality, Style, Durability,\n",
    "    attr_list = ['Price', 'Brand', 'Quality', 'Material', 'Size', 'Appearance', 'Washability', 'Functionality', 'Style']\n",
    "elif meta_dataset == 'Sports_and_Outdoors':\n",
    "    # Price, Quality, Functionality,Size,Protection,Material,Comfort\n",
    "    attr_list = ['Price', 'Quality', 'Functionality', 'Size', 'Protection', 'Material', 'Comfort']\n",
    "elif meta_dataset == 'Yelp':\n",
    "    # Price, Brand, Quality, Material, Size, Appearance, Washability, Functionality, Style, Durability,\n",
    "    attr_list = ['Price', 'Food', 'Location', 'Ambience', 'Service', 'Cleanliness', 'Parking']\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "if meta_dataset == 'Beauty' or meta_dataset == 'Home_and_Kitchen':\n",
    "    data_all = data_all.reset_index()[['sessionID', 'itemID', attr_list[0], attr_list[1],attr_list[2],attr_list[3],attr_list[4],attr_list[5],attr_list[6],attr_list[7],attr_list[8]]]\n",
    "else:\n",
    "    data_all = data_all.reset_index()[['sessionID', 'itemID', attr_list[0], attr_list[1],attr_list[2],attr_list[3],attr_list[4],attr_list[5],attr_list[6]]]\n",
    "\n",
    "attr_num = len(attr_list)\n",
    "\n",
    "# recorde all items\n",
    "# dict {sessionID:[itemID,itemID]} \n",
    "sess_all = {}\n",
    "#  dict {sessionID:[opinID,opinID]}\n",
    "attr1_all = {}\n",
    "attr2_all = {}\n",
    "attr3_all = {}\n",
    "attr4_all = {}\n",
    "attr5_all = {}\n",
    "attr6_all = {}\n",
    "attr7_all = {}\n",
    "attr8_all = {}\n",
    "attr9_all = {}\n",
    "\n",
    "item_count = 0\n",
    "user_count = 0\n",
    "inter_count = 0\n",
    "\n",
    "for _, row in data_all.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    attr1_opin = row[attr_list[0]]\n",
    "    attr2_opin = row[attr_list[1]]\n",
    "    attr3_opin = row[attr_list[2]]\n",
    "    attr4_opin = row[attr_list[3]]\n",
    "    attr5_opin = row[attr_list[4]]\n",
    "    attr6_opin = row[attr_list[5]]\n",
    "    attr7_opin = row[attr_list[6]]\n",
    "    if meta_dataset == 'Beauty' or meta_dataset == 'Home_and_Kitchen':\n",
    "        attr8_opin = row[attr_list[7]]\n",
    "        attr9_opin = row[attr_list[8]]\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "        attr1_all[sess_id].append(attr1_opin)\n",
    "        attr2_all[sess_id].append(attr2_opin)\n",
    "        attr3_all[sess_id].append(attr3_opin)\n",
    "        attr4_all[sess_id].append(attr4_opin)\n",
    "        attr5_all[sess_id].append(attr5_opin)\n",
    "        attr6_all[sess_id].append(attr6_opin)\n",
    "        attr7_all[sess_id].append(attr7_opin)\n",
    "        if meta_dataset == 'Beauty' or meta_dataset == 'Home_and_Kitchen':\n",
    "            attr8_all[sess_id].append(attr8_opin)\n",
    "            attr9_all[sess_id].append(attr9_opin)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "        attr1_all[sess_id] = []\n",
    "        attr1_all[sess_id].append(attr1_opin)\n",
    "        attr2_all[sess_id] = []\n",
    "        attr2_all[sess_id].append(attr2_opin)\n",
    "        attr3_all[sess_id] = []\n",
    "        attr3_all[sess_id].append(attr3_opin)\n",
    "        attr4_all[sess_id] = []\n",
    "        attr4_all[sess_id].append(attr4_opin)\n",
    "        attr5_all[sess_id] = []\n",
    "        attr5_all[sess_id].append(attr5_opin)\n",
    "        attr6_all[sess_id] = []\n",
    "        attr6_all[sess_id].append(attr6_opin)\n",
    "        attr7_all[sess_id] = []\n",
    "        attr7_all[sess_id].append(attr7_opin)\n",
    "        if meta_dataset == 'Beauty' or meta_dataset == 'Home_and_Kitchen':\n",
    "            attr8_all[sess_id] = []\n",
    "            attr9_all[sess_id] = []\n",
    "            attr8_all[sess_id].append(attr8_opin)\n",
    "            attr9_all[sess_id].append(attr9_opin)\n",
    "\n",
    "# 限制每个seq长度\n",
    "for sess_temp in sess_all.keys():\n",
    "    seq = sess_all[sess_temp]\n",
    "    if len(seq) >20:\n",
    "        sess_all[sess_temp] = seq[-20:]\n",
    "        attr1_all[sess_id] = attr1_all[sess_id][-20:]\n",
    "        attr2_all[sess_id] = attr2_all[sess_id][-20:]\n",
    "        attr3_all[sess_id] = attr3_all[sess_id][-20:]\n",
    "        attr4_all[sess_id] = attr4_all[sess_id][-20:]\n",
    "        attr5_all[sess_id] = attr5_all[sess_id][-20:]\n",
    "        attr6_all[sess_id] = attr6_all[sess_id][-20:]\n",
    "        attr7_all[sess_id] = attr7_all[sess_id][-20:]\n",
    "        if meta_dataset == 'Beauty' or meta_dataset == 'Home_and_Kitchen':\n",
    "            attr8_all[sess_id] = attr8_all[sess_id][-20:]\n",
    "            attr9_all[sess_id] = attr9_all[sess_id][-20:]\n",
    "\n",
    "# 形成train set\n",
    "# 统计 train set 出现的item，给item重新编号\n",
    "item_num = 1\n",
    "# dict{lod_itemID:new_itemID}\n",
    "item_dict = {}\n",
    "# train_set [[u_id ...], [x_t-1...], [label...]\n",
    "tra = [[],[],[]]\n",
    "# dict {train_sessionID:[itemID,itemID]}  去掉了seq中last item(不包含test的label)， 也可以调整为去掉两个item\n",
    "train_sess = {}\n",
    "\n",
    "# dict {item_id:[user_id, user_id]}\n",
    "train_items = {}\n",
    "\n",
    "# dict {item_id:opin_id}\n",
    "attr1_item_opin = {}\n",
    "attr2_item_opin = {}\n",
    "attr3_item_opin = {}\n",
    "attr4_item_opin = {}\n",
    "attr5_item_opin = {}\n",
    "attr6_item_opin = {}\n",
    "attr7_item_opin = {}\n",
    "attr8_item_opin = {}\n",
    "attr9_item_opin = {}\n",
    "\n",
    "\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    train_seqs_t = all_seqs[:-1]\n",
    "    a1_opin = attr1_all[sess_temp][:-1]\n",
    "    a2_opin = attr2_all[sess_temp][:-1]\n",
    "    a3_opin = attr3_all[sess_temp][:-1]\n",
    "    a4_opin = attr4_all[sess_temp][:-1]\n",
    "    a5_opin = attr5_all[sess_temp][:-1]\n",
    "    a6_opin = attr6_all[sess_temp][:-1]\n",
    "    a7_opin = attr7_all[sess_temp][:-1]\n",
    "    if meta_dataset == 'Beauty' or meta_dataset == 'Home_and_Kitchen':\n",
    "        a8_opin = attr8_all[sess_temp][:-1]\n",
    "        a9_opin = attr9_all[sess_temp][:-1]\n",
    "    else:\n",
    "        a8_opin = attr6_all[sess_temp][:-1]\n",
    "        a9_opin = attr7_all[sess_temp][:-1]\n",
    "    inter_count += len(train_seqs_t)\n",
    "#     item_seq with new ID\n",
    "    new_seq = []\n",
    "    for x, a1, a2, a3, a4, a5, a6, a7, a8, a9 in zip(train_seqs_t, a1_opin, a2_opin,a3_opin,a4_opin,a5_opin,a6_opin,a7_opin,a8_opin,a9_opin):\n",
    "        if x in item_dict:\n",
    "            new_seq.append(item_dict[x])\n",
    "            if sess_temp not in train_items[item_dict[x]]:\n",
    "                train_items[item_dict[x]].append(sess_temp)\n",
    "                attr1_item_opin[item_dict[x]].append(a1)\n",
    "                attr2_item_opin[item_dict[x]].append(a2)\n",
    "                attr3_item_opin[item_dict[x]].append(a3)\n",
    "                attr4_item_opin[item_dict[x]].append(a4)\n",
    "                attr5_item_opin[item_dict[x]].append(a5)\n",
    "                attr6_item_opin[item_dict[x]].append(a6)\n",
    "                attr7_item_opin[item_dict[x]].append(a7)\n",
    "                if meta_dataset == 'Beauty' or meta_dataset == 'Home_and_Kitchen':\n",
    "                    attr8_item_opin[item_dict[x]].append(a8)\n",
    "                    attr9_item_opin[item_dict[x]].append(a9)\n",
    "        else:\n",
    "            item_dict[x] = item_num\n",
    "            new_seq.append(item_num)\n",
    "            \n",
    "            train_items[item_num] = [] \n",
    "            train_items[item_num].append(sess_temp)\n",
    "            attr1_item_opin[item_num] = []\n",
    "            attr2_item_opin[item_num] = []\n",
    "            attr3_item_opin[item_num] = []\n",
    "            attr4_item_opin[item_num] = []\n",
    "            attr5_item_opin[item_num] = []\n",
    "            attr6_item_opin[item_num] = []\n",
    "            attr7_item_opin[item_num] = []\n",
    "            \n",
    "            attr1_item_opin[item_num].append(a1)\n",
    "            attr2_item_opin[item_num].append(a2)\n",
    "            attr3_item_opin[item_num].append(a3)\n",
    "            attr4_item_opin[item_num].append(a4)\n",
    "            attr5_item_opin[item_num].append(a5)\n",
    "            attr6_item_opin[item_num].append(a6)\n",
    "            attr7_item_opin[item_num].append(a7)\n",
    "            if meta_dataset == 'Beauty' or meta_dataset == 'Home_and_Kitchen':\n",
    "                attr8_item_opin[item_num] = []\n",
    "                attr9_item_opin[item_num] = []\n",
    "                attr8_item_opin[item_num].append(a8)\n",
    "                attr9_item_opin[item_num].append(a9)\n",
    "            \n",
    "            item_num += 1\n",
    "    train_sess[sess_temp] = new_seq\n",
    "    tra[0].append(sess_temp)\n",
    "#     预测时考虑last item\n",
    "    tra[1].append(new_seq[:-1])\n",
    "    tra[2].append(new_seq[-1])\n",
    "    \n",
    "item_count =  item_num-1\n",
    "user_count = len(tra[0])\n",
    "\n",
    "fre_item_mat = np.zeros((item_count, item_count), dtype=int)\n",
    "# 全部的共现关系Original \n",
    "for sess in train_sess.keys():\n",
    "    item_seq = np.unique(train_sess[sess])\n",
    "    for it1 in item_seq:\n",
    "        for it2 in item_seq:\n",
    "            if it1 != it2:\n",
    "#                 print(sess)\n",
    "                fre_item_mat[int(it1)-1, int(it2)-1] +=1 \n",
    "                \n",
    "fre_user_mat = np.zeros((user_count, user_count), dtype=int)\n",
    "# 全部的共现关系Original \n",
    "for item_t in train_items.keys():\n",
    "    user_seq = np.unique(train_items[item_t])\n",
    "    for it1 in user_seq:\n",
    "        for it2 in user_seq:\n",
    "            if it1 != it2:\n",
    "                fre_user_mat[int(it1)-1, int(it2)-1] +=1 \n",
    "\n",
    "# building user_fre & item_fre csr_matrix\n",
    "def spar_graph(fre_mat):\n",
    "    indices = [] # 非0元素的列坐标，item_id -1\n",
    "    all_data = [] # 非0元素值，opin_id\n",
    "    indptr = [0] # i i-1 行非0元素个数，0-0， n-总共元素个数，行数+1\n",
    "    for i_list in fre_mat:\n",
    "        i_temp_list = np.nonzero(i_list)[0].tolist()\n",
    "#         if len(i_temp_list) == 0:\n",
    "#             non_co += 1\n",
    "        indices += i_temp_list\n",
    "        all_data += [1]*len(i_temp_list)\n",
    "        indptr.append(indptr[-1]+len(i_temp_list))\n",
    "#     print('item without co-occurren: ',str(non_co))\n",
    "    # indptr:session长度累加和; indices:item_id 减1, 由每个session内item组成; data:item在session内的权重，全部为1.\n",
    "    coo_mat = (all_data, indices, indptr)\n",
    "    return coo_mat\n",
    "\n",
    "item_co_mat = spar_graph(fre_item_mat)\n",
    "user_co_mat = spar_graph(fre_user_mat)\n",
    "\n",
    "\n",
    "# test_set [[u_id ...], [x_t-1...], [label...]\n",
    "tes = [[],[],[]]\n",
    "tes_seq_num = 0\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if all_seqs[-1] in item_dict:\n",
    "        tes_seq_num += 1\n",
    "        new_seq = []\n",
    "        for x in all_seqs:\n",
    "            new_seq.append(item_dict[x])\n",
    "        tes[0].append(sess_temp)\n",
    "    #     预测时考虑last item\n",
    "        tes[1].append(new_seq[:-1])\n",
    "        tes[2].append(item_dict[all_seqs[-1]])\n",
    "    \n",
    "\n",
    "print('item_num: ',str(item_count))\n",
    "print('train_seq_num & user_num: ',str(user_count))\n",
    "print('test_seq_num: ',str(tes_seq_num))\n",
    "\n",
    "inter_count += tes_seq_num\n",
    "\n",
    "#  obtain user-item interaction sparsity matrix in each attribute\n",
    "# def spar_matrix(attr, tra_seq):\n",
    "#     indices = [] # 非0元素的列坐标，item_id -1\n",
    "#     all_data = [] # 非0元素值，opin_id\n",
    "#     indptr = [0] # i i-1 行非0元素个数，0-0， n-总共元素个数，行数+1\n",
    "#     for sess_temp in attr.keys():\n",
    "#         op_list = attr[sess_temp][:-1]\n",
    "#         it_list = tra_seq[sess_temp]\n",
    "#         np_op = np.array(op_list)\n",
    "#         np_it = np.array(it_list)\n",
    "#         op_f = np_op[np.nonzero(np_op)[0]]\n",
    "#         it_f = np_it[np.nonzero(np_op)[0]]\n",
    "\n",
    "#         item_l = np.unique(it_f)\n",
    "#         op_l = op_f[np.unique(it_f, return_index=True)[1]]\n",
    "#         indices += (item_l-1).tolist()\n",
    "#         all_data += op_l.tolist()\n",
    "#         indptr.append(indptr[-1] + len(item_l.tolist()))\n",
    "#     results = (all_data, indices, indptr)\n",
    "#     return results\n",
    "\n",
    "# attr1_mat = spar_matrix(attr1_all, train_sess)\n",
    "# attr2_mat = spar_matrix(attr2_all, train_sess)\n",
    "# attr3_mat = spar_matrix(attr3_all, train_sess)\n",
    "# attr4_mat = spar_matrix(attr4_all, train_sess)\n",
    "# attr5_mat = spar_matrix(attr5_all, train_sess)\n",
    "# attr6_mat = spar_matrix(attr6_all, train_sess)\n",
    "# attr7_mat = spar_matrix(attr7_all, train_sess)\n",
    "\n",
    "# def spar_matrix(tra_seq):\n",
    "#     indices = [] # 非0元素的列坐标，item_id -1\n",
    "#     all_data = [] # 非0元素值，opin_id\n",
    "#     indptr = [0] # i i-1 行非0元素个数，0-0， n-总共元素个数，行数+1\n",
    "#     for sess_temp in tra_seq.keys():\n",
    "#         op_list = attr[sess_temp][:-1]\n",
    "#         it_list = tra_seq[sess_temp]\n",
    "#         np_op = np.array(op_list)\n",
    "#         np_it = np.array(it_list)\n",
    "#         op_f = np_op[np.nonzero(np_op)[0]]\n",
    "#         it_f = np_it[np.nonzero(np_op)[0]]\n",
    "\n",
    "#         item_l = np.unique(it_f)\n",
    "#         op_l = op_f[np.unique(it_f, return_index=True)[1]]\n",
    "#         indices += (item_l-1).tolist()\n",
    "#         all_data += op_l.tolist()\n",
    "#         indptr.append(indptr[-1] + len(item_l.tolist()))\n",
    "#     results = (all_data, indices, indptr)\n",
    "#     return results\n",
    "\n",
    "def spar_matrix(tra_seq):\n",
    "    indices = [] # 非0元素的列坐标，item_id -1\n",
    "    all_data = [] # 非0元素值，opin_id\n",
    "    indptr = [0] # i i-1 行非0元素个数，0-0， n-总共元素个数，行数+1\n",
    "    for sess_temp in tra_seq.keys():\n",
    "        it_list = tra_seq[sess_temp]\n",
    "        it_index = np.unique(it_list) - 1\n",
    "        indices += (it_index).tolist()\n",
    "        all_data += [1]*len(it_index)\n",
    "        indptr.append(indptr[-1] + len(it_index.tolist()))\n",
    "    results = (all_data, indices, indptr)\n",
    "    return results\n",
    "\n",
    "ui_co_mat = spar_matrix(train_sess)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 返回 [[u1,u2],[attr1_opin, attr1_opin]]\n",
    "def formulate_seq(dict_ui, dict_op):\n",
    "    results = [[],[]]\n",
    "    temp_res = [[],[]]\n",
    "    max_len =0\n",
    "    for x in dict_ui.keys():\n",
    "        seqs_id = dict_ui[x]\n",
    "        opin_id = dict_op[x]\n",
    "        new_us_seq = []\n",
    "        new_op_seq = []\n",
    "        for id_t, op_t in zip(seqs_id, opin_id):\n",
    "            if op_t != 0 and id_t not in new_us_seq:\n",
    "                new_us_seq.append(id_t)\n",
    "                new_op_seq.append(op_t)\n",
    "        if len(new_us_seq)==0:\n",
    "            new_us_seq.append(0)\n",
    "            new_op_seq.append(0)\n",
    "        if len(new_us_seq)>max_len:\n",
    "            max_len = len(new_us_seq)\n",
    "        temp_res[0].append(new_us_seq)\n",
    "        temp_res[1].append(new_op_seq)\n",
    "    for ui_seq, op_seq in zip(temp_res[0],temp_res[1]):\n",
    "        pad_len = max_len - len(ui_seq)\n",
    "        pad_ui_seq = ui_seq + [0]*pad_len\n",
    "        pad_op_seq = op_seq + [0]*pad_len\n",
    "        results[0].append(pad_ui_seq)\n",
    "        results[1].append(pad_op_seq)\n",
    "#     print('max_len:',str(max_len))\n",
    "    return results\n",
    "\n",
    "# u_seq = {1:[1,2,3,1],2:[4,5], 3:[7,8,9]}\n",
    "# u_op = {1:[45,56,0,45],2:[66,0], 3:[0,33,0]}\n",
    "# # [[[1,2],[4],[8]],[[45,56],[66],[33]]]\n",
    "# formulate_seq(u_seq,u_op)\n",
    "\n",
    "ui_1 = formulate_seq(train_sess,attr1_all)\n",
    "ui_2 = formulate_seq(train_sess,attr2_all)\n",
    "ui_3 = formulate_seq(train_sess,attr3_all)\n",
    "ui_4 = formulate_seq(train_sess,attr4_all)\n",
    "ui_5 = formulate_seq(train_sess,attr5_all)\n",
    "ui_6 = formulate_seq(train_sess,attr6_all)\n",
    "ui_7 = formulate_seq(train_sess,attr7_all)\n",
    "iu_1 = formulate_seq(train_items,attr1_item_opin)\n",
    "iu_2 = formulate_seq(train_items,attr2_item_opin)\n",
    "iu_3 = formulate_seq(train_items,attr3_item_opin)\n",
    "iu_4 = formulate_seq(train_items,attr4_item_opin)\n",
    "iu_5 = formulate_seq(train_items,attr5_item_opin)\n",
    "iu_6 = formulate_seq(train_items,attr6_item_opin)\n",
    "iu_7 = formulate_seq(train_items,attr7_item_opin)\n",
    "\n",
    "if meta_dataset == 'Beauty' or meta_dataset == 'Home_and_Kitchen':\n",
    "    ui_8 = formulate_seq(train_sess,attr8_all)\n",
    "    ui_9 = formulate_seq(train_sess,attr9_all)\n",
    "    iu_8 = formulate_seq(train_items,attr8_item_opin)\n",
    "    iu_9 = formulate_seq(train_items,attr9_item_opin)\n",
    "\n",
    "def ui_seq(dict_ui):\n",
    "    results = []\n",
    "    temp_res = []\n",
    "    max_len =0\n",
    "    for x in dict_ui.keys():\n",
    "        seqs_id = dict_ui[x]\n",
    "        if len(seqs_id)>max_len:\n",
    "            max_len = len(seqs_id)\n",
    "        temp_res.append(seqs_id)\n",
    "    for temp_l in temp_res:\n",
    "        pad_len = max_len - len(temp_l)\n",
    "        pad_ui_seq = temp_l + [0]*pad_len\n",
    "        results.append(pad_ui_seq)\n",
    "    return results\n",
    "\n",
    "ui_list = ui_seq(train_sess)\n",
    "iu_list = ui_seq(train_items)\n",
    "\n",
    "print('#interactions: ',inter_count)\n",
    "\n",
    "print('sequence average length: ', (inter_count)/(user_count) * 1.0)\n",
    "# print('cold sequence average length: ', (all_train+all_tes_nocold+all_tes_cold)/(len(tra_seqs) + len(tes_seqs_nocold) +len(tes_seqs_cold) * 1.0))\n",
    "train_data_path = './datasets/' + meta_dataset\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "path_data_train = train_data_path + \"/train.txt\"\n",
    "path_normal_test = train_data_path + \"/test.txt\"\n",
    "\n",
    "tra_save = (tra[0], tra[1], tra[2],item_co_mat,user_co_mat, ui_list, iu_list, ui_1, ui_2, ui_3, ui_4, ui_5, ui_6, ui_7, iu_1, iu_2, iu_3, iu_4, iu_5, iu_6, iu_7)\n",
    "tes_save = (tes[0], tes[1], tes[2])\n",
    "\n",
    "if meta_dataset == 'Beauty' or meta_dataset == 'Home_and_Kitchen':\n",
    "    tra_save = (tra[0], tra[1], tra[2],item_co_mat,user_co_mat, ui_list, iu_list, ui_1, ui_2, ui_3, ui_4, ui_5, ui_6, ui_7,ui_8, ui_9, iu_1, iu_2, iu_3, iu_4, iu_5, iu_6, iu_7, iu_8, iu_9)\n",
    "\n",
    "pickle.dump(tra_save, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes_save, open(path_normal_test, 'wb'))\n",
    "print(\"dataset: \", meta_dataset)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NARM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_num:  12391\n",
      "train_seq_num & user_num:  12373\n",
      "test_seq_num:  12311\n",
      "#interactions:  110313\n",
      "sequence average length:  8.915622726905358\n",
      "NARM dataset:  Yelp\n",
      "2023-12-29 08:51:52\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "meta_dataset = 'Yelp'\n",
    "data_path =  './dict/' + meta_dataset + '/session_data.csv'\n",
    "\n",
    "if meta_dataset == 'Beauty' or meta_dataset == 'Beauty_4':\n",
    "    # 'Brand','Ingredients','Effectiveness','Scent','Price','Color','Size'\n",
    "    attr_list = ['Brand','Ingredients','Effectiveness','Scent','Price','Color','Size']\n",
    "elif meta_dataset == 'Cell_Phones_and_Accessories':\n",
    "    # Brand, Performance, color, Battery ,Connectivity,Price, Size\n",
    "    attr_list = ['Brand', 'Performance', 'Color', 'Battery' ,'Connectivity','Price', 'Size']\n",
    "elif meta_dataset == 'Clothing_Shoes_and_Jewelry':\n",
    "    # Price, Style, Size, Fabric, Brand, Quality, Color\n",
    "    attr_list = ['Price', 'Style', 'Size', 'Fabric', 'Brand', 'Quality', 'Color']\n",
    "elif meta_dataset == 'Sports_and_Outdoors':\n",
    "    # Price, Quality, Functionality,Size,Protection,Material,Comfort\n",
    "    attr_list = ['Price', 'Quality', 'Functionality', 'Size', 'Protection', 'Material', 'Comfort']\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data_all = data_all.reset_index()[['sessionID', 'itemID']]\n",
    "# data_all = data_all[['sessionID', 'itemID']]\n",
    "\n",
    "\n",
    "# recorde all items\n",
    "# dict {sessionID:[itemID,itemID]} \n",
    "sess_all = {}\n",
    "#  dict {sessionID:[opinID,opinID]}\n",
    "\n",
    "\n",
    "item_count = 0\n",
    "user_count = 0\n",
    "inter_count = 0\n",
    "\n",
    "for _, row in data_all.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    if sess_id in sess_all: \n",
    "        sess_all[sess_id].append(item_id)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "\n",
    "# 限制每个seq长度\n",
    "for sess_temp in sess_all.keys():\n",
    "    seq = sess_all[sess_temp]\n",
    "    if len(seq) >20:\n",
    "        sess_all[sess_temp] = seq[-20:]\n",
    "# 形成train set\n",
    "# 统计 train set 出现的item，给item重新编号\n",
    "item_num = 1\n",
    "# dict{lod_itemID:new_itemID}\n",
    "item_dict = {}\n",
    "# train_set [[x1, x2, x3], [label...]\n",
    "tra = [[],[]]\n",
    "# dict {train_sessionID:[itemID,itemID]}  去掉了seq中last item(不包含test的label)， 也可以调整为去掉两个item\n",
    "train_sess = {}\n",
    "\n",
    "# dict {item_id:[user_id, user_id]}\n",
    "train_items = {}\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    train_seqs_t = all_seqs[:-1]\n",
    "    inter_count += len(train_seqs_t)\n",
    "#     item_seq with new ID\n",
    "    new_seq = []\n",
    "    for x in train_seqs_t:\n",
    "        if x in item_dict:\n",
    "            new_seq.append(item_dict[x])\n",
    "            if sess_temp not in train_items[item_dict[x]]:\n",
    "                train_items[item_dict[x]].append(sess_temp)\n",
    "        else:\n",
    "            item_dict[x] = item_num\n",
    "            new_seq.append(item_num)\n",
    "            train_items[item_num] = [] \n",
    "            train_items[item_num].append(sess_temp)\n",
    "            \n",
    "            item_num += 1\n",
    "    train_sess[sess_temp] = new_seq\n",
    "    tra[0].append(new_seq[:-1])\n",
    "#     预测时考虑last item\n",
    "    tra[1].append(new_seq[-1])\n",
    "    \n",
    "item_count =  item_num-1\n",
    "user_count = len(tra[0])\n",
    "\n",
    "# test_set [[x_id ...],[label...]\n",
    "tes = [[],[]]\n",
    "tes_seq_num = 0\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if all_seqs[-1] in item_dict:\n",
    "        new_seq = []\n",
    "        for x in all_seqs:\n",
    "            new_seq.append(item_dict[x])\n",
    "        tes_seq_num += 1\n",
    "        tes[0].append(new_seq[:-1])\n",
    "    #     预测时考虑last item\n",
    "        tes[1].append(new_seq[-1])\n",
    "    \n",
    "\n",
    "print('item_num: ',str(item_count))\n",
    "print('train_seq_num & user_num: ',str(user_count))\n",
    "print('test_seq_num: ',str(tes_seq_num))\n",
    "\n",
    "inter_count += tes_seq_num\n",
    "\n",
    "\n",
    "print('#interactions: ',inter_count)\n",
    "\n",
    "print('sequence average length: ', (inter_count)/(user_count) * 1.0)\n",
    "# print('cold sequence average length: ', (all_train+all_tes_nocold+all_tes_cold)/(len(tra_seqs) + len(tes_seqs_nocold) +len(tes_seqs_cold) * 1.0))\n",
    "train_data_path = './NARM/' + meta_dataset\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "path_data_train = train_data_path + \"/train.txt\"\n",
    "path_normal_test = train_data_path + \"/test.txt\"\n",
    "tra_save = (tra[0], tra[1])\n",
    "tes_save = (tes[0], tes[1])\n",
    "\n",
    "pickle.dump(tra_save, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes_save, open(path_normal_test, 'wb'))\n",
    "print(\"NARM dataset: \", meta_dataset)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT4Rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "meta_dataset = 'Cell_Phones_and_Accessories'\n",
    "data_path =  './dict/' + meta_dataset + '/session_data.csv'\n",
    "\n",
    "if meta_dataset == 'Beauty' or meta_dataset == 'Beauty_4':\n",
    "    # 'Brand','Ingredients','Effectiveness','Scent','Price','Color','Size'\n",
    "    attr_list = ['Brand','Ingredients','Effectiveness','Scent','Price','Color','Size']\n",
    "elif meta_dataset == 'Cell_Phones_and_Accessories':\n",
    "    # Brand, Performance, color, Battery ,Connectivity,Price, Size\n",
    "    attr_list = ['Brand', 'Performance', 'Color', 'Battery' ,'Connectivity','Price', 'Size']\n",
    "elif meta_dataset == 'Clothing_Shoes_and_Jewelry':\n",
    "    # Price, Style, Size, Fabric, Brand, Quality, Color\n",
    "    attr_list = ['Price', 'Style', 'Size', 'Fabric', 'Brand', 'Quality', 'Color']\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data_all = data_all.reset_index()[['sessionID', 'itemID', attr_list[0], attr_list[1],attr_list[2],attr_list[3],attr_list[4],attr_list[5],attr_list[6]]]\n",
    "data_all = data_all[['sessionID', 'itemID']]\n",
    "\n",
    "\n",
    "# recorde all items\n",
    "# dict {sessionID:[itemID,itemID]} \n",
    "sess_all = {}\n",
    "#  dict {sessionID:[opinID,opinID]}\n",
    "\n",
    "\n",
    "item_count = 0\n",
    "user_count = 0\n",
    "inter_count = 0\n",
    "\n",
    "for _, row in data_all.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "\n",
    "# 限制每个seq长度\n",
    "for sess_temp in sess_all.keys():\n",
    "    seq = sess_all[sess_temp]\n",
    "    if len(seq) >20:\n",
    "        sess_all[sess_temp] = seq[-20:]\n",
    "        \n",
    "# 形成train set\n",
    "# 统计 train set 出现的item，给item重新编号\n",
    "item_num = 1\n",
    "# dict{lod_itemID:new_itemID}\n",
    "item_dict = {}\n",
    "# train_set [[user1,user2],[x1, x2, x3], [label...]\n",
    "tra = [[],[],[]]\n",
    "# dict {train_sessionID:[itemID,itemID]}  去掉了seq中last item(不包含test的label)， 也可以调整为去掉两个item\n",
    "train_sess = {}\n",
    "\n",
    "# dict {item_id:[user_id, user_id]}\n",
    "train_items = {}\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    train_seqs_t = all_seqs[:-1]\n",
    "    inter_count += len(train_seqs_t)\n",
    "#     item_seq with new ID\n",
    "    new_seq = []\n",
    "    for x in train_seqs_t:\n",
    "        if x in item_dict:\n",
    "            new_seq.append(item_dict[x])\n",
    "            if sess_temp not in train_items[item_dict[x]]:\n",
    "                train_items[item_dict[x]].append(sess_temp)\n",
    "        else:\n",
    "            item_dict[x] = item_num\n",
    "            new_seq.append(item_num)\n",
    "            train_items[item_num] = [] \n",
    "            train_items[item_num].append(sess_temp)\n",
    "            \n",
    "            item_num += 1\n",
    "    train_sess[sess_temp] = new_seq\n",
    "    tra[0].append(sess_temp)\n",
    "    tra[1].append(new_seq[:-1])\n",
    "#     预测时考虑last item\n",
    "    tra[2].append(new_seq[-1])\n",
    "    \n",
    "item_count =  item_num-1\n",
    "user_count = len(tra[0])\n",
    "\n",
    "# test_set [[user1,user2],[x_id ...],[label...]\n",
    "tes = [[],[],[]]\n",
    "tes_seq_num = 0\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if all_seqs[-1] in item_dict:\n",
    "        new_seq = []\n",
    "        for x in all_seqs:\n",
    "            new_seq.append(item_dict[x])\n",
    "        tes_seq_num += 1\n",
    "        tes[0].append(sess_temp)\n",
    "        tes[1].append(new_seq[:-1])\n",
    "    #     预测时考虑last item\n",
    "        tes[2].append(new_seq[-1])\n",
    "    \n",
    "\n",
    "print('item_num: ',str(item_count))\n",
    "print('train_seq_num & user_num: ',str(user_count))\n",
    "print('test_seq_num: ',str(tes_seq_num))\n",
    "\n",
    "inter_count += tes_seq_num\n",
    "\n",
    "\n",
    "print('#interactions: ',inter_count)\n",
    "\n",
    "print('sequence average length: ', (inter_count)/(user_count) * 1.0)\n",
    "# print('cold sequence average length: ', (all_train+all_tes_nocold+all_tes_cold)/(len(tra_seqs) + len(tes_seqs_nocold) +len(tes_seqs_cold) * 1.0))\n",
    "train_data_path = './BERT4Rec/' + meta_dataset\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "path_data_train = train_data_path + \"/train.txt\"\n",
    "path_normal_test = train_data_path + \"/test.txt\"\n",
    "tra_save = (tra[0], tra[1], tra[2])\n",
    "tes_save = (tes[0], tes[1], tes[2])\n",
    "\n",
    "pickle.dump(tra_save, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes_save, open(path_normal_test, 'wb'))\n",
    "print(\"BERT4Rec dataset: \", meta_dataset)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UniSRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_num:  12390\n",
      "train_seq_num & user_num:  12373\n",
      "test_seq_num:  12311\n",
      "#interactions:  110313\n",
      "sequence average length:  8.915622726905358\n",
      "dataset:  Yelp\n",
      "model: UniSRec\n",
      "2023-12-29 08:52:16\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "datasets_name = 'Yelp'\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "# data_path = 'digineticaBuy/digineticaBuy_data.csv'\n",
    "\n",
    "# datasets_name = '2019-Oct'\n",
    "# price_num = 5\n",
    "\n",
    "# data_path = 'cosmeticsdata/' + datasets_name + '_data.csv'\n",
    "# datasets_name = 'digineticaBuy'\n",
    "# data_path = 'digineticaBuy/' + datasets_name + '_data.csv'\n",
    "\n",
    "\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID', 'text']]\n",
    "\n",
    "# item_inter_num = pd.DataFrame(data.groupby(data['itemID']).count())\n",
    "# item_num=item_inter_num.reset_index().rename(columns={'sessionID':'item_num'})\n",
    "# data = pd.merge(data, item_num, how='left', on = 'itemID')\n",
    "# data = data[data['item_num']>9]\n",
    "# data = data[['sessionID', 'itemID']]\n",
    "\n",
    "item_count = 0\n",
    "user_count = 0\n",
    "inter_count = 0\n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "#  dict {old_itemID: title}\n",
    "olditem_title = {}\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    if item_id not in olditem_title:\n",
    "        olditem_title[item_id] = row['text']\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "\n",
    "# 限制每个seq长度\n",
    "for sess_temp in sess_all.keys():\n",
    "    seq = sess_all[sess_temp]\n",
    "    if len(seq) >20:\n",
    "        sess_all[sess_temp] = seq[-20:]\n",
    "        \n",
    "item_num = 0\n",
    "# dict{lod_itemID:new_itemID}\n",
    "item_dict = {}\n",
    "# dict {new_item_id:text}\n",
    "newitem_title = {}\n",
    "# train_set [[u1, u2],[x1, x2, x3], [label...]\n",
    "tra = [[],[],[]]\n",
    "val = [[], [], []]\n",
    "# dict {train_sessionID:[itemID,itemID]}  去掉了seq中last item(不包含test的label)， 也可以调整为去掉两个item\n",
    "train_sess = {}\n",
    "\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    train_seqs_t = all_seqs[:-1]\n",
    "    inter_count += len(train_seqs_t)\n",
    "#     item_seq with new ID\n",
    "    new_seq = []\n",
    "    for x in train_seqs_t:\n",
    "        if x in item_dict:\n",
    "            new_seq.append(item_dict[x])\n",
    "        else:\n",
    "            item_dict[x] = item_num\n",
    "            newitem_title[item_num] = olditem_title[x]\n",
    "            new_seq.append(item_num)\n",
    "            \n",
    "            item_num += 1\n",
    "    train_sess[sess_temp] = new_seq\n",
    "    tra[0].append(sess_temp)\n",
    "    tra[1].append(new_seq[:-1])\n",
    "#     预测时考虑last item\n",
    "    tra[2].append(new_seq[-1])\n",
    "    val[0].append(sess_temp)\n",
    "    val[1].append(new_seq[:-2])\n",
    "#     预测时考虑last item\n",
    "    val[2].append(new_seq[-2])\n",
    "    \n",
    "item_count =  item_num-1\n",
    "user_count = len(tra[0])\n",
    "\n",
    "# test_set [[x_id ...],[label...]\n",
    "tes = [[],[],[]]\n",
    "tes_seq_num = 0\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if all_seqs[-1] in item_dict:\n",
    "        new_seq = []\n",
    "        for x in all_seqs:\n",
    "            new_seq.append(item_dict[x])\n",
    "        tes_seq_num += 1\n",
    "        tes[0].append(sess_temp)\n",
    "        tes[1].append(new_seq[:-1])\n",
    "    #     预测时考虑last item\n",
    "        tes[2].append(new_seq[-1])\n",
    "    \n",
    "\n",
    "print('item_num: ',str(item_count))\n",
    "print('train_seq_num & user_num: ',str(user_count))\n",
    "print('test_seq_num: ',str(tes_seq_num))\n",
    "\n",
    "inter_count += tes_seq_num\n",
    "\n",
    "\n",
    "print('#interactions: ',inter_count)\n",
    "\n",
    "print('sequence average length: ', (inter_count)/(user_count) * 1.0)\n",
    "\n",
    "train_data_path = './UniSRec/' + datasets_name + '/'\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "    \n",
    "path_data_train = train_data_path + f'{datasets_name}Senti.train.inter'\n",
    "path_data_valid = train_data_path + f'{datasets_name}Senti.valid.inter'\n",
    "path_data_test = train_data_path + f'{datasets_name}Senti.test.inter'\n",
    "# tra[0], tra[1], tra[2]\n",
    "# tes[0], tes[1], tes[2]\n",
    "# split_valid = int(len(tra[2])/9*7)\n",
    "# train_sess = tr_seqs[:split_valid]\n",
    "# train_labs = tr_labs[:split_valid]\n",
    "# valid_sess = tr_seqs[split_valid:]\n",
    "# valid_labs = tr_labs[split_valid:]\n",
    "\n",
    "with open(path_data_train, 'w') as file:\n",
    "    file.write('user_id:token\\titem_id_list:token_seq\\titem_id:token\\n')\n",
    "    for index, sess, lab in zip(tra[0], tra[1], tra[2]):\n",
    "        i_str = str(index)\n",
    "        s_str = [str(x) for x in sess]\n",
    "        l_str = str(lab)\n",
    "        file.write(f'{i_str}\\t{\" \".join(s_str)}\\t{l_str}\\n')\n",
    "                   \n",
    "with open(path_data_valid, 'w') as file:\n",
    "    file.write('user_id:token\\titem_id_list:token_seq\\titem_id:token\\n')\n",
    "    for index, sess, lab in zip(val[0], val[1], val[2]):\n",
    "        i_str = str(index)\n",
    "        s_str = [str(x) for x in sess]\n",
    "        l_str = str(lab)\n",
    "        file.write(f'{i_str}\\t{\" \".join(s_str)}\\t{l_str}\\n')\n",
    "                   \n",
    "with open(path_data_test, 'w') as file:\n",
    "    file.write('user_id:token\\titem_id_list:token_seq\\titem_id:token\\n')\n",
    "    for index, sess, lab in zip(tes[0], tes[1], tes[2]):\n",
    "        i_str = str(index)\n",
    "        s_str = [str(x) for x in sess]\n",
    "        l_str = str(lab)\n",
    "        file.write(f'{i_str}\\t{\" \".join(s_str)}\\t{l_str}\\n')\n",
    "\n",
    "uni_text_path = train_data_path + 'textlist.npy'\n",
    "np.save(uni_text_path, list(newitem_title.values()))                   \n",
    "\n",
    "print(\"dataset: \", datasets_name)\n",
    "print(\"model: UniSRec\")\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCLRec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:  Yelp\n",
      "model: MCLRec\n",
      "2023-12-29 08:52:54\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "datasets_name = 'Yelp'\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "# data_path = 'digineticaBuy/digineticaBuy_data.csv'\n",
    "\n",
    "# datasets_name = '2019-Oct'\n",
    "# price_num = 5\n",
    "\n",
    "# data_path = 'cosmeticsdata/' + datasets_name + '_data.csv'\n",
    "# datasets_name = 'digineticaBuy'\n",
    "# data_path = 'digineticaBuy/' + datasets_name + '_data.csv'\n",
    "\n",
    "\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID']]\n",
    "\n",
    "# item_inter_num = pd.DataFrame(data.groupby(data['itemID']).count())\n",
    "# item_num=item_inter_num.reset_index().rename(columns={'sessionID':'item_num'})\n",
    "# data = pd.merge(data, item_num, how='left', on = 'itemID')\n",
    "# data = data[data['item_num']>9]\n",
    "# data = data[['sessionID', 'itemID']]\n",
    "\n",
    "item_count = 0\n",
    "user_count = 0\n",
    "inter_count = 0\n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = int(row['sessionID'])\n",
    "    item_id = row['itemID']\n",
    "    \n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "# 限制每个seq长度\n",
    "for sess_temp in sess_all.keys():\n",
    "    seq = sess_all[sess_temp]\n",
    "    if len(seq) >20:\n",
    "        sess_all[sess_temp] = seq[-20:]\n",
    "        \n",
    "item_num =1\n",
    "# dict{lod_itemID:new_itemID}\n",
    "item_dict = {}\n",
    "# train_set [[u1, u2],[x1, x2, x3], [time...]\n",
    "tra = [[],[]]\n",
    "# val = [[], [], []]\n",
    "# dict {train_sessionID:[itemID,itemID]}  去掉了seq中last item(不包含test的label)， 也可以调整为去掉两个item\n",
    "train_sess = {}\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    inter_count += len(train_seqs_t)\n",
    "#     item_seq with new ID\n",
    "    new_seq = []\n",
    "    for x in all_seqs:\n",
    "        if x in item_dict:\n",
    "            new_seq.append(item_dict[x])\n",
    "        else:\n",
    "            item_dict[x] = item_num\n",
    "            new_seq.append(item_num)\n",
    "            item_num += 1\n",
    "    train_sess[sess_temp] = new_seq\n",
    "    tra[0] += [sess_temp]*len(new_seq)\n",
    "    tra[1] += new_seq\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "train_data_path = './MCLRec/' + datasets_name + '/'\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "    \n",
    "path_data_test = train_data_path + datasets_name + '.txt'\n",
    "\n",
    "\n",
    "# tra_dict = {'user_id':tra[0], 'item_id':tra[1], 'timestamp':tra[2]}\n",
    "# tra_df = pd.DataFrame(tra_dict)\n",
    "       \n",
    "# tes_dict = {'user_id':tes[0], 'item_id':tes[1], 'timestamp':tes[2]}\n",
    "# tes_df = pd.DataFrame(tes_dict)\n",
    "\n",
    "pickle.dump(tra, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes, open(path_data_test, 'wb'))\n",
    "\n",
    "path_inter = train_data_path + f'{datasets_name}.inter'\n",
    "\n",
    "\n",
    "with open(path_inter, 'w') as file:\n",
    "    file.write('user_id:token\\titem_id:token\\n')\n",
    "    for u_id, i_id in zip(tra[0], tra[1]):\n",
    "        u_str = str(u_id)\n",
    "        i_str = str(i_id)\n",
    "        file.write(f'{u_str}\\t{i_str}\\n')  \n",
    "        \n",
    "with open(path_data_test, 'w') as file:\n",
    "    for sess_temp in train_sess.keys():\n",
    "        u_str = str(sess_temp)\n",
    "        sess = train_sess[sess_temp]\n",
    "        s_str = [str(x) for x in sess]\n",
    "        file.write(f'{i_str} {\" \".join(s_str)}\\n')\n",
    "\n",
    "print(\"dataset: \", datasets_name)\n",
    "print(\"model: MCLRec\")\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTSR & FEARec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:  Yelp\n",
      "model: ACTSR\n",
      "2023-12-29 08:53:08\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "datasets_name = 'Yelp'\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "# data_path = 'digineticaBuy/digineticaBuy_data.csv'\n",
    "\n",
    "# datasets_name = '2019-Oct'\n",
    "# price_num = 5\n",
    "\n",
    "# data_path = 'cosmeticsdata/' + datasets_name + '_data.csv'\n",
    "# datasets_name = 'digineticaBuy'\n",
    "# data_path = 'digineticaBuy/' + datasets_name + '_data.csv'\n",
    "\n",
    "\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID']]\n",
    "\n",
    "# item_inter_num = pd.DataFrame(data.groupby(data['itemID']).count())\n",
    "# item_num=item_inter_num.reset_index().rename(columns={'sessionID':'item_num'})\n",
    "# data = pd.merge(data, item_num, how='left', on = 'itemID')\n",
    "# data = data[data['item_num']>9]\n",
    "# data = data[['sessionID', 'itemID']]\n",
    "\n",
    "item_count = 0\n",
    "user_count = 0\n",
    "inter_count = 0\n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = int(row['sessionID'])\n",
    "    item_id = row['itemID']\n",
    "    \n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "# 限制每个seq长度\n",
    "for sess_temp in sess_all.keys():\n",
    "    seq = sess_all[sess_temp]\n",
    "    if len(seq) >20:\n",
    "        sess_all[sess_temp] = seq[-20:]\n",
    "        \n",
    "item_num =1\n",
    "# dict{lod_itemID:new_itemID}\n",
    "item_dict = {}\n",
    "# train_set [[u1, u2],[x1, x2, x3], [time...]\n",
    "tra = [[],[]]\n",
    "# val = [[], [], []]\n",
    "# dict {train_sessionID:[itemID,itemID]}  去掉了seq中last item(不包含test的label)， 也可以调整为去掉两个item\n",
    "train_sess = {}\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    inter_count += len(train_seqs_t)\n",
    "#     item_seq with new ID\n",
    "    new_seq = []\n",
    "    for x in all_seqs:\n",
    "        if x in item_dict:\n",
    "            new_seq.append(item_dict[x])\n",
    "        else:\n",
    "            item_dict[x] = item_num\n",
    "            new_seq.append(item_num)\n",
    "            item_num += 1\n",
    "    train_sess[sess_temp] = new_seq\n",
    "    tra[0] += [sess_temp]*len(new_seq)\n",
    "    tra[1] += new_seq\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "train_data_path = './ACTSR/' + datasets_name + '/'\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "    \n",
    "path_data_test = train_data_path + datasets_name + '.txt'\n",
    "\n",
    "\n",
    "# tra_dict = {'user_id':tra[0], 'item_id':tra[1], 'timestamp':tra[2]}\n",
    "# tra_df = pd.DataFrame(tra_dict)\n",
    "       \n",
    "# tes_dict = {'user_id':tes[0], 'item_id':tes[1], 'timestamp':tes[2]}\n",
    "# tes_df = pd.DataFrame(tes_dict)\n",
    "\n",
    "pickle.dump(tra, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes, open(path_data_test, 'wb'))\n",
    "\n",
    "path_inter = train_data_path + f'{datasets_name}.inter'\n",
    "\n",
    "\n",
    "with open(path_inter, 'w') as file:\n",
    "    file.write('user_id:token\\titem_id:token\\ttimestamp:token\\n')\n",
    "    for u_id, i_id, t_time in zip(tra[0], tra[1], range(len(tra[0]))):\n",
    "        u_str = str(u_id)\n",
    "        i_str = str(i_id)\n",
    "        t_str = str(t_time)\n",
    "        file.write(f'{u_str}\\t{i_str}\\t{t_str}\\n')  \n",
    "        \n",
    "with open(path_data_test, 'w') as file:\n",
    "    for sess_temp in train_sess.keys():\n",
    "        u_str = str(sess_temp)\n",
    "        sess = train_sess[sess_temp]\n",
    "        s_str = [str(x) for x in sess]\n",
    "        file.write(f'{i_str} {\" \".join(s_str)}\\n')\n",
    "\n",
    "print(\"dataset: \", datasets_name)\n",
    "print(\"model: ACTSR\")\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word number: 82627\n",
      "item_num:  12391\n",
      "train_seq_num & user_num:  12373\n",
      "test_seq_num:  0\n",
      "#interactions:  98002\n",
      "sequence average length:  7.920633637759638\n",
      "dataset:  Yelp\n",
      "model: RNS\n",
      "2023-12-29 08:53:32\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "datasets_name = 'Yelp'\n",
    "# Cell_Phones_and_Accessories/\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "# data_path = 'digineticaBuy/digineticaBuy_data.csv'\n",
    "\n",
    "# datasets_name = '2019-Oct'\n",
    "# price_num = 5\n",
    "\n",
    "# data_path = 'cosmeticsdata/' + datasets_name + '_data.csv'\n",
    "# datasets_name = 'digineticaBuy'\n",
    "# data_path = 'digineticaBuy/' + datasets_name + '_data.csv'\n",
    "\n",
    "\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID','review_time', 'reviewText']]\n",
    "\n",
    "# item_inter_num = pd.DataFrame(data.groupby(data['itemID']).count())\n",
    "# item_num=item_inter_num.reset_index().rename(columns={'sessionID':'item_num'})\n",
    "# data = pd.merge(data, item_num, how='left', on = 'itemID')\n",
    "# data = data[data['item_num']>9]\n",
    "# data = data[['sessionID', 'itemID']]\n",
    "\n",
    "item_count = 0\n",
    "user_count = 0\n",
    "inter_count = 0\n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "\n",
    "# dict {sessionID:reviewtext1}\n",
    "u_review = {}\n",
    "# dict {sessionID:[time,time]}\n",
    "# u_i_time = {}\n",
    "#  dict {old_itemID: review}\n",
    "olditem_review = {}\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = int(row['sessionID'])-1\n",
    "    item_id = row['itemID']\n",
    "    review_t = row['reviewText']\n",
    "    time_t = row['review_time']\n",
    "    if item_id not in olditem_review:\n",
    "        olditem_review[item_id] = review_t\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "        u_review[sess_id] = review_t\n",
    "\n",
    "# 限制每个seq长度\n",
    "for sess_temp in sess_all.keys():\n",
    "    seq = sess_all[sess_temp]\n",
    "    if len(seq) >20:\n",
    "        sess_all[sess_temp] = seq[-20:]\n",
    "        \n",
    "item_num = 0\n",
    "# dict{lod_itemID:new_itemID}\n",
    "item_dict = {}\n",
    "# dict {new_item_id:text}\n",
    "newitem_review = {}\n",
    "# train_set [[u1, u2],[x1, x2, x3], [label...]\n",
    "tra = [[],[],[],[]]\n",
    "# val = [[], [], []]\n",
    "# dict {train_sessionID:[itemID,itemID]}  去掉了seq中last item(不包含test的label)， 也可以调整为去掉两个item\n",
    "train_sess = {}\n",
    "\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    train_seqs_t = all_seqs[:-1]\n",
    "    inter_count += len(train_seqs_t)\n",
    "#     item_seq with new ID\n",
    "    new_seq = []\n",
    "    for x in train_seqs_t:\n",
    "        if x in item_dict:\n",
    "            new_seq.append(item_dict[x])\n",
    "        else:\n",
    "            item_dict[x] = item_num\n",
    "            newitem_review[item_num] = olditem_review[x]\n",
    "            new_seq.append(item_num)\n",
    "            \n",
    "            item_num += 1\n",
    "    train_sess[sess_temp] = new_seq\n",
    "    tra[0].append([sess_temp]*len(new_seq))\n",
    "    tra[1].append(new_seq)\n",
    "    tra[2].append([1]*len(new_seq))\n",
    "    tra[3].append([2]*len(new_seq))\n",
    "#     预测时考虑last item\n",
    "    \n",
    "item_count =  item_num\n",
    "user_count = len(tra[0])\n",
    "\n",
    "# test_set [[x_id ...],[label...]\n",
    "tes = [[],[],[],[]]\n",
    "tes_seq_num = 0\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if all_seqs[-1] in item_dict:\n",
    "        tes[0].append(sess_temp)\n",
    "        tes[1].append(item_dict[all_seqs[-1]])\n",
    "        tes[2].append(1)\n",
    "        tes[3].append(2)\n",
    "    else:\n",
    "        tes[0].append(sess_temp)\n",
    "        tes[1].append(0)\n",
    "        tes[2].append(1)\n",
    "        tes[3].append(2)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "word_dict = {}\n",
    "word_num = 0\n",
    "u_num = {}\n",
    "max_review_word = 30\n",
    "for u_t in u_review.keys():\n",
    "    rev = u_review[u_t].split(' ')\n",
    "    if len(rev) == 0:\n",
    "        u_num[u_t] = [[0]]\n",
    "    else:\n",
    "        u_num[u_t] = []\n",
    "        wor_l = []\n",
    "        for word_t in rev:\n",
    "            if word_t in word_dict:\n",
    "                wor_l.append(word_dict[word_t])\n",
    "            else:\n",
    "                word_dict[word_t] = word_num\n",
    "                wor_l.append(word_num)\n",
    "                word_num +=1\n",
    "        if len(wor_l) < max_review_word:\n",
    "            wor_l = wor_l + [0]*(max_review_word-len(wor_l))\n",
    "        else:\n",
    "            wor_l = wor_l[:max_review_word]\n",
    "        u_num[u_t].append(wor_l)\n",
    "        \n",
    "word_dict[0]='a12sasa2'\n",
    "                    \n",
    "\n",
    "i_num = {}\n",
    "for i_t in newitem_review.keys():\n",
    "    rev = newitem_review[i_t].split(' ')\n",
    "    if len(rev) == 0:\n",
    "        i_num[i_t] = [[0]]\n",
    "    else:\n",
    "        i_num[i_t] = []\n",
    "        wor_l = []\n",
    "        for word_t in rev:\n",
    "            if word_t in word_dict:\n",
    "                wor_l.append(word_dict[word_t])\n",
    "            else:\n",
    "                word_dict[word_t] = word_num\n",
    "                wor_l.append(word_num)\n",
    "                word_num +=1\n",
    "        if len(wor_l) < max_review_word:\n",
    "            wor_l = wor_l + [0]*(max_review_word-len(wor_l))\n",
    "        else:\n",
    "            wor_l = wor_l[:max_review_word]\n",
    "        i_num[i_t].append(wor_l)\n",
    "\n",
    "print('word number:',str(word_num-1))\n",
    "\n",
    "print('item_num: ',str(item_count))\n",
    "print('train_seq_num & user_num: ',str(user_count))\n",
    "print('test_seq_num: ',str(tes_seq_num))\n",
    "\n",
    "inter_count += tes_seq_num\n",
    "\n",
    "\n",
    "print('#interactions: ',inter_count)\n",
    "\n",
    "print('sequence average length: ', (inter_count)/(user_count) * 1.0)\n",
    "\n",
    "train_data_path = './RNS/' + datasets_name + '/'\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "    \n",
    "path_data_train = train_data_path + 'train.csv'\n",
    "path_data_test = train_data_path + 'test.csv'\n",
    "path_i_text = train_data_path + 'i_text'\n",
    "path_u_text = train_data_path + '/u_text'\n",
    "path_voc = train_data_path + 'vocabulary'\n",
    "\n",
    "\n",
    "with open(path_data_train, 'w') as file:\n",
    "#     file.write('user_id:token\\titem_id_list:token_seq\\titem_id:token\\n')\n",
    "    for u,i, r, t in zip(tra[0], tra[1], tra[2], tra[3]):\n",
    "        for u_t, i_t, r_t, t_t in zip(u,i,r,t):\n",
    "            str_1 = str(u_t)\n",
    "            str_2 = str(i_t)\n",
    "            str_3 = str(r_t)\n",
    "            str_4 = str(t_t)\n",
    "            file.write(f'{str_1},{str_2},{str_3},{str_4}\\n')\n",
    "                   \n",
    "                   \n",
    "with open(path_data_test, 'w') as file:\n",
    "    for u_t, i_t, r_t, t_t in zip(tes[0], tes[1], tes[2], tes[3]):\n",
    "        str_1 = str(u_t)\n",
    "        str_2 = str(i_t)\n",
    "        str_3 = str(r_t)\n",
    "        str_4 = str(t_t)\n",
    "        file.write(f'{str_1},{str_2},{str_3},{str_4}\\n')\n",
    "            \n",
    "with open(path_i_text, \"wb\") as file_object:\n",
    "    pickle.dump(i_num, file_object)\n",
    "with open(path_u_text, \"wb\") as file_object:\n",
    "    pickle.dump(u_num, file_object)            \n",
    "\n",
    "with open(path_voc, \"wb\") as file_object:\n",
    "    pickle.dump(word_dict, file_object) \n",
    "print(\"dataset: \", datasets_name)\n",
    "print(\"model: RNS\")\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******SKNN******\n",
      "item_num:  12391\n",
      "train_seq_num & user_num:  12373\n",
      "test_seq_num:  12311\n",
      "#interactions:  110313\n",
      "sequence average length:  8.915622726905358\n",
      "dataset:  Yelp\n",
      "2023-12-29 08:53:55\n",
      "SKNN done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "datasets_name = 'Yelp'\n",
    "# data_path = 'data_time_interval/' + datasets_name +'_data.csv'\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "\n",
    "# datasets_name = '2019-Oct'\n",
    "# price_num = 'logistic'\n",
    "\n",
    "# data_path = 'cosmeticsdata/' + datasets_name + '_data.csv'\n",
    "\n",
    "# datasets_name = 'digineticaBuy'\n",
    "# data_path = 'digineticaBuy/' + datasets_name + '_data.csv'\n",
    "\n",
    "\n",
    "print(\"******SKNN******\")\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID', 'review_time']]\n",
    "\n",
    "\n",
    "# item_inter_num = pd.DataFrame(data.groupby(data['itemID']).count())\n",
    "# item_num=item_inter_num.reset_index().rename(columns={'sessionID':'item_num'})[['itemID', 'item_num']]\n",
    "# data = pd.merge(data, item_num, how='left', on = 'itemID')\n",
    "# data = data[data['item_num']>9]\n",
    "# data = data[['sessionID', 'itemID', 'time']]\n",
    " \n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "sess_date = {}\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    timestamp = row['review_time']\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "        sess_date[sess_id].append(timestamp)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "        sess_date[sess_id] = []        \n",
    "        sess_date[sess_id].append(timestamp)\n",
    "        \n",
    "# 限制每个seq长度\n",
    "for sess_temp in sess_all.keys():\n",
    "    seq = sess_all[sess_temp]\n",
    "    time_seq = sess_date[sess_temp]\n",
    "    if len(seq) >20:\n",
    "        sess_all[sess_temp] = seq[-20:]\n",
    "        sess_date[sess_temp] = time_seq[-20:]\n",
    "\n",
    "tra = [[],[],[], []]\n",
    "# dict {train_sessionID:[itemID,itemID]}  去掉了seq中last item(不包含test的label)， 也可以调整为去掉两个item\n",
    "train_sess = {}\n",
    "item_dict = {}\n",
    "item_count = 0\n",
    "user_count = 0\n",
    "inter_count = 0\n",
    "item_num = 1\n",
    "\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    train_seqs_t = all_seqs[:-1]\n",
    "    t_time_seq = sess_date[sess_temp][:-1]\n",
    "    inter_count += len(train_seqs_t)\n",
    "#     item_seq with new ID\n",
    "    new_seq = []\n",
    "    for x in train_seqs_t:\n",
    "        if x in item_dict:\n",
    "            new_seq.append(item_dict[x])\n",
    "        else:\n",
    "            item_dict[x] = item_num\n",
    "            new_seq.append(item_num)\n",
    "            item_num += 1\n",
    "    train_sess[sess_temp] = new_seq\n",
    "    tra[0].append(sess_temp)\n",
    "    tra[1].append(new_seq[:-1])\n",
    "#     预测时考虑last item\n",
    "    tra[2].append(new_seq[-1])\n",
    "    tra[3].append(t_time_seq)\n",
    "    \n",
    "item_count =  item_num-1\n",
    "user_count = len(tra[0])\n",
    "\n",
    "# test_set [[x_id ...],[label...]\n",
    "tes = [[],[],[], []]\n",
    "tes_seq_num = 0\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    all_time = sess_date[sess_temp]\n",
    "    if all_seqs[-1] in item_dict:\n",
    "        new_seq = []\n",
    "        for x in all_seqs:\n",
    "            new_seq.append(item_dict[x])\n",
    "        tes_seq_num += 1\n",
    "        tes[0].append(sess_temp)\n",
    "        tes[1].append(new_seq[:-1])\n",
    "    #     预测时考虑last item\n",
    "        tes[2].append(new_seq[-1])\n",
    "        tes[3].append(all_time)\n",
    "    \n",
    "\n",
    "print('item_num: ',str(item_count))\n",
    "print('train_seq_num & user_num: ',str(user_count))\n",
    "print('test_seq_num: ',str(tes_seq_num))\n",
    "\n",
    "inter_count += tes_seq_num\n",
    "\n",
    "\n",
    "print('#interactions: ',inter_count)\n",
    "\n",
    "print('sequence average length: ', (inter_count)/(user_count) * 1.0)\n",
    "\n",
    "tra = (tra[0], tra[1], tra[2], tra[3])\n",
    "tes = (tes[0], tes[1], tes[2], tes[3])\n",
    "\n",
    "train_data_path = './SKNN/' + datasets_name\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "path_data_train = train_data_path + \"/train.txt\"\n",
    "path_data_test = train_data_path + \"/test.txt\"\n",
    "\n",
    "pickle.dump(tra, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes, open(path_data_test, 'wb'))\n",
    "print(\"dataset: \", datasets_name)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"SKNN done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSLRec Fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_num:  12391\n",
      "train_seq_num & user_num:  12373\n",
      "test_seq_num:  12311\n",
      "#interactions:  110313\n",
      "sequence average length:  8.915622726905358\n",
      "dataset:  Yelp\n",
      "model: SSLRec\n",
      "2023-12-29 09:23:49\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "datasets_name = 'Yelp'\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "# data_path = 'digineticaBuy/digineticaBuy_data.csv'\n",
    "\n",
    "# datasets_name = '2019-Oct'\n",
    "# price_num = 5\n",
    "\n",
    "# data_path = 'cosmeticsdata/' + datasets_name + '_data.csv'\n",
    "# datasets_name = 'digineticaBuy'\n",
    "# data_path = 'digineticaBuy/' + datasets_name + '_data.csv'\n",
    "\n",
    "\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID']]\n",
    "\n",
    "item_count = 0\n",
    "user_count = 0\n",
    "inter_count = 0\n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "#  dict {old_itemID: title}\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "\n",
    "# 限制每个seq长度\n",
    "for sess_temp in sess_all.keys():\n",
    "    seq = sess_all[sess_temp]\n",
    "    if len(seq) >20:\n",
    "        sess_all[sess_temp] = seq[-20:]\n",
    "        \n",
    "item_num = 1\n",
    "# dict{lod_itemID:new_itemID}\n",
    "item_dict = {}\n",
    "# train_set [[u1, u2],[x1, x2, x3], [label...]\n",
    "tra = [[],[],[]]\n",
    "# dict {train_sessionID:[itemID,itemID]}  去掉了seq中last item(不包含test的label)， 也可以调整为去掉两个item\n",
    "train_sess = {}\n",
    "\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    train_seqs_t = all_seqs[:-1]\n",
    "    inter_count += len(train_seqs_t)\n",
    "#     item_seq with new ID\n",
    "    new_seq = []\n",
    "    for x in train_seqs_t:\n",
    "        if x in item_dict:\n",
    "            new_seq.append(item_dict[x])\n",
    "        else:\n",
    "            item_dict[x] = item_num\n",
    "            new_seq.append(item_num)            \n",
    "            item_num += 1\n",
    "    train_sess[sess_temp] = new_seq\n",
    "    tra[0].append(sess_temp)\n",
    "    tra[1].append(new_seq[:-1])\n",
    "#     预测时考虑last item\n",
    "    tra[2].append(new_seq[-1])\n",
    "    \n",
    "item_count =  item_num-1\n",
    "user_count = len(tra[0])\n",
    "\n",
    "# test_set [[x_id ...],[label...]\n",
    "tes = [[],[],[]]\n",
    "tes_seq_num = 0\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if all_seqs[-1] in item_dict:\n",
    "        new_seq = []\n",
    "        for x in all_seqs:\n",
    "            new_seq.append(item_dict[x])\n",
    "        tes_seq_num += 1\n",
    "        tes[0].append(sess_temp)\n",
    "        tes[1].append(new_seq[:-1])\n",
    "    #     预测时考虑last item\n",
    "        tes[2].append(new_seq[-1])\n",
    "    \n",
    "\n",
    "print('item_num: ',str(item_count))\n",
    "print('train_seq_num & user_num: ',str(user_count))\n",
    "print('test_seq_num: ',str(tes_seq_num))\n",
    "\n",
    "inter_count += tes_seq_num\n",
    "\n",
    "\n",
    "print('#interactions: ',inter_count)\n",
    "\n",
    "print('sequence average length: ', (inter_count)/(user_count) * 1.0)\n",
    "\n",
    "train_data_path = 'SSLRecFine/' + datasets_name + 'Fine/'\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "\n",
    "\n",
    "path_data_train = train_data_path + 'train.tsv'\n",
    "# path_data_valid = train_data_path + f'{datasets_name_c}.valid.inter'\n",
    "path_data_test = train_data_path + 'test.tsv'\n",
    "# train_sessid_list = range(len(tr_labs))\n",
    "with open(path_data_train, 'w') as file:\n",
    "    file.write('session_id\\t item_id_seq\\t item_id \\n')\n",
    "    for index, sess, lab in zip(tra[0],tra[1], tra[2]):\n",
    "        i_str = str(index-1)\n",
    "        s_str = [str(x) for x in sess]\n",
    "        l_str = str(lab)\n",
    "        file.write(f'{i_str}\\t{\" \".join(s_str)}\\t{l_str}\\n')\n",
    "\n",
    "\n",
    "with open(path_data_test, 'w') as file:\n",
    "    file.write('session_id\\t item_id_seq\\t item_id \\n')\n",
    "    for index, sess, lab in zip(range(len(tes[0])),tes[1], tes[2]):\n",
    "        i_str = str(index-1)\n",
    "        s_str = [str(x) for x in sess]\n",
    "        l_str = str(lab)\n",
    "        file.write(f'{i_str}\\t{\" \".join(s_str)}\\t{l_str}\\n')\n",
    "\n",
    "\n",
    "# train_sessid_list = range(len(tr_labs))\n",
    "# train_seq_list = tr_seqs\n",
    "# train_lab_list = tr_labs\n",
    "# train_pd = pd.DataFrame({'session_id':train_sessid_list, 'item_id_seq':train_seq_list, 'item_id':train_lab_list})\n",
    "\n",
    "# test_sessid_list = range(len(te_labs))\n",
    "# test_seq_list = te_seqs\n",
    "# test_lab_list = te_labs\n",
    "# test_pd = pd.DataFrame({'session_id':test_sessid_list, 'item_id_seq':test_seq_list, 'item_id':test_lab_list})\n",
    "\n",
    "# train_pd.to_csv(path_data_train, sep = '\\t', index=False)\n",
    "# test_pd.to_csv(path_data_test, sep = '\\t', index=False)\n",
    "\n",
    "print(\"dataset: \", datasets_name)\n",
    "print(\"model: SSLRec\")\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoSecRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_num:  6208\n",
      "train_seq_num & user_num:  42868\n",
      "test_seq_num:  7272\n",
      "#interactions:  50140\n",
      "sequence average length:  1.1696370252869273\n",
      "dataset:  Cell_Phones_and_Accessories\n",
      "model: AutoSecRec\n",
      "2023-12-23 14:28:45\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "datasets_name = 'Cell_Phones_and_Accessories'\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "# data_path = 'digineticaBuy/digineticaBuy_data.csv'\n",
    "\n",
    "# datasets_name = '2019-Oct'\n",
    "# price_num = 5\n",
    "\n",
    "# data_path = 'cosmeticsdata/' + datasets_name + '_data.csv'\n",
    "# datasets_name = 'digineticaBuy'\n",
    "# data_path = 'digineticaBuy/' + datasets_name + '_data.csv'\n",
    "\n",
    "\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID','review_time']]\n",
    "\n",
    "# item_inter_num = pd.DataFrame(data.groupby(data['itemID']).count())\n",
    "# item_num=item_inter_num.reset_index().rename(columns={'sessionID':'item_num'})\n",
    "# data = pd.merge(data, item_num, how='left', on = 'itemID')\n",
    "# data = data[data['item_num']>9]\n",
    "# data = data[['sessionID', 'itemID']]\n",
    "\n",
    "item_count = 0\n",
    "user_count = 0\n",
    "inter_count = 0\n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "# dict{sessionID:[time]}\n",
    "time_all = {}\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = int(row['sessionID'])-1\n",
    "    item_id = row['itemID']\n",
    "    time_t = row['review_time']\n",
    "    \n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "        time_all[sess_id].append(time_t)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "        time_all[sess_id] = []\n",
    "        time_all[sess_id].append(time_t)\n",
    "\n",
    "# 限制每个seq长度\n",
    "for sess_temp in sess_all.keys():\n",
    "    seq = sess_all[sess_temp]\n",
    "    if len(seq) >20:\n",
    "        sess_all[sess_temp] = seq[-20:]\n",
    "        time_all[sess_temp] = time_all[sess_temp][-20:]\n",
    "        \n",
    "item_num = 0\n",
    "# dict{lod_itemID:new_itemID}\n",
    "item_dict = {}\n",
    "# train_set [[u1, u2],[x1, x2, x3], [time...]\n",
    "tra = [[],[],[]]\n",
    "# val = [[], [], []]\n",
    "# dict {train_sessionID:[itemID,itemID]}  去掉了seq中last item(不包含test的label)， 也可以调整为去掉两个item\n",
    "train_sess = {}\n",
    "\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    train_seqs_t = all_seqs[:-1]\n",
    "    train_time_t = time_all[sess_temp][:-1]\n",
    "    inter_count += len(train_seqs_t)\n",
    "#     item_seq with new ID\n",
    "    new_seq = []\n",
    "    for x in train_seqs_t:\n",
    "        if x in item_dict:\n",
    "            new_seq.append(item_dict[x])\n",
    "        else:\n",
    "            item_dict[x] = item_num\n",
    "            new_seq.append(item_num)\n",
    "            \n",
    "            item_num += 1\n",
    "    train_sess[sess_temp] = new_seq\n",
    "    tra[0] += [sess_temp]*len(new_seq)\n",
    "    tra[1] += new_seq\n",
    "    tra[2] += train_time_t\n",
    "#     预测时考虑last item\n",
    "    \n",
    "item_count =  item_num\n",
    "user_count = len(tra[0])\n",
    "\n",
    "# test_set [[x_id ...],[label...]\n",
    "tes = [[],[],[]]\n",
    "tes_seq_num = 0\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if all_seqs[-1] in item_dict:\n",
    "        tra_seqs = train_sess[sess_temp]\n",
    "        tes[0] += [sess_temp]*(len(tra_seqs)+1)\n",
    "        tes[1] += tra_seqs + [(item_dict[all_seqs[-1]])]\n",
    "        tes[2] += range(len(tra_seqs)+1)\n",
    "        tes_seq_num +=1\n",
    "        \n",
    "#         tes[0].append(sess_temp)\n",
    "#         tes[1].append(item_dict[all_seqs[-1]])\n",
    "#         tes[2].append(time_all[sess_temp][-1])\n",
    "#         tes_seq_num +=1\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "print('item_num: ',str(item_count))\n",
    "print('train_seq_num & user_num: ',str(user_count))\n",
    "print('test_seq_num: ',str(tes_seq_num))\n",
    "\n",
    "inter_count += tes_seq_num\n",
    "\n",
    "\n",
    "print('#interactions: ',inter_count)\n",
    "\n",
    "print('sequence average length: ', (inter_count)/(user_count) * 1.0)\n",
    "\n",
    "train_data_path = './AutoSecRec/' + datasets_name + '/'\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "    \n",
    "path_data_train = train_data_path + 'train.txt'\n",
    "path_data_test = train_data_path + 'test.txt'\n",
    "\n",
    "\n",
    "# tra_dict = {'user_id':tra[0], 'item_id':tra[1], 'timestamp':tra[2]}\n",
    "# tra_df = pd.DataFrame(tra_dict)\n",
    "       \n",
    "# tes_dict = {'user_id':tes[0], 'item_id':tes[1], 'timestamp':tes[2]}\n",
    "# tes_df = pd.DataFrame(tes_dict)\n",
    "\n",
    "pickle.dump(tra, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes, open(path_data_test, 'wb'))\n",
    "\n",
    "# with open(path_data_train, \"wb\") as file_object:\n",
    "#     pickle.dump(tra_dict, file_object)\n",
    "    \n",
    "# with open(path_data_test, \"wb\") as file_object:\n",
    "#     pickle.dump(tes_dict, file_object)\n",
    "\n",
    "# tra_df.to_csv(path_data_train, sep=\",\", header=False, index=False, encoding='utf-8')\n",
    "# tes_df.to_csv(path_data_test, sep=\",\", header=False, index=False, encoding='utf-8')   \n",
    "\n",
    "print(\"dataset: \", datasets_name)\n",
    "print(\"model: AutoSecRec\")\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48335"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tes[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAERec SIGIR 2023 Graph Masked Autoencoder for Sequential Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_num:  6208\n",
      "train_seq_num & user_num:  7598\n",
      "test_seq_num:  7272\n",
      "#interactions:  50140\n",
      "sequence average length:  6.599105027638853\n",
      "Constructed i-i graph, density=0.003265\n",
      "dataset:  Cell_Phones_and_Accessories\n",
      "Model: MAERec\n",
      "2023-12-21 20:48:26\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "\n",
    "meta_dataset = 'Cell_Phones_and_Accessories'\n",
    "data_path = 'dict/' + meta_dataset +'/session_data.csv'\n",
    "# data_path = 'digineticaBuy/digineticaBuy_data.csv'\n",
    "\n",
    "# datasets_name = '2019-Oct'\n",
    "# price_num = 5\n",
    "\n",
    "# data_path = 'cosmeticsdata/' + datasets_name + '_data.csv'\n",
    "# datasets_name = 'digineticaBuy'\n",
    "# data_path = 'digineticaBuy/' + datasets_name + '_data.csv'\n",
    "\n",
    "\n",
    "if meta_dataset == 'Beauty' or meta_dataset == 'Beauty_4':\n",
    "    # 'Brand','Ingredients','Effectiveness','Scent','Price','Color','Size'\n",
    "    attr_list = ['Brand','Ingredients','Effectiveness','Scent','Price','Color','Size']\n",
    "elif meta_dataset == 'Cell_Phones_and_Accessories':\n",
    "    # Brand, Performance, color, Battery ,Connectivity,Price, Size\n",
    "    attr_list = ['Brand', 'Performance', 'Color', 'Battery' ,'Connectivity','Price', 'Size']\n",
    "elif meta_dataset == 'Clothing_Shoes_and_Jewelry':\n",
    "    # Price, Style, Size, Fabric, Brand, Quality, Color\n",
    "    attr_list = ['Price', 'Style', 'Size', 'Fabric', 'Brand', 'Quality', 'Color']\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data_all = data_all.reset_index()[['sessionID', 'itemID', attr_list[0], attr_list[1],attr_list[2],attr_list[3],attr_list[4],attr_list[5],attr_list[6]]]\n",
    "data_all = data_all[['sessionID', 'itemID']]\n",
    "\n",
    "\n",
    "# dict {sessionID:[itemID,itemID]} \n",
    "sess_all = {}\n",
    "#  dict {sessionID:[opinID,opinID]}\n",
    "\n",
    "\n",
    "item_count = 0\n",
    "user_count = 0\n",
    "inter_count = 0\n",
    "\n",
    "for _, row in data_all.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    if sess_id in sess_all: \n",
    "        sess_all[sess_id].append(item_id)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "\n",
    "# 限制每个seq长度\n",
    "for sess_temp in sess_all.keys():\n",
    "    seq = sess_all[sess_temp]\n",
    "    if len(seq) >20:\n",
    "        sess_all[sess_temp] = seq[-20:]\n",
    "\n",
    "tra_seqs = []\n",
    "tes_seqs = []\n",
    "        \n",
    "# 形成train set\n",
    "# 统计 train set 出现的item，给item重新编号\n",
    "item_num = 1\n",
    "# dict{lod_itemID:new_itemID}\n",
    "item_dict = {}\n",
    "# train_set [[x1, x2, x3], [label...]\n",
    "# dict {train_sessionID:[itemID,itemID]}  去掉了seq中last item(不包含test的label)， 也可以调整为去掉两个item\n",
    "train_sess = {}\n",
    "\n",
    "# dict {item_id:[user_id, user_id]}\n",
    "train_items = {}\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    train_seqs_t = all_seqs[:-1]\n",
    "    inter_count += len(train_seqs_t)\n",
    "#     item_seq with new ID\n",
    "    new_seq = []\n",
    "    for x in train_seqs_t:\n",
    "        if x in item_dict:\n",
    "            new_seq.append(item_dict[x])\n",
    "            if sess_temp not in train_items[item_dict[x]]:\n",
    "                train_items[item_dict[x]].append(sess_temp)\n",
    "        else:\n",
    "            item_dict[x] = item_num\n",
    "            new_seq.append(item_num)\n",
    "            train_items[item_num] = [] \n",
    "            train_items[item_num].append(sess_temp)\n",
    "            \n",
    "            item_num += 1\n",
    "    train_sess[sess_temp] = new_seq\n",
    "    tra_seqs.append(new_seq)\n",
    "    \n",
    "item_count =  item_num-1\n",
    "user_count = len(tra_seqs)\n",
    "\n",
    "\n",
    "# test_set [[x_id ...],[label...]\n",
    "tes = [[],[]]\n",
    "tes_seq_num = 0\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if all_seqs[-1] in item_dict:\n",
    "        new_seq = []\n",
    "        for x in all_seqs:\n",
    "            new_seq.append(item_dict[x])\n",
    "        tes_seq_num += 1\n",
    "        tes_seqs.append(new_seq)\n",
    "    \n",
    "\n",
    "print('item_num: ',str(item_count))\n",
    "print('train_seq_num & user_num: ',str(user_count))\n",
    "print('test_seq_num: ',str(tes_seq_num))\n",
    "\n",
    "inter_count += tes_seq_num\n",
    "\n",
    "\n",
    "print('#interactions: ',inter_count)\n",
    "\n",
    "print('sequence average length: ', (inter_count)/(user_count) * 1.0)       \n",
    "\n",
    "\n",
    "train_data_path = 'MAERec/' + meta_dataset + '/'\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "\n",
    "user = list()\n",
    "r, c, d = list(), list(), list()\n",
    "# k=4\n",
    "for i, seq in enumerate(tra_seqs):\n",
    "    for dist in range(1, 3):\n",
    "        if dist >= len(seq): break;\n",
    "        r += copy.deepcopy(seq[+dist:])\n",
    "        c += copy.deepcopy(seq[:-dist])\n",
    "        r += copy.deepcopy(seq[:-dist])\n",
    "        c += copy.deepcopy(seq[+dist:])\n",
    "d = np.ones_like(r)\n",
    "iigraph = csr_matrix((d, (r, c)), shape=(item_num, item_num))\n",
    "print('Constructed i-i graph, density=%.6f' % (len(d) / ((item_num) ** 2)))\n",
    "neg_seqs = []\n",
    "for t_seq in tes_seqs:\n",
    "    non_lab = t_seq[-1]\n",
    "    ran_list = np.random.randint(1, item_num, 99)\n",
    "    ran_list = [1 if i == non_lab else i for i in ran_list]\n",
    "    neg_seqs.append(ran_list)\n",
    "\n",
    "with open(train_data_path + 'trn', 'wb') as fs:\n",
    "    pickle.dump(iigraph, fs)\n",
    "\n",
    "with open(train_data_path + 'seq', 'wb') as fs:\n",
    "    pickle.dump(tra_seqs, fs)\n",
    "    \n",
    "with open(train_data_path + 'tst', 'wb') as fs:\n",
    "    pickle.dump(tes_seqs, fs)\n",
    "    \n",
    "with open(train_data_path + 'neg', 'wb') as fs:\n",
    "    pickle.dump(neg_seqs, fs)\n",
    "    \n",
    "print(\"dataset: \", meta_dataset)\n",
    "print(\"Model: MAERec\")\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## senti backup 有每个attribute下，user-item交互的矩阵，用于attribute-graph update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "meta_dataset = 'Cell_Phones_and_Accessories'\n",
    "data_path =  './dict/' + meta_dataset + '/session_data.csv'\n",
    "\n",
    "if meta_dataset == 'Beauty' or meta_dataset == 'Beauty_4':\n",
    "    # 'Brand','Ingredients','Effectiveness','Scent','Price','Color','Size'\n",
    "    attr_list = ['Brand','Ingredients','Effectiveness','Scent','Price','Color','Size']\n",
    "elif meta_dataset == 'Cell_Phones_and_Accessories':\n",
    "    # Brand, Performance, color, Battery ,Connectivity,Price, Size\n",
    "    attr_list = ['Brand', 'Performance', 'Color', 'Battery' ,'Connectivity','Price', 'Size']\n",
    "elif meta_dataset == 'Clothing_Shoes_and_Jewelry':\n",
    "    # Price, Style, Size, Fabric, Brand, Quality, Color\n",
    "    attr_list = ['Price', 'Style', 'Size', 'Fabric', 'Brand', 'Quality', 'Color']\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data_all = data_all.reset_index()[['sessionID', 'itemID', attr_list[0], attr_list[1],attr_list[2],attr_list[3],attr_list[4],attr_list[5],attr_list[6]]]\n",
    "\n",
    "attr_num = len(attr_list)\n",
    "\n",
    "# recorde all items\n",
    "# dict {sessionID:[itemID,itemID]} \n",
    "sess_all = {}\n",
    "#  dict {sessionID:[opinID,opinID]}\n",
    "attr1_all = {}\n",
    "attr2_all = {}\n",
    "attr3_all = {}\n",
    "attr4_all = {}\n",
    "attr5_all = {}\n",
    "attr6_all = {}\n",
    "attr7_all = {}\n",
    "\n",
    "item_count = 0\n",
    "user_count = 0\n",
    "inter_count = 0\n",
    "\n",
    "for _, row in data_all.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    attr1_opin = row[attr_list[0]]\n",
    "    attr2_opin = row[attr_list[1]]\n",
    "    attr3_opin = row[attr_list[2]]\n",
    "    attr4_opin = row[attr_list[3]]\n",
    "    attr5_opin = row[attr_list[4]]\n",
    "    attr6_opin = row[attr_list[5]]\n",
    "    attr7_opin = row[attr_list[6]]\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "        attr1_all[sess_id].append(attr1_opin)\n",
    "        attr2_all[sess_id].append(attr2_opin)\n",
    "        attr3_all[sess_id].append(attr3_opin)\n",
    "        attr4_all[sess_id].append(attr4_opin)\n",
    "        attr5_all[sess_id].append(attr5_opin)\n",
    "        attr6_all[sess_id].append(attr6_opin)\n",
    "        attr7_all[sess_id].append(attr7_opin)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "        attr1_all[sess_id] = []\n",
    "        attr1_all[sess_id].append(attr1_opin)\n",
    "        attr2_all[sess_id] = []\n",
    "        attr2_all[sess_id].append(attr2_opin)\n",
    "        attr3_all[sess_id] = []\n",
    "        attr3_all[sess_id].append(attr3_opin)\n",
    "        attr4_all[sess_id] = []\n",
    "        attr4_all[sess_id].append(attr4_opin)\n",
    "        attr5_all[sess_id] = []\n",
    "        attr5_all[sess_id].append(attr5_opin)\n",
    "        attr6_all[sess_id] = []\n",
    "        attr6_all[sess_id].append(attr6_opin)\n",
    "        attr7_all[sess_id] = []\n",
    "        attr7_all[sess_id].append(attr7_opin)\n",
    "\n",
    "\n",
    "# 形成train set\n",
    "# 统计 train set 出现的item，给item重新编号\n",
    "item_num = 1\n",
    "# dict{lod_itemID:new_itemID}\n",
    "item_dict = {}\n",
    "# train_set [[u_id ...], [x_t-1...], [label...]\n",
    "tra = [[],[],[]]\n",
    "# dict {train_sessionID:[itemID,itemID]}  去掉了seq中last item(不包含test的label)， 也可以调整为去掉两个item\n",
    "train_sess = {}\n",
    "\n",
    "# dict {item_id:[user_id, user_id]}\n",
    "train_items = {}\n",
    "\n",
    "# dict {item_id:opin_id}\n",
    "attr1_item_opin = {}\n",
    "attr2_item_opin = {}\n",
    "attr3_item_opin = {}\n",
    "attr4_item_opin = {}\n",
    "attr5_item_opin = {}\n",
    "attr6_item_opin = {}\n",
    "attr7_item_opin = {}\n",
    "\n",
    "\n",
    "\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    train_seqs_t = all_seqs[:-1]\n",
    "    a1_opin = attr1_all[sess_temp][:-1]\n",
    "    a2_opin = attr2_all[sess_temp][:-1]\n",
    "    a3_opin = attr3_all[sess_temp][:-1]\n",
    "    a4_opin = attr4_all[sess_temp][:-1]\n",
    "    a5_opin = attr5_all[sess_temp][:-1]\n",
    "    a6_opin = attr6_all[sess_temp][:-1]\n",
    "    a7_opin = attr7_all[sess_temp][:-1]\n",
    "    inter_count += len(train_seqs_t)\n",
    "#     item_seq with new ID\n",
    "    new_seq = []\n",
    "    for x, a1, a2, a3, a4, a5, a6, a7 in zip(train_seqs_t, a1_opin, a2_opin,a3_opin,a4_opin,a5_opin,a6_opin,a7_opin):\n",
    "        if x in item_dict:\n",
    "            new_seq.append(item_dict[x])\n",
    "            if sess_temp not in train_items[item_dict[x]]:\n",
    "                train_items[item_dict[x]].append(sess_temp)\n",
    "                attr1_item_opin[item_dict[x]].append(a1)\n",
    "                attr2_item_opin[item_dict[x]].append(a2)\n",
    "                attr3_item_opin[item_dict[x]].append(a3)\n",
    "                attr4_item_opin[item_dict[x]].append(a4)\n",
    "                attr5_item_opin[item_dict[x]].append(a5)\n",
    "                attr6_item_opin[item_dict[x]].append(a6)\n",
    "                attr7_item_opin[item_dict[x]].append(a7)\n",
    "        else:\n",
    "            item_dict[x] = item_num\n",
    "            new_seq.append(item_num)\n",
    "            \n",
    "            train_items[item_num] = [] \n",
    "            train_items[item_num].append(sess_temp)\n",
    "            attr1_item_opin[item_num] = []\n",
    "            attr2_item_opin[item_num] = []\n",
    "            attr3_item_opin[item_num] = []\n",
    "            attr4_item_opin[item_num] = []\n",
    "            attr5_item_opin[item_num] = []\n",
    "            attr6_item_opin[item_num] = []\n",
    "            attr7_item_opin[item_num] = []\n",
    "            attr1_item_opin[item_num].append(a1)\n",
    "            attr2_item_opin[item_num].append(a2)\n",
    "            attr3_item_opin[item_num].append(a3)\n",
    "            attr4_item_opin[item_num].append(a4)\n",
    "            attr5_item_opin[item_num].append(a5)\n",
    "            attr6_item_opin[item_num].append(a6)\n",
    "            attr7_item_opin[item_num].append(a7)\n",
    "            \n",
    "            item_num += 1\n",
    "    train_sess[sess_temp] = new_seq\n",
    "    tra[0].append(sess_temp)\n",
    "#     预测时考虑last item\n",
    "    tra[1].append(new_seq[:-1])\n",
    "    tra[2].append(new_seq[-1])\n",
    "    \n",
    "item_count =  item_num-1\n",
    "user_count = len(tra[0])\n",
    "\n",
    "fre_item_mat = np.zeros((item_count, item_count), dtype=int)\n",
    "# 全部的共现关系Original \n",
    "for sess in train_sess.keys():\n",
    "    item_seq = np.unique(train_sess[sess])\n",
    "    for it1 in item_seq:\n",
    "        for it2 in item_seq:\n",
    "            if it1 != it2:\n",
    "#                 print(sess)\n",
    "                fre_item_mat[int(it1)-1, int(it2)-1] +=1 \n",
    "                \n",
    "fre_user_mat = np.zeros((user_count, user_count), dtype=int)\n",
    "# 全部的共现关系Original \n",
    "for item_t in train_items.keys():\n",
    "    user_seq = np.unique(train_items[item_t])\n",
    "    for it1 in user_seq:\n",
    "        for it2 in user_seq:\n",
    "            if it1 != it2:\n",
    "                fre_user_mat[int(it1)-1, int(it2)-1] +=1 \n",
    "\n",
    "# building user_fre & item_fre csr_matrix\n",
    "def spar_graph(fre_mat):\n",
    "    indices = [] # 非0元素的列坐标，item_id -1\n",
    "    all_data = [] # 非0元素值，opin_id\n",
    "    indptr = [0] # i i-1 行非0元素个数，0-0， n-总共元素个数，行数+1\n",
    "    for i_list in fre_mat:\n",
    "        i_temp_list = np.nonzero(i_list)[0].tolist()\n",
    "#         if len(i_temp_list) == 0:\n",
    "#             non_co += 1\n",
    "        indices += i_temp_list\n",
    "        all_data += [1]*len(i_temp_list)\n",
    "        indptr.append(indptr[-1]+len(i_temp_list))\n",
    "#     print('item without co-occurren: ',str(non_co))\n",
    "    # indptr:session长度累加和; indices:item_id 减1, 由每个session内item组成; data:item在session内的权重，全部为1.\n",
    "    coo_mat = (all_data, indices, indptr)\n",
    "    return coo_mat\n",
    "\n",
    "item_co_mat = spar_graph(fre_item_mat)\n",
    "user_co_mat = spar_graph(fre_user_mat)\n",
    "\n",
    "\n",
    "# test_set [[u_id ...], [x_t-1...], [label...]\n",
    "tes = [[],[],[]]\n",
    "tes_seq_num = 0\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if all_seqs[-1] in item_dict:\n",
    "        tes_seq_num += 1\n",
    "        new_seq = []\n",
    "        for x in all_seqs:\n",
    "            new_seq.append(item_dict[x])\n",
    "        tes[0].append(sess_temp)\n",
    "    #     预测时考虑last item\n",
    "        tes[1].append(new_seq[:-1])\n",
    "        tes[2].append(item_dict[all_seqs[-1]])\n",
    "    \n",
    "\n",
    "print('item_num: ',str(item_count))\n",
    "print('train_seq_num & user_num: ',str(user_count))\n",
    "print('test_seq_num: ',str(tes_seq_num))\n",
    "\n",
    "inter_count += tes_seq_num\n",
    "\n",
    "#  obtain user-item interaction sparsity matrix in each attribute\n",
    "def spar_matrix(attr, tra_seq):\n",
    "    indices = [] # 非0元素的列坐标，item_id -1\n",
    "    all_data = [] # 非0元素值，opin_id\n",
    "    indptr = [0] # i i-1 行非0元素个数，0-0， n-总共元素个数，行数+1\n",
    "    for sess_temp in attr.keys():\n",
    "        op_list = attr[sess_temp][:-1]\n",
    "        it_list = tra_seq[sess_temp]\n",
    "        np_op = np.array(op_list)\n",
    "        np_it = np.array(it_list)\n",
    "        op_f = np_op[np.nonzero(np_op)[0]]\n",
    "        it_f = np_it[np.nonzero(np_op)[0]]\n",
    "\n",
    "        item_l = np.unique(it_f)\n",
    "        op_l = op_f[np.unique(it_f, return_index=True)[1]]\n",
    "        indices += (item_l-1).tolist()\n",
    "        all_data += op_l.tolist()\n",
    "        indptr.append(indptr[-1] + len(item_l.tolist()))\n",
    "    results = (all_data, indices, indptr)\n",
    "    return results\n",
    "\n",
    "attr1_mat = spar_matrix(attr1_all, train_sess)\n",
    "attr2_mat = spar_matrix(attr2_all, train_sess)\n",
    "attr3_mat = spar_matrix(attr3_all, train_sess)\n",
    "attr4_mat = spar_matrix(attr4_all, train_sess)\n",
    "attr5_mat = spar_matrix(attr5_all, train_sess)\n",
    "attr6_mat = spar_matrix(attr6_all, train_sess)\n",
    "attr7_mat = spar_matrix(attr7_all, train_sess)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 返回 [[u1,u2],[attr1_opin, attr1_opin]]\n",
    "def formulate_seq(dict_ui, dict_op):\n",
    "    results = [[],[]]\n",
    "    temp_res = [[],[]]\n",
    "    max_len =0\n",
    "    for x in dict_ui.keys():\n",
    "        seqs_id = dict_ui[x]\n",
    "        opin_id = dict_op[x]\n",
    "        new_us_seq = []\n",
    "        new_op_seq = []\n",
    "        for id_t, op_t in zip(seqs_id, opin_id):\n",
    "            if op_t != 0 and id_t not in new_us_seq:\n",
    "                new_us_seq.append(id_t)\n",
    "                new_op_seq.append(op_t)\n",
    "        if len(new_us_seq)==0:\n",
    "            new_us_seq.append(0)\n",
    "            new_op_seq.append(0)\n",
    "        if len(new_us_seq)>max_len:\n",
    "            max_len = len(new_us_seq)\n",
    "        temp_res[0].append(new_us_seq)\n",
    "        temp_res[1].append(new_op_seq)\n",
    "    for ui_seq, op_seq in zip(temp_res[0],temp_res[1]):\n",
    "        pad_len = max_len - len(ui_seq)\n",
    "        pad_ui_seq = ui_seq + [0]*pad_len\n",
    "        pad_op_seq = op_seq + [0]*pad_len\n",
    "        results[0].append(pad_ui_seq)\n",
    "        results[1].append(pad_op_seq)\n",
    "#     print('max_len:',str(max_len))\n",
    "    return results\n",
    "\n",
    "# u_seq = {1:[1,2,3,1],2:[4,5], 3:[7,8,9]}\n",
    "# u_op = {1:[45,56,0,45],2:[66,0], 3:[0,33,0]}\n",
    "# # [[[1,2],[4],[8]],[[45,56],[66],[33]]]\n",
    "# formulate_seq(u_seq,u_op)\n",
    "\n",
    "ui_1 = formulate_seq(train_sess,attr1_all)\n",
    "ui_2 = formulate_seq(train_sess,attr2_all)\n",
    "ui_3 = formulate_seq(train_sess,attr3_all)\n",
    "ui_4 = formulate_seq(train_sess,attr4_all)\n",
    "ui_5 = formulate_seq(train_sess,attr5_all)\n",
    "ui_6 = formulate_seq(train_sess,attr6_all)\n",
    "ui_7 = formulate_seq(train_sess,attr7_all)\n",
    "iu_1 = formulate_seq(train_items,attr1_item_opin)\n",
    "iu_2 = formulate_seq(train_items,attr2_item_opin)\n",
    "iu_3 = formulate_seq(train_items,attr3_item_opin)\n",
    "iu_4 = formulate_seq(train_items,attr4_item_opin)\n",
    "iu_5 = formulate_seq(train_items,attr5_item_opin)\n",
    "iu_6 = formulate_seq(train_items,attr6_item_opin)\n",
    "iu_7 = formulate_seq(train_items,attr7_item_opin)\n",
    "\n",
    "print('#interactions: ',inter_count)\n",
    "\n",
    "print('sequence average length: ', (inter_count)/(user_count) * 1.0)\n",
    "# print('cold sequence average length: ', (all_train+all_tes_nocold+all_tes_cold)/(len(tra_seqs) + len(tes_seqs_nocold) +len(tes_seqs_cold) * 1.0))\n",
    "train_data_path = './datasets/' + meta_dataset\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "path_data_train = train_data_path + \"/train.txt\"\n",
    "path_normal_test = train_data_path + \"/test.txt\"\n",
    "tra_save = (tra[0], tra[1], tra[2],item_co_mat,user_co_mat, attr1_mat, attr2_mat, attr3_mat, attr4_mat, attr5_mat, attr6_mat, attr7_mat, ui_1, ui_2, ui_3, ui_4, ui_5, ui_6, ui_7, iu_1, iu_2, iu_3, iu_4, iu_5, iu_6, iu_7)\n",
    "tes_save = (tes[0], tes[1], tes[2])\n",
    "\n",
    "pickle.dump(tra_save, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes_save, open(path_normal_test, 'wb'))\n",
    "print(\"dataset: \", meta_dataset)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 0, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,3,4]+2*[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## senti backup 都是用矩阵来进行运算，-> heterogeneity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "meta_dataset = 'Beauty'\n",
    "data_path =  './dict/' + meta_dataset + '/session_data.csv'\n",
    "\n",
    "if meta_dataset == 'Beauty' or meta_dataset == 'Beauty_4':\n",
    "    # 'Brand','Ingredients','Effectiveness','Scent','Price','Color','Size'\n",
    "    attr_list = ['Brand','Ingredients','Effectiveness','Scent','Price','Color','Size']\n",
    "elif meta_dataset == 'Cell_Phones_and_Accessories':\n",
    "    # Brand, Performance, color, Battery ,Connectivity,Price, Size\n",
    "    attr_list = ['Brand', 'Performance', 'Color', 'Battery' ,'Connectivity','Price', 'Size']\n",
    "elif meta_dataset == 'Clothing_Shoes_and_Jewelry':\n",
    "    # Price, Style, Size, Fabric, Brand, Quality, Color\n",
    "    attr_list = ['Price', 'Style', 'Size', 'Fabric', 'Brand', 'Quality', 'Color']\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data_all = data_all.reset_index()[['sessionID', 'itemID', attr_list[0], attr_list[1],attr_list[2],attr_list[3],attr_list[4],attr_list[5],attr_list[6]]]\n",
    "\n",
    "attr_num = len(attr_list)\n",
    "\n",
    "# recorde all items\n",
    "# dict {sessionID:[itemID,itemID]} \n",
    "sess_all = {}\n",
    "#  dict {sessionID:[opinID,opinID]}\n",
    "attr1_all = {}\n",
    "attr2_all = {}\n",
    "attr3_all = {}\n",
    "attr4_all = {}\n",
    "attr5_all = {}\n",
    "attr6_all = {}\n",
    "attr7_all = {}\n",
    "\n",
    "item_count = 0\n",
    "user_count = 0\n",
    "inter_count = 0\n",
    "\n",
    "for _, row in data_all.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    attr1_opin = row[attr_list[0]]\n",
    "    attr2_opin = row[attr_list[1]]\n",
    "    attr3_opin = row[attr_list[2]]\n",
    "    attr4_opin = row[attr_list[3]]\n",
    "    attr5_opin = row[attr_list[4]]\n",
    "    attr6_opin = row[attr_list[5]]\n",
    "    attr7_opin = row[attr_list[6]]\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "        attr1_all[sess_id].append(attr1_opin)\n",
    "        attr2_all[sess_id].append(attr2_opin)\n",
    "        attr3_all[sess_id].append(attr3_opin)\n",
    "        attr4_all[sess_id].append(attr4_opin)\n",
    "        attr5_all[sess_id].append(attr5_opin)\n",
    "        attr6_all[sess_id].append(attr6_opin)\n",
    "        attr7_all[sess_id].append(attr7_opin)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "        attr1_all[sess_id] = []\n",
    "        attr1_all[sess_id].append(attr1_opin)\n",
    "        attr2_all[sess_id] = []\n",
    "        attr2_all[sess_id].append(attr2_opin)\n",
    "        attr3_all[sess_id] = []\n",
    "        attr3_all[sess_id].append(attr3_opin)\n",
    "        attr4_all[sess_id] = []\n",
    "        attr4_all[sess_id].append(attr4_opin)\n",
    "        attr5_all[sess_id] = []\n",
    "        attr5_all[sess_id].append(attr5_opin)\n",
    "        attr6_all[sess_id] = []\n",
    "        attr6_all[sess_id].append(attr6_opin)\n",
    "        attr7_all[sess_id] = []\n",
    "        attr7_all[sess_id].append(attr7_opin)\n",
    "\n",
    "\n",
    "# 形成train set\n",
    "# 统计 train set 出现的item，给item重新编号\n",
    "item_num = 1\n",
    "# dict{lod_itemID:new_itemID}\n",
    "item_dict = {}\n",
    "# train_set [[u_id ...], [x_t-1...], [label...]\n",
    "tra = [[],[],[]]\n",
    "# dict {train_sessionID:[itemID,itemID]}  去掉了seq中last item(不包含test的label)， 也可以调整为去掉两个item\n",
    "train_sess = {}\n",
    "\n",
    "# dict {item_id:[user_id, user_id]}\n",
    "train_items = {}\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    train_seqs_t = all_seqs[:-1]\n",
    "    inter_count += len(train_seqs_t)\n",
    "#     item_seq with new ID\n",
    "    new_seq = []\n",
    "    for x in train_seqs_t:\n",
    "        if x in item_dict:\n",
    "            new_seq.append(item_dict[x])\n",
    "            if sess_temp not in train_items[item_dict[x]]:\n",
    "                train_items[item_dict[x]].append(sess_temp)\n",
    "        else:\n",
    "            item_dict[x] = item_num\n",
    "            new_seq.append(item_num)\n",
    "            train_items[item_num] = [] \n",
    "            train_items[item_num].append(sess_temp)\n",
    "            \n",
    "            item_num += 1\n",
    "    train_sess[sess_temp] = new_seq\n",
    "    tra[0].append(sess_temp)\n",
    "#     预测时考虑last item\n",
    "    tra[1].append(new_seq[-2])\n",
    "    tra[2].append(new_seq[-1])\n",
    "    \n",
    "item_count =  item_num-1\n",
    "user_count = len(tra[0])\n",
    "\n",
    "fre_item_mat = np.zeros((item_count, item_count), dtype=int)\n",
    "# 全部的共现关系Original \n",
    "for sess in train_sess.keys():\n",
    "    item_seq = np.unique(train_sess[sess])\n",
    "    for it1 in item_seq:\n",
    "        for it2 in item_seq:\n",
    "            if it1 != it2:\n",
    "#                 print(sess)\n",
    "                fre_item_mat[int(it1)-1, int(it2)-1] +=1 \n",
    "                \n",
    "fre_user_mat = np.zeros((user_count, user_count), dtype=int)\n",
    "# 全部的共现关系Original \n",
    "for item_t in train_items.keys():\n",
    "    user_seq = np.unique(train_items[item_t])\n",
    "    for it1 in user_seq:\n",
    "        for it2 in user_seq:\n",
    "            if it1 != it2:\n",
    "                fre_user_mat[int(it1)-1, int(it2)-1] +=1 \n",
    "\n",
    "# building user_fre & item_fre csr_matrix\n",
    "def spar_graph(fre_mat):\n",
    "    indices = [] # 非0元素的列坐标，item_id -1\n",
    "    all_data = [] # 非0元素值，opin_id\n",
    "    indptr = [0] # i i-1 行非0元素个数，0-0， n-总共元素个数，行数+1\n",
    "    for i_list in fre_mat:\n",
    "        i_temp_list = np.nonzero(i_list)[0].tolist()\n",
    "#         if len(i_temp_list) == 0:\n",
    "#             non_co += 1\n",
    "        indices += i_temp_list\n",
    "        all_data += [1]*len(i_temp_list)\n",
    "        indptr.append(indptr[-1]+len(i_temp_list))\n",
    "#     print('item without co-occurren: ',str(non_co))\n",
    "    # indptr:session长度累加和; indices:item_id 减1, 由每个session内item组成; data:item在session内的权重，全部为1.\n",
    "    coo_mat = (all_data, indices, indptr)\n",
    "    return coo_mat\n",
    "\n",
    "item_co_mat = spar_graph(fre_item_mat)\n",
    "user_co_mat = spar_graph(fre_user_mat)\n",
    "\n",
    "\n",
    "# test_set [[u_id ...], [x_t-1...], [label...]\n",
    "tes = [[],[],[]]\n",
    "tes_seq_num = 0\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if all_seqs[-1] in item_dict:\n",
    "        tes_seq_num += 1\n",
    "        tes[0].append(sess_temp)\n",
    "    #     预测时考虑last item\n",
    "        tes[1].append(item_dict[all_seqs[-2]])\n",
    "        tes[2].append(item_dict[all_seqs[-1]])\n",
    "    \n",
    "\n",
    "print('item_num: ',str(item_count))\n",
    "print('train_seq_num & user_num: ',str(user_count))\n",
    "print('test_seq_num: ',str(tes_seq_num))\n",
    "\n",
    "inter_count += tes_seq_num\n",
    "\n",
    "#  obtain user-item interaction sparsity matrix in each attribute\n",
    "def spar_matrix(attr, tra_seq):\n",
    "    indices = [] # 非0元素的列坐标，item_id -1\n",
    "    all_data = [] # 非0元素值，opin_id\n",
    "    indptr = [0] # i i-1 行非0元素个数，0-0， n-总共元素个数，行数+1\n",
    "    for sess_temp in attr.keys():\n",
    "        op_list = attr[sess_temp][:-1]\n",
    "        it_list = tra_seq[sess_temp]\n",
    "        np_op = np.array(op_list)\n",
    "        np_it = np.array(it_list)\n",
    "        op_f = np_op[np.nonzero(np_op)[0]]\n",
    "        it_f = np_it[np.nonzero(np_op)[0]]\n",
    "\n",
    "        item_l = np.unique(it_f)\n",
    "        op_l = op_f[np.unique(it_f, return_index=True)[1]]\n",
    "        indices += (item_l-1).tolist()\n",
    "        all_data += op_l.tolist()\n",
    "        indptr.append(indptr[-1] + len(item_l.tolist()))\n",
    "    results = (all_data, indices, indptr)\n",
    "    return results\n",
    "\n",
    "attr1_mat = spar_matrix(attr1_all, train_sess)\n",
    "attr2_mat = spar_matrix(attr2_all, train_sess)\n",
    "attr3_mat = spar_matrix(attr3_all, train_sess)\n",
    "attr4_mat = spar_matrix(attr4_all, train_sess)\n",
    "attr5_mat = spar_matrix(attr5_all, train_sess)\n",
    "attr6_mat = spar_matrix(attr6_all, train_sess)\n",
    "attr7_mat = spar_matrix(attr7_all, train_sess)\n",
    "\n",
    "print('#interactions: ',inter_count)\n",
    "\n",
    "print('sequence average length: ', (inter_count)/(user_count) * 1.0)\n",
    "# print('cold sequence average length: ', (all_train+all_tes_nocold+all_tes_cold)/(len(tra_seqs) + len(tes_seqs_nocold) +len(tes_seqs_cold) * 1.0))\n",
    "train_data_path = './datasets/' + meta_dataset\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "path_data_train = train_data_path + \"/train.txt\"\n",
    "path_normal_test = train_data_path + \"/test.txt\"\n",
    "tra_save = (tra[0], tra[1], tra[2],item_co_mat,user_co_mat, attr1_mat, attr2_mat, attr3_mat, attr4_mat, attr5_mat, attr6_mat, attr7_mat)\n",
    "tes_save = (tes[0], tes[1], tes[2])\n",
    "\n",
    "pickle.dump(tra_save, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes_save, open(path_normal_test, 'wb'))\n",
    "print(\"dataset: \", meta_dataset)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NARM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_num:  10346\n",
      "train_seq_num & user_num:  27777\n",
      "test_seq_num:  27566\n",
      "#interactions:  189133\n",
      "sequence average length:  6.8089786514022395\n",
      "NARM dataset:  Cell_Phones_and_Accessories\n",
      "2023-12-20 18:09:01\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "meta_dataset = 'Cell_Phones_and_Accessories'\n",
    "data_path =  './dict/' + meta_dataset + '/session_data.csv'\n",
    "\n",
    "if meta_dataset == 'Beauty' or meta_dataset == 'Beauty_4':\n",
    "    # 'Brand','Ingredients','Effectiveness','Scent','Price','Color','Size'\n",
    "    attr_list = ['Brand','Ingredients','Effectiveness','Scent','Price','Color','Size']\n",
    "elif meta_dataset == 'Cell_Phones_and_Accessories':\n",
    "    # Brand, Performance, color, Battery ,Connectivity,Price, Size\n",
    "    attr_list = ['Brand', 'Performance', 'Color', 'Battery' ,'Connectivity','Price', 'Size']\n",
    "elif meta_dataset == 'Clothing_Shoes_and_Jewelry':\n",
    "    # Price, Style, Size, Fabric, Brand, Quality, Color\n",
    "    attr_list = ['Price', 'Style', 'Size', 'Fabric', 'Brand', 'Quality', 'Color']\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data_all = data_all.reset_index()[['sessionID', 'itemID', attr_list[0], attr_list[1],attr_list[2],attr_list[3],attr_list[4],attr_list[5],attr_list[6]]]\n",
    "data_all = data_all[['sessionID', 'itemID']]\n",
    "\n",
    "\n",
    "# recorde all items\n",
    "# dict {sessionID:[itemID,itemID]} \n",
    "sess_all = {}\n",
    "#  dict {sessionID:[opinID,opinID]}\n",
    "\n",
    "\n",
    "item_count = 0\n",
    "user_count = 0\n",
    "inter_count = 0\n",
    "\n",
    "for _, row in data_all.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    if sess_id in sess_all:\n",
    "        if len(sess_all[sess_id]) < 21: \n",
    "            sess_all[sess_id].append(item_id)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "\n",
    "\n",
    "# 形成train set\n",
    "# 统计 train set 出现的item，给item重新编号\n",
    "item_num = 1\n",
    "# dict{lod_itemID:new_itemID}\n",
    "item_dict = {}\n",
    "# train_set [[x1, x2, x3], [label...]\n",
    "tra = [[],[]]\n",
    "# dict {train_sessionID:[itemID,itemID]}  去掉了seq中last item(不包含test的label)， 也可以调整为去掉两个item\n",
    "train_sess = {}\n",
    "\n",
    "# dict {item_id:[user_id, user_id]}\n",
    "train_items = {}\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    train_seqs_t = all_seqs[:-1]\n",
    "    inter_count += len(train_seqs_t)\n",
    "#     item_seq with new ID\n",
    "    new_seq = []\n",
    "    for x in train_seqs_t:\n",
    "        if x in item_dict:\n",
    "            new_seq.append(item_dict[x])\n",
    "            if sess_temp not in train_items[item_dict[x]]:\n",
    "                train_items[item_dict[x]].append(sess_temp)\n",
    "        else:\n",
    "            item_dict[x] = item_num\n",
    "            new_seq.append(item_num)\n",
    "            train_items[item_num] = [] \n",
    "            train_items[item_num].append(sess_temp)\n",
    "            \n",
    "            item_num += 1\n",
    "    train_sess[sess_temp] = new_seq\n",
    "    tra[0].append(new_seq[:-1])\n",
    "#     预测时考虑last item\n",
    "    tra[1].append(new_seq[-1])\n",
    "    \n",
    "item_count =  item_num-1\n",
    "user_count = len(tra[0])\n",
    "\n",
    "# test_set [[x_id ...],[label...]\n",
    "tes = [[],[]]\n",
    "tes_seq_num = 0\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if all_seqs[-1] in item_dict:\n",
    "        new_seq = []\n",
    "        for x in all_seqs:\n",
    "            new_seq.append(item_dict[x])\n",
    "        tes_seq_num += 1\n",
    "        tes[0].append(new_seq[:-1])\n",
    "    #     预测时考虑last item\n",
    "        tes[1].append(new_seq[-1])\n",
    "    \n",
    "\n",
    "print('item_num: ',str(item_count))\n",
    "print('train_seq_num & user_num: ',str(user_count))\n",
    "print('test_seq_num: ',str(tes_seq_num))\n",
    "\n",
    "inter_count += tes_seq_num\n",
    "\n",
    "\n",
    "print('#interactions: ',inter_count)\n",
    "\n",
    "print('sequence average length: ', (inter_count)/(user_count) * 1.0)\n",
    "# print('cold sequence average length: ', (all_train+all_tes_nocold+all_tes_cold)/(len(tra_seqs) + len(tes_seqs_nocold) +len(tes_seqs_cold) * 1.0))\n",
    "train_data_path = './NARM/' + meta_dataset\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "path_data_train = train_data_path + \"/train.txt\"\n",
    "path_normal_test = train_data_path + \"/test.txt\"\n",
    "tra_save = (tra[0], tra[1])\n",
    "tes_save = (tes[0], tes[1])\n",
    "\n",
    "pickle.dump(tra_save, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes_save, open(path_normal_test, 'wb'))\n",
    "print(\"NARM dataset: \", meta_dataset)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT4Rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_num:  10388\n",
      "train_seq_num & user_num:  27777\n",
      "test_seq_num:  27645\n",
      "#interactions:  193599\n",
      "sequence average length:  6.969759153256291\n",
      "NARM dataset:  Cell_Phones_and_Accessories\n",
      "2023-12-20 17:44:22\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "meta_dataset = 'Cell_Phones_and_Accessories'\n",
    "data_path =  './dict/' + meta_dataset + '/session_data.csv'\n",
    "\n",
    "if meta_dataset == 'Beauty' or meta_dataset == 'Beauty_4':\n",
    "    # 'Brand','Ingredients','Effectiveness','Scent','Price','Color','Size'\n",
    "    attr_list = ['Brand','Ingredients','Effectiveness','Scent','Price','Color','Size']\n",
    "elif meta_dataset == 'Cell_Phones_and_Accessories':\n",
    "    # Brand, Performance, color, Battery ,Connectivity,Price, Size\n",
    "    attr_list = ['Brand', 'Performance', 'Color', 'Battery' ,'Connectivity','Price', 'Size']\n",
    "elif meta_dataset == 'Clothing_Shoes_and_Jewelry':\n",
    "    # Price, Style, Size, Fabric, Brand, Quality, Color\n",
    "    attr_list = ['Price', 'Style', 'Size', 'Fabric', 'Brand', 'Quality', 'Color']\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data_all = data_all.reset_index()[['sessionID', 'itemID', attr_list[0], attr_list[1],attr_list[2],attr_list[3],attr_list[4],attr_list[5],attr_list[6]]]\n",
    "data_all = data_all[['sessionID', 'itemID']]\n",
    "\n",
    "\n",
    "# recorde all items\n",
    "# dict {sessionID:[itemID,itemID]} \n",
    "sess_all = {}\n",
    "#  dict {sessionID:[opinID,opinID]}\n",
    "\n",
    "\n",
    "item_count = 0\n",
    "user_count = 0\n",
    "inter_count = 0\n",
    "\n",
    "for _, row in data_all.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "\n",
    "\n",
    "# 形成train set\n",
    "# 统计 train set 出现的item，给item重新编号\n",
    "item_num = 1\n",
    "# dict{lod_itemID:new_itemID}\n",
    "item_dict = {}\n",
    "# train_set [[user1,user2],[x1, x2, x3], [label...]\n",
    "tra = [[],[],[]]\n",
    "# dict {train_sessionID:[itemID,itemID]}  去掉了seq中last item(不包含test的label)， 也可以调整为去掉两个item\n",
    "train_sess = {}\n",
    "\n",
    "# dict {item_id:[user_id, user_id]}\n",
    "train_items = {}\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    train_seqs_t = all_seqs[:-1]\n",
    "    inter_count += len(train_seqs_t)\n",
    "#     item_seq with new ID\n",
    "    new_seq = []\n",
    "    for x in train_seqs_t:\n",
    "        if x in item_dict:\n",
    "            new_seq.append(item_dict[x])\n",
    "            if sess_temp not in train_items[item_dict[x]]:\n",
    "                train_items[item_dict[x]].append(sess_temp)\n",
    "        else:\n",
    "            item_dict[x] = item_num\n",
    "            new_seq.append(item_num)\n",
    "            train_items[item_num] = [] \n",
    "            train_items[item_num].append(sess_temp)\n",
    "            \n",
    "            item_num += 1\n",
    "    train_sess[sess_temp] = new_seq\n",
    "    tra[0].append(sess_temp)\n",
    "    tra[1].append(new_seq[:-1])\n",
    "#     预测时考虑last item\n",
    "    tra[2].append(new_seq[-1])\n",
    "    \n",
    "item_count =  item_num-1\n",
    "user_count = len(tra[0])\n",
    "\n",
    "# test_set [[user1,user2],[x_id ...],[label...]\n",
    "tes = [[],[],[]]\n",
    "tes_seq_num = 0\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if all_seqs[-1] in item_dict:\n",
    "        new_seq = []\n",
    "        for x in all_seqs:\n",
    "            new_seq.append(item_dict[x])\n",
    "        tes_seq_num += 1\n",
    "        tes[0].append(sess_temp)\n",
    "        tes[1].append(new_seq[:-1])\n",
    "    #     预测时考虑last item\n",
    "        tes[2].append(new_seq[-1])\n",
    "    \n",
    "\n",
    "print('item_num: ',str(item_count))\n",
    "print('train_seq_num & user_num: ',str(user_count))\n",
    "print('test_seq_num: ',str(tes_seq_num))\n",
    "\n",
    "inter_count += tes_seq_num\n",
    "\n",
    "\n",
    "print('#interactions: ',inter_count)\n",
    "\n",
    "print('sequence average length: ', (inter_count)/(user_count) * 1.0)\n",
    "# print('cold sequence average length: ', (all_train+all_tes_nocold+all_tes_cold)/(len(tra_seqs) + len(tes_seqs_nocold) +len(tes_seqs_cold) * 1.0))\n",
    "train_data_path = './BERT4Rec/' + meta_dataset\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "path_data_train = train_data_path + \"/train.txt\"\n",
    "path_normal_test = train_data_path + \"/test.txt\"\n",
    "tra_save = (tra[0], tra[1], tra[2])\n",
    "tes_save = (tes[0], tes[1], tes[2])\n",
    "\n",
    "pickle.dump(tra_save, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes_save, open(path_normal_test, 'wb'))\n",
    "print(\"BERT4Rec dataset: \", meta_dataset)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
